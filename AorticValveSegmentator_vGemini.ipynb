{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mardenlelis/ConfusionMatrixFunction/blob/main/AorticValveSegmentator_vGemini.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AorticValveSegmentator - Notebook Melhorado\n",
        "\n",
        "Este notebook foi aprimorado com base nas sugest√µes de melhoria para o pipeline de segmenta√ß√£o da v√°lvula a√≥rtica usando nnU-Netv2, incluindo corre√ß√µes de caminho, mensagens mais claras e prepara√ß√£o para lidar com erros de mem√≥ria durante o treinamento."
      ],
      "metadata": {
        "id": "new-notebook-intro"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "üöÄ C√âLULA 1 ‚Äî Montar Google Drive"
      ],
      "metadata": {
        "id": "-uNH1mHlNfBB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# üì¶ Montar Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N9VGP8MPNZa1",
        "outputId": "7c5f5ac2-2cca-49ef-ffcf-bb7f3bd562ff"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "üîß C√©lula 2 ‚Äî Instalar depend√™ncias"
      ],
      "metadata": {
        "id": "aAcl4Z7NNivs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# üìå Instalar depend√™ncias\n",
        "# ‚úÖ Instalar bibliotecas essenciais e nnU-Net v2\n",
        "!pip install --upgrade pip\n",
        "!pip install numpy pandas nibabel tqdm SimpleITK openpyxl --quiet\n",
        "!pip install medpy --quiet\n",
        "!pip install nnunetv2 --quiet\n",
        "# A flag --index-url √© crucial para garantir a instala√ß√£o de uma vers√£o de torch compat√≠vel com a GPU (cu118)\n",
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118 --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cH6XBSicNlL-",
        "outputId": "cd1b9c2b-0786-4c70-eaa1-1646f3dfdd20"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.11/dist-packages (24.1.2)\n",
            "Collecting pip\n",
            "  Downloading pip-25.1.1-py3-none-any.whl.metadata (3.6 kB)\n",
            "Downloading pip-25.1.1-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m75.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pip\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 24.1.2\n",
            "    Uninstalling pip-24.1.2:\n",
            "      Successfully uninstalled pip-24.1.2\n",
            "Successfully installed pip-25.1.1\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m52.6/52.6 MB\u001b[0m \u001b[31m121.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[33m  DEPRECATION: Building 'medpy' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'medpy'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
            "\u001b[0m  Building wheel for medpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m59.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m152.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m149.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m51.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m43.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m64.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m124.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m130.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m74.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m149.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m93.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m45.6/45.6 MB\u001b[0m \u001b[31m110.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for nnunetv2 (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[33m  DEPRECATION: Building 'acvl-utils' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'acvl-utils'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
            "\u001b[0m  Building wheel for acvl-utils (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[33m  DEPRECATION: Building 'dynamic-network-architectures' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'dynamic-network-architectures'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
            "\u001b[0m  Building wheel for dynamic-network-architectures (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[33m  DEPRECATION: Building 'batchgenerators' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'batchgenerators'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
            "\u001b[0m  Building wheel for batchgenerators (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for batchgeneratorsv2 (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m23/23\u001b[0m [nnunetv2]\n",
            "\u001b[1A\u001b[2K"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚öôÔ∏è C√âLULA 3 ‚Äî Verificar GPU e dispositivo"
      ],
      "metadata": {
        "id": "lBKYMcMsNniZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚öôÔ∏è Verificar disponibilidade de GPU\n",
        "import torch\n",
        "print(\"CUDA dispon√≠vel:\", torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print(\"Dispositivo:\", torch.cuda.get_device_name(0))\n",
        "else:\n",
        "    print(\"AVISO: Nenhuma GPU CUDA detectada. O treinamento ser√° muito lento.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zcH031XlNszp",
        "outputId": "0c7449d4-973d-4b6e-d30b-6e1b4a8bb3ec"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA dispon√≠vel: True\n",
            "Dispositivo: Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "üßπ C√©lula 4 ‚Äî Limpar diret√≥rios de origem de imagens e r√≥tulos (opcional)"
      ],
      "metadata": {
        "id": "Xswwn7A1Sh5q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# AVISO: Esta c√©lula apagar√° **TODOS** os arquivos nas pastas de imagens e r√≥tulos originais\n",
        "# dentro da estrutura do nnU-Net. Use com cautela se voc√™ j√° tiver dados l√°.\n",
        "# √â √∫til para recome√ßar do zero, mas pode ser comentada se os dados j√° estiverem preparados.\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "dir_originais_imagens = '/content/drive/MyDrive/nnunet_data/nnUNet_raw/Dataset001_AorticValve/originais/imagens'\n",
        "dir_originais_rotulos = '/content/drive/MyDrive/nnunet_data/nnUNet_raw/Dataset001_AorticValve/originais/rotulos'\n",
        "\n",
        "print(\"Limpando diret√≥rios de origem para Dataset001_AorticValve/originais/imagens e rotulos...\")\n",
        "\n",
        "for d in [dir_originais_imagens, dir_originais_rotulos]:\n",
        "    if os.path.exists(d):\n",
        "        # Remove o conte√∫do, mas mant√©m o diret√≥rio para recria√ß√£o se necess√°rio\n",
        "        for f in os.listdir(d):\n",
        "            file_path = os.path.join(d, f)\n",
        "            try:\n",
        "                if os.path.isfile(file_path) or os.path.islink(file_path):\n",
        "                    os.unlink(file_path)\n",
        "                elif os.path.isdir(file_path):\n",
        "                    shutil.rmtree(file_path)\n",
        "            except Exception as e:\n",
        "                print(f'Falha ao deletar {file_path}. Raz√£o: {e}')\n",
        "    else:\n",
        "        print(f'Diret√≥rio n√£o encontrado: {d}. Criando...')\n",
        "        os.makedirs(d, exist_ok=True)\n",
        "\n",
        "print(\"Limpeza conclu√≠da.\")\n"
      ],
      "metadata": {
        "id": "5vGv35lXS039",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2cd06f1f-4bc3-46b8-f334-92dc99599bc2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Limpando diret√≥rios de origem para Dataset001_AorticValve/originais/imagens e rotulos...\n",
            "Limpeza conclu√≠da.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "üìÅ C√©lula 5 - Copia os arquivos do diret√≥rio origem e transfere para a estrutura de arquivos nnU-Net"
      ],
      "metadata": {
        "id": "r-q3Z5XkS4aN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Essa c√©lula copia os arquivos do diret√≥rio origem e transfere para a estrutura de arquivos\n",
        "# utilizadas pelas c√©lulas de prepara√ß√£o do dataset, com renomea√ß√£o espec√≠fica.\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# --- Caminhos ---\n",
        "# ATEN√á√ÉO: Ajuste 'dir_origem' para o local dos seus arquivos NRRD de origem.\n",
        "dir_origem = '/content/drive/MyDrive/TC_DICOM/nrrd'\n",
        "\n",
        "# Diret√≥rios de destino para as imagens e os r√≥tulos (labels) na estrutura nnU-Net_raw\n",
        "dir_destino_imagens = '/content/drive/MyDrive/nnunet_data/nnUNet_raw/Dataset001_AorticValve/originais/imagens'\n",
        "dir_destino_labels = '/content/drive/MyDrive/nnunet_data/nnUNet_raw/Dataset001_AorticValve/originais/rotulos'\n",
        "\n",
        "# Garante que os diret√≥rios de destino existam\n",
        "os.makedirs(dir_destino_imagens, exist_ok=True)\n",
        "os.makedirs(dir_destino_labels, exist_ok=True)\n",
        "\n",
        "print(\"Iniciando a busca, c√≥pia e RENOMEA√á√ÉO com crit√©rios espec√≠ficos...\")\n",
        "\n",
        "if not os.path.isdir(dir_origem):\n",
        "    print(f\"\\nERRO: O diret√≥rio de origem '{dir_origem}' n√£o foi encontrado. Por favor, verifique o caminho e a montagem do Drive.\")\n",
        "else:\n",
        "    found_any_file = False\n",
        "    for raiz, subpastas, arquivos in os.walk(dir_origem):\n",
        "        if raiz == dir_origem: # Pula o diret√≥rio raiz em si\n",
        "            continue\n",
        "\n",
        "        id_paciente_str = os.path.basename(raiz)\n",
        "        if not id_paciente_str.isdigit():\n",
        "            print(f\"  AVISO: Ignorando pasta '{raiz}' pois o nome n√£o √© um ID de paciente num√©rico v√°lido.\")\n",
        "            continue\n",
        "\n",
        "        id_paciente_formatado = id_paciente_str.zfill(3)\n",
        "        print(f\"\\nAnalisando pasta do paciente: {id_paciente_formatado}\")\n",
        "\n",
        "        label_copiado = False\n",
        "        imagem_copiada = False\n",
        "\n",
        "        for nome_arquivo in arquivos:\n",
        "            caminho_completo_origem = os.path.join(raiz, nome_arquivo)\n",
        "\n",
        "            # 1. Procura pelo arquivo de r√≥tulo que termina com '.seg.nrrd'\n",
        "            if nome_arquivo.endswith('.seg.nrrd') and not label_copiado:\n",
        "                novo_nome_label = f\"{id_paciente_formatado}.nrrd\"\n",
        "                caminho_completo_destino = os.path.join(dir_destino_labels, novo_nome_label)\n",
        "                print(f\"  -> R√ìTULO encontrado e renomeado para: {novo_nome_label}\")\n",
        "                shutil.copy2(caminho_completo_origem, caminho_completo_destino)\n",
        "                label_copiado = True\n",
        "                found_any_file = True\n",
        "\n",
        "            # 2. Procura pelo arquivo de imagem com nome exato '120 KV.nrrd'\n",
        "            if nome_arquivo.endswith('120 KV.nrrd') and not imagem_copiada:\n",
        "                novo_nome_imagem = f\"{id_paciente_formatado}.nrrd\"\n",
        "                caminho_completo_destino = os.path.join(dir_destino_imagens, novo_nome_imagem)\n",
        "                print(f\"  -> IMAGEM encontrada e renomeada para: {novo_nome_imagem}\")\n",
        "                shutil.copy2(caminho_completo_origem, caminho_completo_destino)\n",
        "                imagem_copiada = True\n",
        "                found_any_file = True\n",
        "\n",
        "        if not label_copiado:\n",
        "            print(f\"  -> AVISO: Nenhum arquivo de R√ìTULO terminando com '.seg.nrrd' foi encontrado para o paciente {id_paciente_formatado}.\")\n",
        "        if not imagem_copiada:\n",
        "            print(f\"  -> AVISO: Nenhum arquivo de IMAGEM terminando com '120 KV.nrrd' foi encontrado para o paciente {id_paciente_formatado}.\")\n",
        "\n",
        "    if not found_any_file:\n",
        "        print(\"\\nAVISO: Nenhuma imagem ou r√≥tulo v√°lido foi encontrado no diret√≥rio de origem ou subdiret√≥rios.\\nCertifique-se de que o caminho est√° correto e os arquivos seguem o padr√£o esperado.\")\n",
        "\n",
        "    print(\"\\n\\nProcesso de c√≥pia e renomea√ß√£o conclu√≠do!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TUC7RpfbS7rK",
        "outputId": "82e2870c-f6bf-49c8-b1d0-c5b933f4e98f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iniciando a busca, c√≥pia e RENOMEA√á√ÉO com crit√©rios espec√≠ficos...\n",
            "\n",
            "Analisando pasta do paciente: 083\n",
            "  -> IMAGEM encontrada e renomeada para: 083.nrrd\n",
            "  -> R√ìTULO encontrado e renomeado para: 083.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 084\n",
            "  -> R√ìTULO encontrado e renomeado para: 084.nrrd\n",
            "  -> IMAGEM encontrada e renomeada para: 084.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 091\n",
            "  -> IMAGEM encontrada e renomeada para: 091.nrrd\n",
            "  -> R√ìTULO encontrado e renomeado para: 091.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 089\n",
            "  -> R√ìTULO encontrado e renomeado para: 089.nrrd\n",
            "  -> IMAGEM encontrada e renomeada para: 089.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 090\n",
            "  -> R√ìTULO encontrado e renomeado para: 090.nrrd\n",
            "  -> IMAGEM encontrada e renomeada para: 090.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 092\n",
            "  -> IMAGEM encontrada e renomeada para: 092.nrrd\n",
            "  -> R√ìTULO encontrado e renomeado para: 092.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 087\n",
            "  -> R√ìTULO encontrado e renomeado para: 087.nrrd\n",
            "  -> IMAGEM encontrada e renomeada para: 087.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 085\n",
            "  -> IMAGEM encontrada e renomeada para: 085.nrrd\n",
            "  -> R√ìTULO encontrado e renomeado para: 085.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 088\n",
            "  -> IMAGEM encontrada e renomeada para: 088.nrrd\n",
            "  -> R√ìTULO encontrado e renomeado para: 088.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 086\n",
            "  -> IMAGEM encontrada e renomeada para: 086.nrrd\n",
            "  -> R√ìTULO encontrado e renomeado para: 086.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 093\n",
            "  -> IMAGEM encontrada e renomeada para: 093.nrrd\n",
            "  -> R√ìTULO encontrado e renomeado para: 093.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 094\n",
            "  -> IMAGEM encontrada e renomeada para: 094.nrrd\n",
            "  -> R√ìTULO encontrado e renomeado para: 094.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 095\n",
            "  -> R√ìTULO encontrado e renomeado para: 095.nrrd\n",
            "  -> IMAGEM encontrada e renomeada para: 095.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 096\n",
            "  -> IMAGEM encontrada e renomeada para: 096.nrrd\n",
            "  -> R√ìTULO encontrado e renomeado para: 096.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 098\n",
            "  -> R√ìTULO encontrado e renomeado para: 098.nrrd\n",
            "  -> IMAGEM encontrada e renomeada para: 098.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 004\n",
            "  -> R√ìTULO encontrado e renomeado para: 004.nrrd\n",
            "  -> IMAGEM encontrada e renomeada para: 004.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 100\n",
            "  -> R√ìTULO encontrado e renomeado para: 100.nrrd\n",
            "  -> IMAGEM encontrada e renomeada para: 100.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 003\n",
            "  -> R√ìTULO encontrado e renomeado para: 003.nrrd\n",
            "  -> IMAGEM encontrada e renomeada para: 003.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 097\n",
            "  -> IMAGEM encontrada e renomeada para: 097.nrrd\n",
            "  -> R√ìTULO encontrado e renomeado para: 097.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 001\n",
            "  -> R√ìTULO encontrado e renomeado para: 001.nrrd\n",
            "  -> IMAGEM encontrada e renomeada para: 001.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 002\n",
            "  -> IMAGEM encontrada e renomeada para: 002.nrrd\n",
            "  -> R√ìTULO encontrado e renomeado para: 002.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 099\n",
            "  -> R√ìTULO encontrado e renomeado para: 099.nrrd\n",
            "  -> IMAGEM encontrada e renomeada para: 099.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 005\n",
            "  -> R√ìTULO encontrado e renomeado para: 005.nrrd\n",
            "  -> IMAGEM encontrada e renomeada para: 005.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 006\n",
            "  -> IMAGEM encontrada e renomeada para: 006.nrrd\n",
            "  -> R√ìTULO encontrado e renomeado para: 006.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 007\n",
            "  -> R√ìTULO encontrado e renomeado para: 007.nrrd\n",
            "  -> IMAGEM encontrada e renomeada para: 007.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 008\n",
            "  -> IMAGEM encontrada e renomeada para: 008.nrrd\n",
            "  -> R√ìTULO encontrado e renomeado para: 008.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 011\n",
            "  -> IMAGEM encontrada e renomeada para: 011.nrrd\n",
            "  -> R√ìTULO encontrado e renomeado para: 011.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 009\n",
            "  -> IMAGEM encontrada e renomeada para: 009.nrrd\n",
            "  -> R√ìTULO encontrado e renomeado para: 009.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 014\n",
            "  -> IMAGEM encontrada e renomeada para: 014.nrrd\n",
            "  -> R√ìTULO encontrado e renomeado para: 014.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 013\n",
            "  -> R√ìTULO encontrado e renomeado para: 013.nrrd\n",
            "  -> IMAGEM encontrada e renomeada para: 013.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 012\n",
            "  -> IMAGEM encontrada e renomeada para: 012.nrrd\n",
            "  -> R√ìTULO encontrado e renomeado para: 012.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 016\n",
            "  -> R√ìTULO encontrado e renomeado para: 016.nrrd\n",
            "  -> IMAGEM encontrada e renomeada para: 016.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 010\n",
            "  -> R√ìTULO encontrado e renomeado para: 010.nrrd\n",
            "  -> IMAGEM encontrada e renomeada para: 010.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 015\n",
            "  -> IMAGEM encontrada e renomeada para: 015.nrrd\n",
            "  -> R√ìTULO encontrado e renomeado para: 015.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 017\n",
            "  -> IMAGEM encontrada e renomeada para: 017.nrrd\n",
            "  -> R√ìTULO encontrado e renomeado para: 017.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 018\n",
            "  -> R√ìTULO encontrado e renomeado para: 018.nrrd\n",
            "  -> IMAGEM encontrada e renomeada para: 018.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 020\n",
            "  -> IMAGEM encontrada e renomeada para: 020.nrrd\n",
            "  -> R√ìTULO encontrado e renomeado para: 020.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 019\n",
            "  -> IMAGEM encontrada e renomeada para: 019.nrrd\n",
            "  -> R√ìTULO encontrado e renomeado para: 019.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 028\n",
            "  -> R√ìTULO encontrado e renomeado para: 028.nrrd\n",
            "  -> IMAGEM encontrada e renomeada para: 028.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 027\n",
            "  -> IMAGEM encontrada e renomeada para: 027.nrrd\n",
            "  -> R√ìTULO encontrado e renomeado para: 027.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 021\n",
            "  -> IMAGEM encontrada e renomeada para: 021.nrrd\n",
            "  -> R√ìTULO encontrado e renomeado para: 021.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 026\n",
            "  -> IMAGEM encontrada e renomeada para: 026.nrrd\n",
            "  -> R√ìTULO encontrado e renomeado para: 026.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 024\n",
            "  -> IMAGEM encontrada e renomeada para: 024.nrrd\n",
            "  -> R√ìTULO encontrado e renomeado para: 024.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 023\n",
            "  -> IMAGEM encontrada e renomeada para: 023.nrrd\n",
            "  -> R√ìTULO encontrado e renomeado para: 023.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 025\n",
            "  -> IMAGEM encontrada e renomeada para: 025.nrrd\n",
            "  -> R√ìTULO encontrado e renomeado para: 025.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 022\n",
            "  -> IMAGEM encontrada e renomeada para: 022.nrrd\n",
            "  -> R√ìTULO encontrado e renomeado para: 022.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 029\n",
            "  -> IMAGEM encontrada e renomeada para: 029.nrrd\n",
            "  -> R√ìTULO encontrado e renomeado para: 029.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 030\n",
            "  -> R√ìTULO encontrado e renomeado para: 030.nrrd\n",
            "  -> IMAGEM encontrada e renomeada para: 030.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 032\n",
            "  -> IMAGEM encontrada e renomeada para: 032.nrrd\n",
            "  -> R√ìTULO encontrado e renomeado para: 032.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 031\n",
            "  -> R√ìTULO encontrado e renomeado para: 031.nrrd\n",
            "  -> IMAGEM encontrada e renomeada para: 031.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 034\n",
            "  -> IMAGEM encontrada e renomeada para: 034.nrrd\n",
            "  -> R√ìTULO encontrado e renomeado para: 034.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 040\n",
            "  -> IMAGEM encontrada e renomeada para: 040.nrrd\n",
            "  -> R√ìTULO encontrado e renomeado para: 040.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 033\n",
            "  -> R√ìTULO encontrado e renomeado para: 033.nrrd\n",
            "  -> IMAGEM encontrada e renomeada para: 033.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 036\n",
            "  -> R√ìTULO encontrado e renomeado para: 036.nrrd\n",
            "  -> IMAGEM encontrada e renomeada para: 036.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 035\n",
            "  -> IMAGEM encontrada e renomeada para: 035.nrrd\n",
            "  -> R√ìTULO encontrado e renomeado para: 035.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 037\n",
            "  -> IMAGEM encontrada e renomeada para: 037.nrrd\n",
            "  -> R√ìTULO encontrado e renomeado para: 037.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 038\n",
            "  -> IMAGEM encontrada e renomeada para: 038.nrrd\n",
            "  -> R√ìTULO encontrado e renomeado para: 038.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 039\n",
            "  -> IMAGEM encontrada e renomeada para: 039.nrrd\n",
            "  -> R√ìTULO encontrado e renomeado para: 039.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 042\n",
            "  -> IMAGEM encontrada e renomeada para: 042.nrrd\n",
            "  -> R√ìTULO encontrado e renomeado para: 042.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 041\n",
            "  -> IMAGEM encontrada e renomeada para: 041.nrrd\n",
            "  -> R√ìTULO encontrado e renomeado para: 041.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 043\n",
            "  -> R√ìTULO encontrado e renomeado para: 043.nrrd\n",
            "  -> IMAGEM encontrada e renomeada para: 043.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 044\n",
            "  -> IMAGEM encontrada e renomeada para: 044.nrrd\n",
            "  -> R√ìTULO encontrado e renomeado para: 044.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 051\n",
            "  -> R√ìTULO encontrado e renomeado para: 051.nrrd\n",
            "  -> IMAGEM encontrada e renomeada para: 051.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 048\n",
            "  -> IMAGEM encontrada e renomeada para: 048.nrrd\n",
            "  -> R√ìTULO encontrado e renomeado para: 048.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 049\n",
            "  -> R√ìTULO encontrado e renomeado para: 049.nrrd\n",
            "  -> IMAGEM encontrada e renomeada para: 049.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 045\n",
            "  -> R√ìTULO encontrado e renomeado para: 045.nrrd\n",
            "  -> IMAGEM encontrada e renomeada para: 045.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 050\n",
            "  -> IMAGEM encontrada e renomeada para: 050.nrrd\n",
            "  -> R√ìTULO encontrado e renomeado para: 050.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 046\n",
            "  -> R√ìTULO encontrado e renomeado para: 046.nrrd\n",
            "  -> IMAGEM encontrada e renomeada para: 046.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 052\n",
            "  -> IMAGEM encontrada e renomeada para: 052.nrrd\n",
            "  -> R√ìTULO encontrado e renomeado para: 052.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 047\n",
            "  -> R√ìTULO encontrado e renomeado para: 047.nrrd\n",
            "  -> IMAGEM encontrada e renomeada para: 047.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 054\n",
            "  -> IMAGEM encontrada e renomeada para: 054.nrrd\n",
            "  -> R√ìTULO encontrado e renomeado para: 054.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 053\n",
            "  -> R√ìTULO encontrado e renomeado para: 053.nrrd\n",
            "  -> IMAGEM encontrada e renomeada para: 053.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 056\n",
            "  -> R√ìTULO encontrado e renomeado para: 056.nrrd\n",
            "  -> IMAGEM encontrada e renomeada para: 056.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 055\n",
            "  -> R√ìTULO encontrado e renomeado para: 055.nrrd\n",
            "  -> IMAGEM encontrada e renomeada para: 055.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 063\n",
            "  -> IMAGEM encontrada e renomeada para: 063.nrrd\n",
            "  -> R√ìTULO encontrado e renomeado para: 063.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 058\n",
            "  -> IMAGEM encontrada e renomeada para: 058.nrrd\n",
            "  -> R√ìTULO encontrado e renomeado para: 058.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 061\n",
            "  -> IMAGEM encontrada e renomeada para: 061.nrrd\n",
            "  -> R√ìTULO encontrado e renomeado para: 061.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 057\n",
            "  -> R√ìTULO encontrado e renomeado para: 057.nrrd\n",
            "  -> IMAGEM encontrada e renomeada para: 057.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 062\n",
            "  -> IMAGEM encontrada e renomeada para: 062.nrrd\n",
            "  -> R√ìTULO encontrado e renomeado para: 062.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 060\n",
            "  -> IMAGEM encontrada e renomeada para: 060.nrrd\n",
            "  -> R√ìTULO encontrado e renomeado para: 060.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 059\n",
            "  -> IMAGEM encontrada e renomeada para: 059.nrrd\n",
            "  -> R√ìTULO encontrado e renomeado para: 059.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 064\n",
            "  -> R√ìTULO encontrado e renomeado para: 064.nrrd\n",
            "  -> IMAGEM encontrada e renomeada para: 064.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 065\n",
            "  -> IMAGEM encontrada e renomeada para: 065.nrrd\n",
            "  -> R√ìTULO encontrado e renomeado para: 065.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 066\n",
            "  -> R√ìTULO encontrado e renomeado para: 066.nrrd\n",
            "  -> IMAGEM encontrada e renomeada para: 066.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 067\n",
            "  -> IMAGEM encontrada e renomeada para: 067.nrrd\n",
            "  -> R√ìTULO encontrado e renomeado para: 067.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 068\n",
            "  -> R√ìTULO encontrado e renomeado para: 068.nrrd\n",
            "  -> IMAGEM encontrada e renomeada para: 068.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 076\n",
            "  -> R√ìTULO encontrado e renomeado para: 076.nrrd\n",
            "  -> IMAGEM encontrada e renomeada para: 076.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 075\n",
            "  -> IMAGEM encontrada e renomeada para: 075.nrrd\n",
            "  -> R√ìTULO encontrado e renomeado para: 075.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 071\n",
            "  -> R√ìTULO encontrado e renomeado para: 071.nrrd\n",
            "  -> IMAGEM encontrada e renomeada para: 071.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 072\n",
            "  -> IMAGEM encontrada e renomeada para: 072.nrrd\n",
            "  -> R√ìTULO encontrado e renomeado para: 072.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 070\n",
            "  -> R√ìTULO encontrado e renomeado para: 070.nrrd\n",
            "  -> IMAGEM encontrada e renomeada para: 070.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 073\n",
            "  -> IMAGEM encontrada e renomeada para: 073.nrrd\n",
            "  -> R√ìTULO encontrado e renomeado para: 073.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 074\n",
            "  -> IMAGEM encontrada e renomeada para: 074.nrrd\n",
            "  -> R√ìTULO encontrado e renomeado para: 074.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 069\n",
            "  -> IMAGEM encontrada e renomeada para: 069.nrrd\n",
            "  -> R√ìTULO encontrado e renomeado para: 069.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 078\n",
            "  -> IMAGEM encontrada e renomeada para: 078.nrrd\n",
            "  -> R√ìTULO encontrado e renomeado para: 078.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 077\n",
            "  -> R√ìTULO encontrado e renomeado para: 077.nrrd\n",
            "  -> IMAGEM encontrada e renomeada para: 077.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 080\n",
            "  -> IMAGEM encontrada e renomeada para: 080.nrrd\n",
            "  -> R√ìTULO encontrado e renomeado para: 080.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 079\n",
            "  -> IMAGEM encontrada e renomeada para: 079.nrrd\n",
            "  -> R√ìTULO encontrado e renomeado para: 079.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 082\n",
            "  -> R√ìTULO encontrado e renomeado para: 082.nrrd\n",
            "  -> IMAGEM encontrada e renomeada para: 082.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 081\n",
            "  -> R√ìTULO encontrado e renomeado para: 081.nrrd\n",
            "  -> IMAGEM encontrada e renomeada para: 081.nrrd\n",
            "\n",
            "\n",
            "Processo de c√≥pia e renomea√ß√£o conclu√≠do!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "üìÅ C√©lula 6 ‚Äî Gerar dataset.json automaticamente e estruturar o dataset para nnU-Net"
      ],
      "metadata": {
        "id": "Tl5IlrYfNvXU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úÖ Prepara√ß√£o COMPLETA do dataset nnUNetv2 com limpeza e estrutura correta\n",
        "!pip install -q SimpleITK\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "import json\n",
        "import SimpleITK as sitk\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ============================================================================\n",
        "# 1. CONFIGURA√á√ÉO DE DIRET√ìRIOS E LIMPEZA DOS DIRET√ìRIOS DE DESTINO\n",
        "# ============================================================================\n",
        "\n",
        "DATASET_NAME = \"AorticValve\"\n",
        "DATASET_ID = \"001\"\n",
        "BASE_DIR = f\"/content/drive/MyDrive/nnunet_data/nnUNet_raw/Dataset{DATASET_ID}_{DATASET_NAME}\"\n",
        "\n",
        "SOURCE_IMAGES_DIR = os.path.join(BASE_DIR, \"originais\", \"imagens\")\n",
        "SOURCE_LABELS_DIR = os.path.join(BASE_DIR, \"originais\", \"rotulos\")\n",
        "\n",
        "IMAGES_TR_DIR = os.path.join(BASE_DIR, \"imagesTr\")\n",
        "LABELS_TR_DIR = os.path.join(BASE_DIR, \"labelsTr\")\n",
        "IMAGES_TS_DIR = os.path.join(BASE_DIR, \"imagesTs\")\n",
        "LABELS_TS_DIR = os.path.join(BASE_DIR, \"labelsTs\")  # Pasta para r√≥tulos do conjunto de teste (para avalia√ß√£o)\n",
        "\n",
        "# üîÑ LIMPEZA DOS DIRET√ìRIOS nnU-Net\n",
        "print(\"üßπ Limpando e recriando diret√≥rios de destino nnU-Net...\")\n",
        "for path in [IMAGES_TR_DIR, LABELS_TR_DIR, IMAGES_TS_DIR, LABELS_TS_DIR]:\n",
        "    if os.path.exists(path): # Remove o diret√≥rio se j√° existir\n",
        "        shutil.rmtree(path)\n",
        "    os.makedirs(path, exist_ok=True) # Cria o diret√≥rio limpo\n",
        "\n",
        "# ============================================================================\n",
        "# 2. FUN√á√ÉO DE CONVERS√ÉO CORRETA PARA .nii.gz COM SimpleITK\n",
        "# ============================================================================\n",
        "\n",
        "def convert_and_save_gzipped(source_path, dest_path):\n",
        "    if not os.path.exists(source_path):\n",
        "        print(f\"  ‚ö†Ô∏è Arquivo n√£o encontrado: {os.path.basename(source_path)}. Pulando a convers√£o.\")\n",
        "        return False\n",
        "    try:\n",
        "        img = sitk.ReadImage(source_path)\n",
        "        sitk.WriteImage(img, dest_path, useCompression=True)\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"  ‚ùå Erro ao converter {os.path.basename(source_path)} para NIfTI: {e}\")\n",
        "        return False\n",
        "\n",
        "# ============================================================================\n",
        "# 3. CONVERS√ÉO DAS IMAGENS E DIVIS√ÉO TREINO/TESTE\n",
        "# ============================================================================\n",
        "\n",
        "print(\"üîé Lendo arquivos de origem...\")\n",
        "image_files_unsorted = [f for f in os.listdir(SOURCE_IMAGES_DIR) if not f.startswith('.')]\n",
        "label_files_unsorted = [f for f in os.listdir(SOURCE_LABELS_DIR) if not f.startswith('.')]\n",
        "\n",
        "# Cria uma lista de tuplas (id_paciente, nome_arquivo_imagem, nome_arquivo_label)\n",
        "paired_files = []\n",
        "for img_fname in image_files_unsorted:\n",
        "    base_name = img_fname.split('.')[0] # Assume que o nome base √© o ID do paciente\n",
        "    label_fname = f\"{base_name}.nrrd\"\n",
        "    if label_fname in label_files_unsorted:\n",
        "        paired_files.append((base_name, img_fname, label_fname))\n",
        "    else:\n",
        "        print(f\"  AVISO: R√≥tulo '{label_fname}' n√£o encontrado para a imagem '{img_fname}'. Ignorando este par.\")\n",
        "\n",
        "paired_files.sort(key=lambda x: x[0]) # Ordena para reprodutibilidade\n",
        "\n",
        "num_total_valid_files = len(paired_files)\n",
        "if num_total_valid_files == 0:\n",
        "    print(\"üö® ERRO: Nenhum par de imagem/r√≥tulo v√°lido encontrado nos diret√≥rios de origem. Certifique-se de que a C√©lula 5 foi executada corretamente.\")\n",
        "    exit() # Sai do script se n√£o houver dados\n",
        "\n",
        "num_train_desired = min(80, num_total_valid_files)\n",
        "num_test_desired = num_total_valid_files - num_train_desired # O restante vai para teste\n",
        "\n",
        "num_train_files = 0\n",
        "num_test_files = 0\n",
        "train_metadata = []\n",
        "test_metadata = []\n",
        "\n",
        "print(f\"üöÄ Processando {num_train_desired} casos para treino...\")\n",
        "for i in tqdm(range(num_train_desired)):\n",
        "    base_name, original_image_fname, original_label_fname = paired_files[i]\n",
        "    original_image_path = os.path.join(SOURCE_IMAGES_DIR, original_image_fname)\n",
        "    original_label_path = os.path.join(SOURCE_LABELS_DIR, original_label_fname)\n",
        "\n",
        "    case_id_nnunet = f\"{DATASET_NAME}_{base_name}\" # ID no formato nnU-Net\n",
        "\n",
        "    image_out_path = os.path.join(IMAGES_TR_DIR, f\"{case_id_nnunet}_0000.nii.gz\")\n",
        "    label_out_path = os.path.join(LABELS_TR_DIR, f\"{case_id_nnunet}.nii.gz\")\n",
        "\n",
        "    img_ok = convert_and_save_gzipped(original_image_path, image_out_path)\n",
        "    lbl_ok = convert_and_save_gzipped(original_label_path, label_out_path)\n",
        "\n",
        "    if img_ok and lbl_ok:\n",
        "        num_train_files += 1\n",
        "        train_metadata.append({\n",
        "            \"image\": f\"./imagesTr/{case_id_nnunet}_0000.nii.gz\",\n",
        "            \"label\": f\"./labelsTr/{case_id_nnunet}.nii.gz\"\n",
        "        })\n",
        "    else:\n",
        "        print(f\"  AVISO: Caso {base_name} n√£o foi inclu√≠do no treino devido a erro na convers√£o.\")\n",
        "\n",
        "print(f\"\\nüß™ Processando {num_test_desired} casos para teste...\")\n",
        "for i in tqdm(range(num_train_desired, num_total_valid_files)):\n",
        "    base_name, original_image_fname, original_label_fname = paired_files[i]\n",
        "    original_image_path = os.path.join(SOURCE_IMAGES_DIR, original_image_fname)\n",
        "    original_label_path = os.path.join(SOURCE_LABELS_DIR, original_label_fname)\n",
        "\n",
        "    case_id_nnunet = f\"{DATASET_NAME}_{base_name}\"\n",
        "\n",
        "    image_out_path = os.path.join(IMAGES_TS_DIR, f\"{case_id_nnunet}_0000.nii.gz\")\n",
        "    label_out_path = os.path.join(LABELS_TS_DIR, f\"{case_id_nnunet}.nii.gz\") # Copiar label para labelsTs para avalia√ß√£o posterior\n",
        "\n",
        "    img_ok = convert_and_save_gzipped(original_image_path, image_out_path)\n",
        "    lbl_ok = convert_and_save_gzipped(original_label_path, label_out_path)\n",
        "\n",
        "    if img_ok:\n",
        "        num_test_files += 1\n",
        "        test_metadata.append(f\"./imagesTs/{case_id_nnunet}_0000.nii.gz\")\n",
        "    else:\n",
        "        print(f\"  AVISO: Caso {base_name} n√£o foi inclu√≠do no teste devido a erro na convers√£o.\")\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# 4. GERAR dataset.json COMPLETO (VERS√ÉO FINAL nnUNetv2)\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\nüìù Gerando dataset.json...\")\n",
        "\n",
        "dataset_info = {\n",
        "    \"channel_names\": {\n",
        "        \"0\": \"CT\" # Nome do canal de entrada (CT, MR, etc.)\n",
        "    },\n",
        "    \"labels\": {\n",
        "        \"background\": \"0\",\n",
        "        \"aortic_valve\": \"1\" # Mapeamento do r√≥tulo 1 para 'aortic_valve'\n",
        "    },\n",
        "    \"modality\": {\n",
        "        \"0\": \"CT\"\n",
        "    },\n",
        "    \"file_ending\": \".nii.gz\",\n",
        "    \"numTraining\": num_train_files,\n",
        "    \"training\": train_metadata,\n",
        "    \"test\": test_metadata,\n",
        "    \"name\": DATASET_NAME, # Nome do dataset\n",
        "    \"description\": \"Segmenta√ß√£o da V√°lvula A√≥rtica em Tomografia Computadorizada\",\n",
        "    \"reference\": \"N/A\",\n",
        "    \"licence\": \"N/A\",\n",
        "    \"release\": \"0.0\"\n",
        "}\n",
        "\n",
        "with open(os.path.join(BASE_DIR, \"dataset.json\"), \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(dataset_info, f, indent=4)\n",
        "\n",
        "print(\"\\n‚úÖ Dataset estruturado com sucesso!\")\n",
        "print(f\"‚úîÔ∏è Treino: {num_train_files} casos\")\n",
        "print(f\"‚úîÔ∏è Teste : {num_test_files} casos (com labelsTs para avalia√ß√£o)\")\n",
        "print(f\"üìÅ Arquivo JSON: {os.path.join(BASE_DIR, 'dataset.json')}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DQxQblz1Nx-a",
        "outputId": "d170c6fd-79c6-466c-a730-8899d9935777"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üßπ Limpando e recriando diret√≥rios de destino nnU-Net...\n",
            "üîé Lendo arquivos de origem...\n",
            "üöÄ Processando 80 casos para treino...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 80/80 [01:10<00:00,  1.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üß™ Processando 20 casos para teste...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:17<00:00,  1.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìù Gerando dataset.json...\n",
            "\n",
            "‚úÖ Dataset estruturado com sucesso!\n",
            "‚úîÔ∏è Treino: 80 casos\n",
            "‚úîÔ∏è Teste : 20 casos (com labelsTs para avalia√ß√£o)\n",
            "üìÅ Arquivo JSON: /content/drive/MyDrive/nnunet_data/nnUNet_raw/Dataset001_AorticValve/dataset.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úÖ ADICIONAR NOVA C√âLULA 6B - Augmentation Avan√ßada:\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "import numpy as np\n",
        "import SimpleITK as sitk\n",
        "from scipy.ndimage import rotate, gaussian_filter\n",
        "\n",
        "def advanced_augmentation(image_path, label_path, output_dir_img, output_dir_lbl, base_name):\n",
        "    \"\"\"Aplica augmentations avan√ßadas espec√≠ficas para v√°lvula a√≥rtica\"\"\"\n",
        "\n",
        "    # Carregar imagem e label\n",
        "    img = sitk.ReadImage(image_path)\n",
        "    lbl = sitk.ReadImage(label_path)\n",
        "\n",
        "    img_array = sitk.GetArrayFromImage(img)\n",
        "    lbl_array = sitk.GetArrayFromImage(lbl)\n",
        "\n",
        "    augmented_pairs = []\n",
        "\n",
        "    # 1. Rota√ß√µes pequenas (v√°lvula a√≥rtica √© sens√≠vel a rota√ß√£o)\n",
        "    for angle in [-5, 5, -10, 10]:\n",
        "        img_rot = rotate(img_array, angle, axes=(1, 2), reshape=False, order=1)\n",
        "        lbl_rot = rotate(lbl_array, angle, axes=(1, 2), reshape=False, order=0)\n",
        "        augmented_pairs.append((img_rot, lbl_rot, f\"{base_name}_rot{angle}\"))\n",
        "\n",
        "    # 2. Flip horizontal/vertical\n",
        "    img_flip_h = np.flip(img_array, axis=2)\n",
        "    lbl_flip_h = np.flip(lbl_array, axis=2)\n",
        "    augmented_pairs.append((img_flip_h, lbl_flip_h, f\"{base_name}_flipH\"))\n",
        "\n",
        "    # 3. Gaussian noise (simula ru√≠do de CT)\n",
        "    noise_std = np.std(img_array) * 0.05\n",
        "    img_noise = img_array + np.random.normal(0, noise_std, img_array.shape)\n",
        "    augmented_pairs.append((img_noise, lbl_array, f\"{base_name}_noise\"))\n",
        "\n",
        "    # 4. Contraste ajustado (importante para CT)\n",
        "    img_contrast = np.clip(img_array * 1.2 - 50, img_array.min(), img_array.max())\n",
        "    augmented_pairs.append((img_contrast, lbl_array, f\"{base_name}_contrast\"))\n",
        "\n",
        "    # Salvar todas as vers√µes aumentadas\n",
        "    for img_aug, lbl_aug, suffix in augmented_pairs:\n",
        "        # Criar novas imagens SimpleITK\n",
        "        img_aug_sitk = sitk.GetImageFromArray(img_aug)\n",
        "        lbl_aug_sitk = sitk.GetImageFromArray(lbl_aug.astype(np.uint8))\n",
        "\n",
        "        # Copiar metadados originais\n",
        "        img_aug_sitk.CopyInformation(img)\n",
        "        lbl_aug_sitk.CopyInformation(lbl)\n",
        "\n",
        "        # Salvar\n",
        "        sitk.WriteImage(img_aug_sitk, os.path.join(output_dir_img, f\"{suffix}_0000.nii.gz\"), useCompression=True)\n",
        "        sitk.WriteImage(lbl_aug_sitk, os.path.join(output_dir_lbl, f\"{suffix}.nii.gz\"), useCompression=True)\n",
        "\n",
        "# Aplicar augmentation apenas aos dados de treino\n",
        "print(\"üîÑ Aplicando data augmentation avan√ßada...\")\n",
        "\n",
        "SOURCE_IMAGES_DIR = os.path.join(BASE_DIR, \"originais\", \"imagens\")\n",
        "SOURCE_LABELS_DIR = os.path.join(BASE_DIR, \"originais\", \"rotulos\")\n",
        "\n",
        "# Pegar apenas os primeiros 80% para treino (mesma l√≥gica da c√©lula original)\n",
        "image_files = sorted([f for f in os.listdir(SOURCE_IMAGES_DIR) if not f.startswith('.')])\n",
        "num_train = min(80, len(image_files))\n",
        "\n",
        "for i in range(num_train):\n",
        "    fname = image_files[i]\n",
        "    base_name = fname.split('.')[0]\n",
        "\n",
        "    img_path = os.path.join(SOURCE_IMAGES_DIR, fname)\n",
        "    lbl_path = os.path.join(SOURCE_LABELS_DIR, fname)\n",
        "\n",
        "    if os.path.exists(img_path) and os.path.exists(lbl_path):\n",
        "        try:\n",
        "            advanced_augmentation(img_path, lbl_path, IMAGES_TR_DIR, LABELS_TR_DIR, f\"AorticValve_{base_name}\")\n",
        "            print(f\"  ‚úì Augmentado: {base_name}\")\n",
        "        except Exception as e:\n",
        "            print(f\"  ‚ùå Erro no {base_name}: {e}\")\n",
        "\n",
        "print(\"‚úÖ Data augmentation conclu√≠da!\")"
      ],
      "metadata": {
        "id": "YvcVy2qnbnjL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üîÅ C√âLULA 7 ‚Äî Prepara√ß√£o e Treinamento do nnU-Netv2"
      ],
      "metadata": {
        "id": "Q_vWnbw3PAY1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úÖ Definindo os paths globais do nnUNetv2\n",
        "import os\n",
        "import torch # Necess√°rio para configurar vari√°veis de ambiente PyTorch\n",
        "\n",
        "os.environ[\"nnUNet_raw\"] = \"/content/drive/MyDrive/nnunet_data/nnUNet_raw\"\n",
        "os.environ[\"nnUNet_preprocessed\"] = \"/content/drive/MyDrive/nnunet_data/nnUNet_preprocessed\"\n",
        "os.environ[\"nnUNet_results\"] = \"/content/drive/MyDrive/nnunet_data/nnUNet_results\"\n",
        "\n",
        "# =========================================================================\n",
        "# SOLU√á√ÉO PARA OUT OF MEMORY (OOM):\n",
        "# 1. Tentar aloca√ß√£o de mem√≥ria expans√≠vel (Pytorch)\n",
        "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
        "# 2. Se o erro de OOM persistir, voc√™ DEVE reduzir o batch_size e/ou patch_size.\n",
        "#    Para fazer isso, execute a c√©lula abaixo para gerar os planos (se ainda n√£o fez).\n",
        "#    Em seguida, V√Å MANUALMENTE ao arquivo:\n",
        "#    /content/drive/MyDrive/nnunet_data/nnUNet_preprocessed/Dataset001_AorticValve/nnUNetResEncUNetLPlans.json\n",
        "#    Edite a se√ß√£o '3d_fullres' e altere:\n",
        "#    \"batch_size\": 2  ->  \"batch_size\": 1  (ou menos, se necess√°rio)\n",
        "#    \"patch_size\": [32, 384, 384] -> [24, 320, 320] (ou tamanhos menores, se necess√°rio)\n",
        "#    AP√ìS A EDI√á√ÉO, N√ÉO EXECUTE NOVAMENTE a etapa de 'plan_and_preprocess' pois ela sobrescrever√° suas altera√ß√µes!\n",
        "# =========================================================================\n",
        "\n",
        "print(\"--- Etapa de Planejamento e Pr√©-processamento ---\")\n",
        "print(\"Isso gerar√° os planos de treinamento e pr√©-processar√° os dados.\")\n",
        "print(\"Se voc√™ j√° editou o arquivo de planos manualmente (para OOM), N√ÉO execute esta linha novamente.\")\n",
        "\n",
        "# Gerar planos e pr√©-processar o dataset\n",
        "# -d 001: ID do dataset\n",
        "# -pl nnUNetPlannerResEncL: Usa o plano de arquitetura Residual Encoder UNet L\n",
        "!nnUNetv2_plan_and_preprocess -d 001 -pl nnUNetPlannerResEncL\n",
        "\n",
        "print(\"\\n--- In√≠cio do Treinamento nnUNetv2 ---\")\n",
        "print(\"Certifique-se de que o arquivo nnUNetResEncUNetLPlans.json foi ajustado se encontrou OutOfMemoryError.\")\n",
        "\n",
        "# Iniciar o treinamento\n",
        "# 001: ID do dataset\n",
        "# 3d_fullres: Configura√ß√£o de treinamento (3D full resolution)\n",
        "# 0: Fold de valida√ß√£o cruzada (usando a fold 0). Para treinamento completo, idealmente, voc√™ treinaria 5 folds.\n",
        "# -p nnUNetResEncUNetLPlans: Especifica o plano de treinamento a ser usado\n",
        "!nnUNetv2_train 001 3d_fullres 0 -p nnUNetResEncUNetLPlans\n",
        "\n",
        "print(\"\\nTreinamento conclu√≠do (ou falhou devido a OOM - verifique a sa√≠da acima)!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m9nA2By7PKBc",
        "outputId": "b38ce231-7cd8-4599-d032-595b6da39fc8"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Etapa de Planejamento e Pr√©-processamento ---\n",
            "Isso gerar√° os planos de treinamento e pr√©-processar√° os dados.\n",
            "Se voc√™ j√° editou o arquivo de planos manualmente (para OOM), N√ÉO execute esta linha novamente.\n",
            "Fingerprint extraction...\n",
            "Dataset001_AorticValve\n",
            "Experiment planning...\n",
            "Dropping 3d_lowres config because the image size difference to 3d_fullres is too small. 3d_fullres: [ 50. 512. 512.], 3d_lowres: [50, 512, 512]\n",
            "2D U-Net configuration:\n",
            "{'data_identifier': 'nnUNetPlans_2d', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 35, 'patch_size': (np.int64(512), np.int64(512)), 'median_image_size_in_voxels': array([512., 512.]), 'spacing': array([0.35058594, 0.35058594]), 'normalization_schemes': ['CTNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.ResidualEncoderUNet', 'arch_kwargs': {'n_stages': 8, 'features_per_stage': (32, 64, 128, 256, 512, 512, 512, 512), 'conv_op': 'torch.nn.modules.conv.Conv2d', 'kernel_sizes': ((3, 3), (3, 3), (3, 3), (3, 3), (3, 3), (3, 3), (3, 3), (3, 3)), 'strides': ((1, 1), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2)), 'n_blocks_per_stage': (1, 3, 4, 6, 6, 6, 6, 6), 'n_conv_per_stage_decoder': (1, 1, 1, 1, 1, 1, 1), 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm2d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}}, '_kw_requires_import': ('conv_op', 'norm_op', 'dropout_op', 'nonlin')}, 'batch_dice': True}\n",
            "\n",
            "Using <class 'nnunetv2.imageio.simpleitk_reader_writer.SimpleITKIO'> as reader/writer\n",
            "3D fullres U-Net configuration:\n",
            "{'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 2, 'patch_size': (np.int64(32), np.int64(384), np.int64(384)), 'median_image_size_in_voxels': array([ 50., 512., 512.]), 'spacing': array([2.5       , 0.35058594, 0.35058594]), 'normalization_schemes': ['CTNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.ResidualEncoderUNet', 'arch_kwargs': {'n_stages': 7, 'features_per_stage': (32, 64, 128, 256, 320, 320, 320), 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': ((1, 3, 3), (1, 3, 3), (3, 3, 3), (3, 3, 3), (3, 3, 3), (3, 3, 3), (3, 3, 3)), 'strides': ((1, 1, 1), (1, 2, 2), (1, 2, 2), (2, 2, 2), (2, 2, 2), (2, 2, 2), (1, 2, 2)), 'n_blocks_per_stage': (1, 3, 4, 6, 6, 6, 6), 'n_conv_per_stage_decoder': (1, 1, 1, 1, 1, 1), 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}}, '_kw_requires_import': ('conv_op', 'norm_op', 'dropout_op', 'nonlin')}, 'batch_dice': False}\n",
            "\n",
            "Plans were saved to /content/drive/MyDrive/nnunet_data/nnUNet_preprocessed/Dataset001_AorticValve/nnUNetResEncUNetLPlans.json\n",
            "Preprocessing...\n",
            "Preprocessing dataset Dataset001_AorticValve\n",
            "Configuration: 2d...\n",
            "100% 80/80 [04:21<00:00,  3.27s/it]\n",
            "Configuration: 3d_fullres...\n",
            "100% 80/80 [05:05<00:00,  3.82s/it]\n",
            "Configuration: 3d_lowres...\n",
            "INFO: Configuration 3d_lowres not found in plans file nnUNetResEncUNetLPlans.json of dataset Dataset001_AorticValve. Skipping.\n",
            "\n",
            "--- In√≠cio do Treinamento nnUNetv2 ---\n",
            "Certifique-se de que o arquivo nnUNetResEncUNetLPlans.json foi ajustado se encontrou OutOfMemoryError.\n",
            "Using device: cuda:0\n",
            "\n",
            "#######################################################################\n",
            "Please cite the following paper when using nnU-Net:\n",
            "Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.\n",
            "#######################################################################\n",
            "\n",
            "2025-07-16 16:17:31.375121: Using torch.compile...\n",
            "2025-07-16 16:17:33.231574: do_dummy_2d_data_aug: True\n",
            "2025-07-16 16:17:33.237743: Using splits from existing split file: /content/drive/MyDrive/nnunet_data/nnUNet_preprocessed/Dataset001_AorticValve/splits_final.json\n",
            "2025-07-16 16:17:33.947770: The split file contains 5 splits.\n",
            "2025-07-16 16:17:33.963364: Desired fold for training: 0\n",
            "2025-07-16 16:17:33.966480: This split has 64 training and 16 validation cases.\n",
            "using pin_memory on device 0\n",
            "using pin_memory on device 0\n",
            "\n",
            "This is the configuration used by this training:\n",
            "Configuration name: 3d_fullres\n",
            " {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 2, 'patch_size': [32, 384, 384], 'median_image_size_in_voxels': [50.0, 512.0, 512.0], 'spacing': [2.5, 0.3505859375, 0.3505859375], 'normalization_schemes': ['CTNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.ResidualEncoderUNet', 'arch_kwargs': {'n_stages': 7, 'features_per_stage': [32, 64, 128, 256, 320, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[1, 3, 3], [1, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [1, 2, 2], [1, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_blocks_per_stage': [1, 3, 4, 6, 6, 6, 6], 'n_conv_per_stage_decoder': [1, 1, 1, 1, 1, 1], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': False} \n",
            "\n",
            "These are the global plan.json settings:\n",
            " {'dataset_name': 'Dataset001_AorticValve', 'plans_name': 'nnUNetResEncUNetLPlans', 'original_median_spacing_after_transp': [2.5000005960464478, 0.3505859375, 0.3505859375], 'original_median_shape_after_transp': [49, 512, 512], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'nnUNetPlannerResEncL', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 991.0, 'mean': 32.83921813964844, 'median': 35.0, 'min': -210.0, 'percentile_00_5': -82.0, 'percentile_99_5': 105.0, 'std': 29.420087814331055}}} \n",
            "\n",
            "2025-07-16 16:17:39.766026: Unable to plot network architecture: nnUNet_compile is enabled!\n",
            "2025-07-16 16:17:40.515191: \n",
            "2025-07-16 16:17:40.525194: Epoch 0\n",
            "2025-07-16 16:17:40.541355: Current learning rate: 0.01\n",
            "W0716 16:18:14.162000 81786 torch/_inductor/utils.py:1137] [0/0] Not enough SMs to use max_autotune_gemm mode\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/nnUNetv2_train\", line 8, in <module>\n",
            "    sys.exit(run_training_entry())\n",
            "             ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/nnunetv2/run/run_training.py\", line 266, in run_training_entry\n",
            "    run_training(args.dataset_name_or_id, args.configuration, args.fold, args.tr, args.p, args.pretrained_weights,\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/nnunetv2/run/run_training.py\", line 207, in run_training\n",
            "    nnunet_trainer.run_training()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/nnunetv2/training/nnUNetTrainer/nnUNetTrainer.py\", line 1371, in run_training\n",
            "    train_outputs.append(self.train_step(next(self.dataloader_train)))\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/nnunetv2/training/nnUNetTrainer/nnUNetTrainer.py\", line 989, in train_step\n",
            "    output = self.network(data)\n",
            "             ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py\", line 574, in _fn\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/dynamic_network_architectures/architectures/unet.py\", line 179, in forward\n",
            "    def forward(self, x):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py\", line 745, in _fn\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/_functorch/aot_autograd.py\", line 1184, in forward\n",
            "    return compiled_fn(full_args)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py\", line 310, in runtime_wrapper\n",
            "    all_outs = call_func_at_runtime_with_args(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/_functorch/_aot_autograd/utils.py\", line 126, in call_func_at_runtime_with_args\n",
            "    out = normalize_as_list(f(args))\n",
            "                            ^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/_functorch/_aot_autograd/utils.py\", line 100, in g\n",
            "    return f(*args)\n",
            "           ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/autograd/function.py\", line 575, in apply\n",
            "    return super().apply(*args, **kwargs)  # type: ignore[misc]\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py\", line 1585, in forward\n",
            "    fw_outs = call_func_at_runtime_with_args(\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/_functorch/_aot_autograd/utils.py\", line 126, in call_func_at_runtime_with_args\n",
            "    out = normalize_as_list(f(args))\n",
            "                            ^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py\", line 490, in wrapper\n",
            "    return compiled_fn(runtime_args)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py\", line 672, in inner_fn\n",
            "    outs = compiled_fn(args)\n",
            "           ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/_inductor/output_code.py\", line 466, in __call__\n",
            "    return self.current_callable(inputs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/_inductor/utils.py\", line 2128, in run\n",
            "    return model(new_inputs)\n",
            "           ^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/torchinductor_root/kr/ckrtimwlx4cfhfd4xxa3gylvav4c5urn7nca7n4wjamc5tvkhucr.py\", line 6548, in call\n",
            "    buf855 = empty_strided_cuda((2, 64, 32, 384, 384), (301989888, 4718592, 147456, 384, 1), torch.float16)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.12 GiB. GPU 0 has a total capacity of 14.74 GiB of which 632.12 MiB is free. Process 196324 has 14.12 GiB memory in use. Of the allocated memory 13.98 GiB is allocated by PyTorch, and 19.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Exception in thread Thread-2 (results_loop):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.11/threading.py\", line 1045, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.11/threading.py\", line 982, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/batchgenerators/dataloading/nondet_multi_threaded_augmenter.py\", line 125, in results_loop\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/batchgenerators/dataloading/nondet_multi_threaded_augmenter.py\", line 103, in results_loop\n",
            "    raise RuntimeError(\"One or more background workers are no longer alive. Exiting. Please check the \"\n",
            "RuntimeError: One or more background workers are no longer alive. Exiting. Please check the print statements above for the actual error message\n",
            "Exception in thread Thread-3 (results_loop):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.11/threading.py\", line 1045, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.11/threading.py\", line 982, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/batchgenerators/dataloading/nondet_multi_threaded_augmenter.py\", line 125, in results_loop\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/batchgenerators/dataloading/nondet_multi_threaded_augmenter.py\", line 103, in results_loop\n",
            "    raise RuntimeError(\"One or more background workers are no longer alive. Exiting. Please check the \"\n",
            "RuntimeError: One or more background workers are no longer alive. Exiting. Please check the print statements above for the actual error message\n",
            "\n",
            "Treinamento conclu√≠do (ou falhou devido a OOM - verifique a sa√≠da acima)!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Iniciar o treinamento\n",
        "# 001: ID do dataset\n",
        "# 3d_fullres: Configura√ß√£o de treinamento (3D full resolution)\n",
        "# 0: Fold de valida√ß√£o cruzada (usando a fold 0). Para treinamento completo, idealmente, voc√™ treinaria 5 folds.\n",
        "# -p nnUNetResEncUNetLPlans: Especifica o plano de treinamento a ser usado\n",
        "!nnUNetv2_train 001 3d_fullres 0 -p nnUNetResEncUNetLPlans"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MhFKnTa-Hnki",
        "outputId": "d6f868f8-61be-4884-914e-84eea7adbfe0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda:0\n",
            "\n",
            "#######################################################################\n",
            "Please cite the following paper when using nnU-Net:\n",
            "Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.\n",
            "#######################################################################\n",
            "\n",
            "2025-07-16 16:43:26.086724: Using torch.compile...\n",
            "2025-07-16 16:43:27.030293: do_dummy_2d_data_aug: True\n",
            "2025-07-16 16:43:27.036246: Using splits from existing split file: /content/drive/MyDrive/nnunet_data/nnUNet_preprocessed/Dataset001_AorticValve/splits_final.json\n",
            "2025-07-16 16:43:27.040446: The split file contains 5 splits.\n",
            "2025-07-16 16:43:27.042759: Desired fold for training: 0\n",
            "2025-07-16 16:43:27.045249: This split has 64 training and 16 validation cases.\n",
            "using pin_memory on device 0\n",
            "using pin_memory on device 0\n",
            "\n",
            "This is the configuration used by this training:\n",
            "Configuration name: 3d_fullres\n",
            " {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 1, 'patch_size': [24, 320, 320], 'median_image_size_in_voxels': [50.0, 512.0, 512.0], 'spacing': [2.5, 0.3505859375, 0.3505859375], 'normalization_schemes': ['CTNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.ResidualEncoderUNet', 'arch_kwargs': {'n_stages': 7, 'features_per_stage': [32, 64, 128, 256, 320, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[1, 3, 3], [1, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [1, 2, 2], [1, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_blocks_per_stage': [1, 3, 4, 6, 6, 6, 6], 'n_conv_per_stage_decoder': [1, 1, 1, 1, 1, 1], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': False} \n",
            "\n",
            "These are the global plan.json settings:\n",
            " {'dataset_name': 'Dataset001_AorticValve', 'plans_name': 'nnUNetResEncUNetLPlans', 'original_median_spacing_after_transp': [2.5000005960464478, 0.3505859375, 0.3505859375], 'original_median_shape_after_transp': [49, 512, 512], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'nnUNetPlannerResEncL', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 991.0, 'mean': 32.83921813964844, 'median': 35.0, 'min': -210.0, 'percentile_00_5': -82.0, 'percentile_99_5': 105.0, 'std': 29.420087814331055}}} \n",
            "\n",
            "2025-07-16 16:43:29.174848: Unable to plot network architecture: nnUNet_compile is enabled!\n",
            "2025-07-16 16:43:29.228820: \n",
            "2025-07-16 16:43:29.241778: Epoch 0\n",
            "2025-07-16 16:43:29.279576: Current learning rate: 0.01\n",
            "W0716 16:43:51.683000 89299 torch/_inductor/utils.py:1137] [0/0] Not enough SMs to use max_autotune_gemm mode\n",
            "2025-07-16 16:49:06.257597: train_loss 0.0522\n",
            "2025-07-16 16:49:06.261341: val_loss 0.019\n",
            "2025-07-16 16:49:06.263656: Pseudo dice [np.float32(0.0)]\n",
            "2025-07-16 16:49:06.266381: Epoch time: 337.04 s\n",
            "2025-07-16 16:49:06.269270: Yayy! New best EMA pseudo Dice: 0.0\n",
            "2025-07-16 16:49:09.877315: \n",
            "2025-07-16 16:49:09.879750: Epoch 1\n",
            "2025-07-16 16:49:09.881649: Current learning rate: 0.00999\n",
            "2025-07-16 16:52:33.184851: train_loss 0.0155\n",
            "2025-07-16 16:52:33.187908: val_loss -0.0085\n",
            "2025-07-16 16:52:33.189773: Pseudo dice [np.float32(0.0)]\n",
            "2025-07-16 16:52:33.191755: Epoch time: 203.31 s\n",
            "2025-07-16 16:52:34.494228: \n",
            "2025-07-16 16:52:34.497118: Epoch 2\n",
            "2025-07-16 16:52:34.499063: Current learning rate: 0.00998\n",
            "2025-07-16 16:55:59.071829: train_loss -0.058\n",
            "2025-07-16 16:55:59.074980: val_loss -0.0688\n",
            "2025-07-16 16:55:59.077101: Pseudo dice [np.float32(0.1701)]\n",
            "2025-07-16 16:55:59.079099: Epoch time: 204.58 s\n",
            "2025-07-16 16:55:59.080935: Yayy! New best EMA pseudo Dice: 0.017000000923871994\n",
            "2025-07-16 16:56:02.986854: \n",
            "2025-07-16 16:56:02.989468: Epoch 3\n",
            "2025-07-16 16:56:02.992174: Current learning rate: 0.00997\n",
            "2025-07-16 16:59:40.462542: train_loss -0.1294\n",
            "2025-07-16 16:59:40.465771: val_loss -0.1727\n",
            "2025-07-16 16:59:40.468891: Pseudo dice [np.float32(0.3087)]\n",
            "2025-07-16 16:59:40.470880: Epoch time: 217.48 s\n",
            "2025-07-16 16:59:40.473045: Yayy! New best EMA pseudo Dice: 0.04619999974966049\n",
            "2025-07-16 16:59:45.746823: \n",
            "2025-07-16 16:59:45.749290: Epoch 4\n",
            "2025-07-16 16:59:45.751070: Current learning rate: 0.00996\n",
            "2025-07-16 17:03:23.320672: train_loss -0.1235\n",
            "2025-07-16 17:03:23.323715: val_loss -0.1273\n",
            "2025-07-16 17:03:23.325863: Pseudo dice [np.float32(0.2549)]\n",
            "2025-07-16 17:03:23.327907: Epoch time: 217.58 s\n",
            "2025-07-16 17:03:23.330267: Yayy! New best EMA pseudo Dice: 0.06700000166893005\n",
            "2025-07-16 17:03:27.152709: \n",
            "2025-07-16 17:03:27.155594: Epoch 5\n",
            "2025-07-16 17:03:27.865802: Current learning rate: 0.00995\n",
            "2025-07-16 17:07:07.298495: train_loss -0.2257\n",
            "2025-07-16 17:07:07.301542: val_loss -0.186\n",
            "2025-07-16 17:07:07.303356: Pseudo dice [np.float32(0.3431)]\n",
            "2025-07-16 17:07:07.305092: Epoch time: 220.15 s\n",
            "2025-07-16 17:07:07.306816: Yayy! New best EMA pseudo Dice: 0.09470000118017197\n",
            "2025-07-16 17:07:11.238206: \n",
            "2025-07-16 17:07:11.240740: Epoch 6\n",
            "2025-07-16 17:07:11.242698: Current learning rate: 0.00995\n",
            "2025-07-16 17:10:51.925495: train_loss -0.2576\n",
            "2025-07-16 17:10:51.929119: val_loss -0.272\n",
            "2025-07-16 17:10:51.931843: Pseudo dice [np.float32(0.4063)]\n",
            "2025-07-16 17:10:51.934455: Epoch time: 220.69 s\n",
            "2025-07-16 17:10:51.937144: Yayy! New best EMA pseudo Dice: 0.1257999986410141\n",
            "2025-07-16 17:10:55.826450: \n",
            "2025-07-16 17:10:55.829020: Epoch 7\n",
            "2025-07-16 17:10:55.831063: Current learning rate: 0.00994\n",
            "2025-07-16 17:14:35.099772: train_loss -0.2768\n",
            "2025-07-16 17:14:35.102806: val_loss -0.3455\n",
            "2025-07-16 17:14:35.105062: Pseudo dice [np.float32(0.5223)]\n",
            "2025-07-16 17:14:35.107753: Epoch time: 219.28 s\n",
            "2025-07-16 17:14:35.110246: Yayy! New best EMA pseudo Dice: 0.1655000001192093\n",
            "2025-07-16 17:14:39.020462: \n",
            "2025-07-16 17:14:39.023690: Epoch 8\n",
            "2025-07-16 17:14:39.025620: Current learning rate: 0.00993\n",
            "2025-07-16 17:18:17.399302: train_loss -0.3058\n",
            "2025-07-16 17:18:17.403065: val_loss -0.2421\n",
            "2025-07-16 17:18:17.405352: Pseudo dice [np.float32(0.4121)]\n",
            "2025-07-16 17:18:17.408181: Epoch time: 218.38 s\n",
            "2025-07-16 17:18:17.410101: Yayy! New best EMA pseudo Dice: 0.19009999930858612\n",
            "2025-07-16 17:18:21.217261: \n",
            "2025-07-16 17:18:21.219769: Epoch 9\n",
            "2025-07-16 17:18:21.221778: Current learning rate: 0.00992\n",
            "2025-07-16 17:21:58.064483: train_loss -0.3196\n",
            "2025-07-16 17:21:58.067598: val_loss -0.37\n",
            "2025-07-16 17:21:58.069442: Pseudo dice [np.float32(0.517)]\n",
            "2025-07-16 17:21:58.071253: Epoch time: 216.85 s\n",
            "2025-07-16 17:21:58.073197: Yayy! New best EMA pseudo Dice: 0.22280000150203705\n",
            "2025-07-16 17:22:02.762275: \n",
            "2025-07-16 17:22:02.765100: Epoch 10\n",
            "2025-07-16 17:22:02.767521: Current learning rate: 0.00991\n",
            "2025-07-16 17:25:40.255215: train_loss -0.4083\n",
            "2025-07-16 17:25:40.258399: val_loss -0.3131\n",
            "2025-07-16 17:25:40.260240: Pseudo dice [np.float32(0.5607)]\n",
            "2025-07-16 17:25:40.262031: Epoch time: 217.5 s\n",
            "2025-07-16 17:25:40.263807: Yayy! New best EMA pseudo Dice: 0.2565999925136566\n",
            "2025-07-16 17:25:43.990595: \n",
            "2025-07-16 17:25:43.993539: Epoch 11\n",
            "2025-07-16 17:25:43.996243: Current learning rate: 0.0099\n",
            "2025-07-16 17:29:23.108934: train_loss -0.3775\n",
            "2025-07-16 17:29:23.111645: val_loss -0.2582\n",
            "2025-07-16 17:29:23.113456: Pseudo dice [np.float32(0.5457)]\n",
            "2025-07-16 17:29:23.127931: Epoch time: 219.12 s\n",
            "2025-07-16 17:29:23.131488: Yayy! New best EMA pseudo Dice: 0.2854999899864197\n",
            "2025-07-16 17:29:26.894687: \n",
            "2025-07-16 17:29:26.897552: Epoch 12\n",
            "2025-07-16 17:29:26.899642: Current learning rate: 0.00989\n",
            "2025-07-16 17:33:07.611405: train_loss -0.4192\n",
            "2025-07-16 17:33:07.614528: val_loss -0.4041\n",
            "2025-07-16 17:33:07.617816: Pseudo dice [np.float32(0.6547)]\n",
            "2025-07-16 17:33:07.619776: Epoch time: 220.72 s\n",
            "2025-07-16 17:33:07.636710: Yayy! New best EMA pseudo Dice: 0.3224000036716461\n",
            "2025-07-16 17:33:11.490237: \n",
            "2025-07-16 17:33:11.492758: Epoch 13\n",
            "2025-07-16 17:33:11.494817: Current learning rate: 0.00988\n",
            "2025-07-16 17:36:48.647212: train_loss -0.4018\n",
            "2025-07-16 17:36:48.650232: val_loss -0.4168\n",
            "2025-07-16 17:36:48.653133: Pseudo dice [np.float32(0.6472)]\n",
            "2025-07-16 17:36:48.655103: Epoch time: 217.16 s\n",
            "2025-07-16 17:36:48.656880: Yayy! New best EMA pseudo Dice: 0.3549000024795532\n",
            "2025-07-16 17:36:52.468811: \n",
            "2025-07-16 17:36:52.471670: Epoch 14\n",
            "2025-07-16 17:36:52.473719: Current learning rate: 0.00987\n",
            "2025-07-16 17:40:33.360626: train_loss -0.385\n",
            "2025-07-16 17:40:33.364264: val_loss -0.3497\n",
            "2025-07-16 17:40:33.367321: Pseudo dice [np.float32(0.5879)]\n",
            "2025-07-16 17:40:33.369342: Epoch time: 220.9 s\n",
            "2025-07-16 17:40:33.385053: Yayy! New best EMA pseudo Dice: 0.3781999945640564\n",
            "2025-07-16 17:40:37.267128: \n",
            "2025-07-16 17:40:37.269497: Epoch 15\n",
            "2025-07-16 17:40:37.271807: Current learning rate: 0.00986\n",
            "2025-07-16 17:44:16.264630: train_loss -0.3987\n",
            "2025-07-16 17:44:16.267525: val_loss -0.3801\n",
            "2025-07-16 17:44:16.269646: Pseudo dice [np.float32(0.6338)]\n",
            "2025-07-16 17:44:16.272342: Epoch time: 219.0 s\n",
            "2025-07-16 17:44:16.274275: Yayy! New best EMA pseudo Dice: 0.40380001068115234\n",
            "2025-07-16 17:44:20.810227: \n",
            "2025-07-16 17:44:20.812770: Epoch 16\n",
            "2025-07-16 17:44:20.814649: Current learning rate: 0.00986\n",
            "2025-07-16 17:48:00.068247: train_loss -0.4053\n",
            "2025-07-16 17:48:00.071137: val_loss -0.414\n",
            "2025-07-16 17:48:00.073139: Pseudo dice [np.float32(0.6276)]\n",
            "2025-07-16 17:48:00.075036: Epoch time: 219.26 s\n",
            "2025-07-16 17:48:00.077014: Yayy! New best EMA pseudo Dice: 0.4262000024318695\n",
            "2025-07-16 17:48:03.842387: \n",
            "2025-07-16 17:48:03.845313: Epoch 17\n",
            "2025-07-16 17:48:04.534153: Current learning rate: 0.00985\n",
            "2025-07-16 17:51:43.615384: train_loss -0.4316\n",
            "2025-07-16 17:51:43.618438: val_loss -0.4333\n",
            "2025-07-16 17:51:43.620346: Pseudo dice [np.float32(0.6137)]\n",
            "2025-07-16 17:51:43.622928: Epoch time: 219.78 s\n",
            "2025-07-16 17:51:43.625666: Yayy! New best EMA pseudo Dice: 0.4449000060558319\n",
            "2025-07-16 17:51:48.418500: \n",
            "2025-07-16 17:51:48.420870: Epoch 18\n",
            "2025-07-16 17:51:48.423670: Current learning rate: 0.00984\n",
            "2025-07-16 17:55:26.669877: train_loss -0.4235\n",
            "2025-07-16 17:55:26.672886: val_loss -0.3614\n",
            "2025-07-16 17:55:26.675041: Pseudo dice [np.float32(0.5566)]\n",
            "2025-07-16 17:55:26.677837: Epoch time: 218.25 s\n",
            "2025-07-16 17:55:26.680576: Yayy! New best EMA pseudo Dice: 0.4560999870300293\n",
            "2025-07-16 17:55:30.441834: \n",
            "2025-07-16 17:55:30.444475: Epoch 19\n",
            "2025-07-16 17:55:30.446395: Current learning rate: 0.00983\n",
            "2025-07-16 17:59:08.061543: train_loss -0.3811\n",
            "2025-07-16 17:59:08.065113: val_loss -0.2929\n",
            "2025-07-16 17:59:08.067477: Pseudo dice [np.float32(0.4764)]\n",
            "2025-07-16 17:59:08.070037: Epoch time: 217.62 s\n",
            "2025-07-16 17:59:08.072201: Yayy! New best EMA pseudo Dice: 0.45809999108314514\n",
            "2025-07-16 17:59:11.989632: \n",
            "2025-07-16 17:59:11.992277: Epoch 20\n",
            "2025-07-16 17:59:11.995228: Current learning rate: 0.00982\n",
            "2025-07-16 18:02:49.407480: train_loss -0.4266\n",
            "2025-07-16 18:02:49.410943: val_loss -0.4671\n",
            "2025-07-16 18:02:49.413748: Pseudo dice [np.float32(0.6311)]\n",
            "2025-07-16 18:02:49.416481: Epoch time: 217.42 s\n",
            "2025-07-16 18:02:49.419101: Yayy! New best EMA pseudo Dice: 0.47540000081062317\n",
            "2025-07-16 18:02:53.317034: \n",
            "2025-07-16 18:02:53.320140: Epoch 21\n",
            "2025-07-16 18:02:53.322626: Current learning rate: 0.00981\n",
            "2025-07-16 18:06:35.011628: train_loss -0.4534\n",
            "2025-07-16 18:06:35.014874: val_loss -0.4741\n",
            "2025-07-16 18:06:35.016785: Pseudo dice [np.float32(0.6654)]\n",
            "2025-07-16 18:06:35.018829: Epoch time: 221.7 s\n",
            "2025-07-16 18:06:35.021692: Yayy! New best EMA pseudo Dice: 0.4943999946117401\n",
            "2025-07-16 18:06:38.925866: \n",
            "2025-07-16 18:06:38.928522: Epoch 22\n",
            "2025-07-16 18:06:38.930766: Current learning rate: 0.0098\n",
            "2025-07-16 18:10:19.889326: train_loss -0.4654\n",
            "2025-07-16 18:10:19.892584: val_loss -0.4247\n",
            "2025-07-16 18:10:19.894327: Pseudo dice [np.float32(0.6543)]\n",
            "2025-07-16 18:10:19.896559: Epoch time: 220.97 s\n",
            "2025-07-16 18:10:19.898524: Yayy! New best EMA pseudo Dice: 0.5103999972343445\n",
            "2025-07-16 18:10:23.764718: \n",
            "2025-07-16 18:10:23.767637: Epoch 23\n",
            "2025-07-16 18:10:23.769521: Current learning rate: 0.00979\n",
            "2025-07-16 18:14:01.405361: train_loss -0.4283\n",
            "2025-07-16 18:14:01.408726: val_loss -0.4104\n",
            "2025-07-16 18:14:01.411837: Pseudo dice [np.float32(0.6223)]\n",
            "2025-07-16 18:14:01.414685: Epoch time: 217.64 s\n",
            "2025-07-16 18:14:01.418067: Yayy! New best EMA pseudo Dice: 0.5216000080108643\n",
            "2025-07-16 18:14:05.264071: \n",
            "2025-07-16 18:14:05.266575: Epoch 24\n",
            "2025-07-16 18:14:05.268520: Current learning rate: 0.00978\n",
            "2025-07-16 18:17:44.777666: train_loss -0.4424\n",
            "2025-07-16 18:17:44.780841: val_loss -0.2916\n",
            "2025-07-16 18:17:44.782743: Pseudo dice [np.float32(0.5156)]\n",
            "2025-07-16 18:17:44.784514: Epoch time: 219.52 s\n",
            "2025-07-16 18:17:46.052855: \n",
            "2025-07-16 18:17:46.055361: Epoch 25\n",
            "2025-07-16 18:17:46.057085: Current learning rate: 0.00977\n",
            "2025-07-16 18:21:11.371463: train_loss -0.4012\n",
            "2025-07-16 18:21:11.374920: val_loss -0.3563\n",
            "2025-07-16 18:21:11.377074: Pseudo dice [np.float32(0.543)]\n",
            "2025-07-16 18:21:11.379005: Epoch time: 205.32 s\n",
            "2025-07-16 18:21:11.380985: Yayy! New best EMA pseudo Dice: 0.5231999754905701\n",
            "2025-07-16 18:21:15.435377: \n",
            "2025-07-16 18:21:15.438448: Epoch 26\n",
            "2025-07-16 18:21:15.441046: Current learning rate: 0.00977\n",
            "2025-07-16 18:24:52.844896: train_loss -0.4346\n",
            "2025-07-16 18:24:52.848675: val_loss -0.4235\n",
            "2025-07-16 18:24:52.851149: Pseudo dice [np.float32(0.5684)]\n",
            "2025-07-16 18:24:52.853429: Epoch time: 217.41 s\n",
            "2025-07-16 18:24:52.855480: Yayy! New best EMA pseudo Dice: 0.5277000069618225\n",
            "2025-07-16 18:24:56.832671: \n",
            "2025-07-16 18:24:56.835601: Epoch 27\n",
            "2025-07-16 18:24:56.837752: Current learning rate: 0.00976\n",
            "2025-07-16 18:28:36.671444: train_loss -0.4657\n",
            "2025-07-16 18:28:36.675047: val_loss -0.4026\n",
            "2025-07-16 18:28:36.677248: Pseudo dice [np.float32(0.6307)]\n",
            "2025-07-16 18:28:36.680209: Epoch time: 219.84 s\n",
            "2025-07-16 18:28:36.681968: Yayy! New best EMA pseudo Dice: 0.5379999876022339\n",
            "2025-07-16 18:28:40.516849: \n",
            "2025-07-16 18:28:40.519209: Epoch 28\n",
            "2025-07-16 18:28:40.521082: Current learning rate: 0.00975\n",
            "2025-07-16 18:32:19.465820: train_loss -0.4692\n",
            "2025-07-16 18:32:19.469284: val_loss -0.4834\n",
            "2025-07-16 18:32:19.472972: Pseudo dice [np.float32(0.6485)]\n",
            "2025-07-16 18:32:19.475134: Epoch time: 218.95 s\n",
            "2025-07-16 18:32:19.477376: Yayy! New best EMA pseudo Dice: 0.5490999817848206\n",
            "2025-07-16 18:32:23.279730: \n",
            "2025-07-16 18:32:23.282897: Epoch 29\n",
            "2025-07-16 18:32:23.285141: Current learning rate: 0.00974\n",
            "2025-07-16 18:36:01.407851: train_loss -0.459\n",
            "2025-07-16 18:36:01.423405: val_loss -0.4251\n",
            "2025-07-16 18:36:01.425397: Pseudo dice [np.float32(0.6262)]\n",
            "2025-07-16 18:36:01.427980: Epoch time: 218.13 s\n",
            "2025-07-16 18:36:01.430403: Yayy! New best EMA pseudo Dice: 0.5568000078201294\n",
            "2025-07-16 18:36:05.350262: \n",
            "2025-07-16 18:36:05.352855: Epoch 30\n",
            "2025-07-16 18:36:05.355589: Current learning rate: 0.00973\n",
            "2025-07-16 18:39:43.388405: train_loss -0.5117\n",
            "2025-07-16 18:39:43.391924: val_loss -0.4267\n",
            "2025-07-16 18:39:43.407487: Pseudo dice [np.float32(0.6599)]\n",
            "2025-07-16 18:39:43.409395: Epoch time: 218.04 s\n",
            "2025-07-16 18:39:43.411165: Yayy! New best EMA pseudo Dice: 0.5670999884605408\n",
            "2025-07-16 18:39:48.778713: \n",
            "2025-07-16 18:39:48.781202: Epoch 31\n",
            "2025-07-16 18:39:48.783163: Current learning rate: 0.00972\n",
            "2025-07-16 18:43:28.281221: train_loss -0.4734\n",
            "2025-07-16 18:43:28.284271: val_loss -0.4535\n",
            "2025-07-16 18:43:28.285995: Pseudo dice [np.float32(0.6881)]\n",
            "2025-07-16 18:43:28.287709: Epoch time: 219.51 s\n",
            "2025-07-16 18:43:28.289511: Yayy! New best EMA pseudo Dice: 0.579200029373169\n",
            "2025-07-16 18:43:32.099758: \n",
            "2025-07-16 18:43:32.102860: Epoch 32\n",
            "2025-07-16 18:43:32.105197: Current learning rate: 0.00971\n",
            "2025-07-16 18:47:11.897536: train_loss -0.4771\n",
            "2025-07-16 18:47:11.900453: val_loss -0.4112\n",
            "2025-07-16 18:47:11.902416: Pseudo dice [np.float32(0.6736)]\n",
            "2025-07-16 18:47:11.904336: Epoch time: 219.8 s\n",
            "2025-07-16 18:47:11.906604: Yayy! New best EMA pseudo Dice: 0.5885999798774719\n",
            "2025-07-16 18:47:17.376064: \n",
            "2025-07-16 18:47:17.378557: Epoch 33\n",
            "2025-07-16 18:47:17.381939: Current learning rate: 0.0097\n",
            "2025-07-16 18:50:58.500175: train_loss -0.4568\n",
            "2025-07-16 18:50:58.504059: val_loss -0.4725\n",
            "2025-07-16 18:50:58.506464: Pseudo dice [np.float32(0.6726)]\n",
            "2025-07-16 18:50:58.508977: Epoch time: 221.13 s\n",
            "2025-07-16 18:50:58.511578: Yayy! New best EMA pseudo Dice: 0.597000002861023\n",
            "2025-07-16 18:51:03.602731: \n",
            "2025-07-16 18:51:03.605671: Epoch 34\n",
            "2025-07-16 18:51:03.608085: Current learning rate: 0.00969\n",
            "2025-07-16 18:54:41.653932: train_loss -0.498\n",
            "2025-07-16 18:54:41.657344: val_loss -0.3859\n",
            "2025-07-16 18:54:41.659414: Pseudo dice [np.float32(0.6383)]\n",
            "2025-07-16 18:54:41.661602: Epoch time: 218.06 s\n",
            "2025-07-16 18:54:41.663304: Yayy! New best EMA pseudo Dice: 0.6011000275611877\n",
            "2025-07-16 18:54:45.817947: \n",
            "2025-07-16 18:54:45.820811: Epoch 35\n",
            "2025-07-16 18:54:45.822968: Current learning rate: 0.00968\n",
            "2025-07-16 18:58:26.353520: train_loss -0.4652\n",
            "2025-07-16 18:58:26.357093: val_loss -0.5114\n",
            "2025-07-16 18:58:26.360236: Pseudo dice [np.float32(0.6864)]\n",
            "2025-07-16 18:58:26.362950: Epoch time: 220.54 s\n",
            "2025-07-16 18:58:26.364839: Yayy! New best EMA pseudo Dice: 0.6097000241279602\n",
            "2025-07-16 18:58:30.355052: \n",
            "2025-07-16 18:58:30.357843: Epoch 36\n",
            "2025-07-16 18:58:30.360253: Current learning rate: 0.00968\n",
            "2025-07-16 19:02:09.503657: train_loss -0.4623\n",
            "2025-07-16 19:02:09.507017: val_loss -0.3438\n",
            "2025-07-16 19:02:09.510127: Pseudo dice [np.float32(0.6662)]\n",
            "2025-07-16 19:02:09.513036: Epoch time: 219.15 s\n",
            "2025-07-16 19:02:09.516053: Yayy! New best EMA pseudo Dice: 0.6152999997138977\n",
            "2025-07-16 19:02:13.436781: \n",
            "2025-07-16 19:02:13.439642: Epoch 37\n",
            "2025-07-16 19:02:13.441857: Current learning rate: 0.00967\n",
            "2025-07-16 19:05:52.429286: train_loss -0.4842\n",
            "2025-07-16 19:05:52.432925: val_loss -0.4957\n",
            "2025-07-16 19:05:52.435281: Pseudo dice [np.float32(0.6676)]\n",
            "2025-07-16 19:05:52.437401: Epoch time: 219.0 s\n",
            "2025-07-16 19:05:52.439364: Yayy! New best EMA pseudo Dice: 0.6205000281333923\n",
            "2025-07-16 19:05:56.345871: \n",
            "2025-07-16 19:05:56.348769: Epoch 38\n",
            "2025-07-16 19:05:56.350838: Current learning rate: 0.00966\n",
            "2025-07-16 19:09:33.318690: train_loss -0.4572\n",
            "2025-07-16 19:09:33.321838: val_loss -0.5115\n",
            "2025-07-16 19:09:33.324424: Pseudo dice [np.float32(0.6635)]\n",
            "2025-07-16 19:09:33.326874: Epoch time: 216.98 s\n",
            "2025-07-16 19:09:33.329205: Yayy! New best EMA pseudo Dice: 0.6248000264167786\n",
            "2025-07-16 19:09:38.724758: \n",
            "2025-07-16 19:09:38.727849: Epoch 39\n",
            "2025-07-16 19:09:38.729916: Current learning rate: 0.00965\n",
            "2025-07-16 19:13:18.833867: train_loss -0.4852\n",
            "2025-07-16 19:13:18.836973: val_loss -0.3705\n",
            "2025-07-16 19:13:18.839036: Pseudo dice [np.float32(0.6173)]\n",
            "2025-07-16 19:13:18.840988: Epoch time: 220.11 s\n",
            "2025-07-16 19:13:20.174585: \n",
            "2025-07-16 19:13:20.177074: Epoch 40\n",
            "2025-07-16 19:13:20.179070: Current learning rate: 0.00964\n",
            "2025-07-16 19:16:45.190168: train_loss -0.4794\n",
            "2025-07-16 19:16:45.193785: val_loss -0.4664\n",
            "2025-07-16 19:16:45.195767: Pseudo dice [np.float32(0.6689)]\n",
            "2025-07-16 19:16:45.197703: Epoch time: 205.02 s\n",
            "2025-07-16 19:16:45.199761: Yayy! New best EMA pseudo Dice: 0.628600001335144\n",
            "2025-07-16 19:16:49.605718: \n",
            "2025-07-16 19:16:49.608289: Epoch 41\n",
            "2025-07-16 19:16:49.610362: Current learning rate: 0.00963\n",
            "2025-07-16 19:20:28.414127: train_loss -0.4886\n",
            "2025-07-16 19:20:28.417019: val_loss -0.487\n",
            "2025-07-16 19:20:28.418899: Pseudo dice [np.float32(0.7035)]\n",
            "2025-07-16 19:20:28.420622: Epoch time: 218.81 s\n",
            "2025-07-16 19:20:28.422286: Yayy! New best EMA pseudo Dice: 0.6360999941825867\n",
            "2025-07-16 19:20:32.326374: \n",
            "2025-07-16 19:20:32.329347: Epoch 42\n",
            "2025-07-16 19:20:32.331499: Current learning rate: 0.00962\n",
            "2025-07-16 19:24:09.836708: train_loss -0.4671\n",
            "2025-07-16 19:24:09.839750: val_loss -0.3817\n",
            "2025-07-16 19:24:09.841579: Pseudo dice [np.float32(0.5851)]\n",
            "2025-07-16 19:24:09.859336: Epoch time: 217.51 s\n",
            "2025-07-16 19:24:11.120362: \n",
            "2025-07-16 19:24:11.123115: Epoch 43\n",
            "2025-07-16 19:24:11.125455: Current learning rate: 0.00961\n",
            "2025-07-16 19:27:36.280566: train_loss -0.461\n",
            "2025-07-16 19:27:36.283586: val_loss -0.4132\n",
            "2025-07-16 19:27:36.286895: Pseudo dice [np.float32(0.6091)]\n",
            "2025-07-16 19:27:36.289792: Epoch time: 205.16 s\n",
            "2025-07-16 19:27:37.586566: \n",
            "2025-07-16 19:27:37.589396: Epoch 44\n",
            "2025-07-16 19:27:37.591330: Current learning rate: 0.0096\n",
            "2025-07-16 19:31:02.233568: train_loss -0.4399\n",
            "2025-07-16 19:31:02.236304: val_loss -0.5135\n",
            "2025-07-16 19:31:02.237931: Pseudo dice [np.float32(0.6803)]\n",
            "2025-07-16 19:31:02.239736: Epoch time: 204.65 s\n",
            "2025-07-16 19:31:03.515772: \n",
            "2025-07-16 19:31:03.518274: Epoch 45\n",
            "2025-07-16 19:31:03.520234: Current learning rate: 0.00959\n",
            "2025-07-16 19:34:28.405008: train_loss -0.4906\n",
            "2025-07-16 19:34:28.408600: val_loss -0.5043\n",
            "2025-07-16 19:34:28.411100: Pseudo dice [np.float32(0.7104)]\n",
            "2025-07-16 19:34:28.413265: Epoch time: 204.89 s\n",
            "2025-07-16 19:34:28.416262: Yayy! New best EMA pseudo Dice: 0.6416000127792358\n",
            "2025-07-16 19:34:32.348977: \n",
            "2025-07-16 19:34:32.351824: Epoch 46\n",
            "2025-07-16 19:34:32.354206: Current learning rate: 0.00959\n",
            "2025-07-16 19:38:10.753350: train_loss -0.4613\n",
            "2025-07-16 19:38:10.756683: val_loss -0.3804\n",
            "2025-07-16 19:38:10.758943: Pseudo dice [np.float32(0.6516)]\n",
            "2025-07-16 19:38:10.761090: Epoch time: 218.41 s\n",
            "2025-07-16 19:38:10.763037: Yayy! New best EMA pseudo Dice: 0.6425999999046326\n",
            "2025-07-16 19:38:14.581113: \n",
            "2025-07-16 19:38:14.584116: Epoch 47\n",
            "2025-07-16 19:38:14.586535: Current learning rate: 0.00958\n",
            "2025-07-16 19:41:51.663059: train_loss -0.4791\n",
            "2025-07-16 19:41:51.666661: val_loss -0.4321\n",
            "2025-07-16 19:41:51.668931: Pseudo dice [np.float32(0.639)]\n",
            "2025-07-16 19:41:51.671319: Epoch time: 217.09 s\n",
            "2025-07-16 19:41:53.012035: \n",
            "2025-07-16 19:41:53.014562: Epoch 48\n",
            "2025-07-16 19:41:53.017451: Current learning rate: 0.00957\n",
            "2025-07-16 19:45:18.123521: train_loss -0.4725\n",
            "2025-07-16 19:45:18.127102: val_loss -0.4731\n",
            "2025-07-16 19:45:18.129432: Pseudo dice [np.float32(0.707)]\n",
            "2025-07-16 19:45:18.131621: Epoch time: 205.12 s\n",
            "2025-07-16 19:45:18.133845: Yayy! New best EMA pseudo Dice: 0.6486999988555908\n",
            "2025-07-16 19:45:23.469674: \n",
            "2025-07-16 19:45:23.472017: Epoch 49\n",
            "2025-07-16 19:45:23.474400: Current learning rate: 0.00956\n",
            "2025-07-16 19:49:00.562215: train_loss -0.497\n",
            "2025-07-16 19:49:00.565075: val_loss -0.3871\n",
            "2025-07-16 19:49:00.566824: Pseudo dice [np.float32(0.5868)]\n",
            "2025-07-16 19:49:00.568877: Epoch time: 217.1 s\n",
            "2025-07-16 19:49:04.314087: \n",
            "2025-07-16 19:49:04.317036: Epoch 50\n",
            "2025-07-16 19:49:04.319110: Current learning rate: 0.00955\n",
            "2025-07-16 19:52:29.947948: train_loss -0.487\n",
            "2025-07-16 19:52:29.951702: val_loss -0.4021\n",
            "2025-07-16 19:52:29.953562: Pseudo dice [np.float32(0.627)]\n",
            "2025-07-16 19:52:29.955357: Epoch time: 205.64 s\n",
            "2025-07-16 19:52:31.264013: \n",
            "2025-07-16 19:52:31.266602: Epoch 51\n",
            "2025-07-16 19:52:31.268512: Current learning rate: 0.00954\n",
            "2025-07-16 19:55:57.105130: train_loss -0.5148\n",
            "2025-07-16 19:55:57.108777: val_loss -0.3726\n",
            "2025-07-16 19:55:57.111197: Pseudo dice [np.float32(0.6087)]\n",
            "2025-07-16 19:55:57.113620: Epoch time: 205.85 s\n",
            "2025-07-16 19:55:58.373763: \n",
            "2025-07-16 19:55:58.376198: Epoch 52\n",
            "2025-07-16 19:55:58.377894: Current learning rate: 0.00953\n",
            "2025-07-16 19:59:24.399223: train_loss -0.4743\n",
            "2025-07-16 19:59:24.417506: val_loss -0.4121\n",
            "2025-07-16 19:59:24.419282: Pseudo dice [np.float32(0.6686)]\n",
            "2025-07-16 19:59:24.421166: Epoch time: 206.03 s\n",
            "2025-07-16 19:59:25.683415: \n",
            "2025-07-16 19:59:25.685812: Epoch 53\n",
            "2025-07-16 19:59:25.687611: Current learning rate: 0.00952\n",
            "2025-07-16 20:02:51.665216: train_loss -0.5132\n",
            "2025-07-16 20:02:51.668573: val_loss -0.3943\n",
            "2025-07-16 20:02:51.670588: Pseudo dice [np.float32(0.6856)]\n",
            "2025-07-16 20:02:51.672527: Epoch time: 205.99 s\n",
            "2025-07-16 20:02:52.945582: \n",
            "2025-07-16 20:02:52.948156: Epoch 54\n",
            "2025-07-16 20:02:52.962770: Current learning rate: 0.00951\n",
            "2025-07-16 20:06:18.499079: train_loss -0.4892\n",
            "2025-07-16 20:06:18.502340: val_loss -0.4151\n",
            "2025-07-16 20:06:18.505445: Pseudo dice [np.float32(0.6542)]\n",
            "2025-07-16 20:06:18.508521: Epoch time: 205.56 s\n",
            "2025-07-16 20:06:19.882646: \n",
            "2025-07-16 20:06:19.886226: Epoch 55\n",
            "2025-07-16 20:06:19.889613: Current learning rate: 0.0095\n",
            "2025-07-16 20:09:45.526703: train_loss -0.5487\n",
            "2025-07-16 20:09:45.530011: val_loss -0.4687\n",
            "2025-07-16 20:09:45.531961: Pseudo dice [np.float32(0.6784)]\n",
            "2025-07-16 20:09:45.533807: Epoch time: 205.65 s\n",
            "2025-07-16 20:09:45.535594: Yayy! New best EMA pseudo Dice: 0.649399995803833\n",
            "2025-07-16 20:09:49.451176: \n",
            "2025-07-16 20:09:49.453794: Epoch 56\n",
            "2025-07-16 20:09:49.455707: Current learning rate: 0.00949\n",
            "2025-07-16 20:13:29.753946: train_loss -0.4889\n",
            "2025-07-16 20:13:29.757611: val_loss -0.4995\n",
            "2025-07-16 20:13:29.759834: Pseudo dice [np.float32(0.7172)]\n",
            "2025-07-16 20:13:29.762153: Epoch time: 220.31 s\n",
            "2025-07-16 20:13:29.764492: Yayy! New best EMA pseudo Dice: 0.6561999917030334\n",
            "2025-07-16 20:13:33.739622: \n",
            "2025-07-16 20:13:33.742808: Epoch 57\n",
            "2025-07-16 20:13:33.745008: Current learning rate: 0.00949\n",
            "2025-07-16 20:17:14.318838: train_loss -0.5018\n",
            "2025-07-16 20:17:14.322206: val_loss -0.3821\n",
            "2025-07-16 20:17:14.338954: Pseudo dice [np.float32(0.675)]\n",
            "2025-07-16 20:17:14.342473: Epoch time: 220.58 s\n",
            "2025-07-16 20:17:14.344782: Yayy! New best EMA pseudo Dice: 0.6581000089645386\n",
            "2025-07-16 20:17:19.250889: \n",
            "2025-07-16 20:17:19.253470: Epoch 58\n",
            "2025-07-16 20:17:19.256198: Current learning rate: 0.00948\n",
            "2025-07-16 20:21:01.064193: train_loss -0.5506\n",
            "2025-07-16 20:21:01.067272: val_loss -0.4655\n",
            "2025-07-16 20:21:01.069415: Pseudo dice [np.float32(0.7055)]\n",
            "2025-07-16 20:21:01.071623: Epoch time: 221.82 s\n",
            "2025-07-16 20:21:01.088108: Yayy! New best EMA pseudo Dice: 0.6628000140190125\n",
            "2025-07-16 20:21:04.975528: \n",
            "2025-07-16 20:21:04.978122: Epoch 59\n",
            "2025-07-16 20:21:04.980763: Current learning rate: 0.00947\n",
            "2025-07-16 20:24:45.315889: train_loss -0.4686\n",
            "2025-07-16 20:24:45.318823: val_loss -0.4283\n",
            "2025-07-16 20:24:45.320616: Pseudo dice [np.float32(0.6343)]\n",
            "2025-07-16 20:24:45.322425: Epoch time: 220.34 s\n",
            "2025-07-16 20:24:46.582119: \n",
            "2025-07-16 20:24:46.584523: Epoch 60\n",
            "2025-07-16 20:24:46.586493: Current learning rate: 0.00946\n",
            "2025-07-16 20:28:12.503515: train_loss -0.4795\n",
            "2025-07-16 20:28:12.506874: val_loss -0.4427\n",
            "2025-07-16 20:28:12.510225: Pseudo dice [np.float32(0.6802)]\n",
            "2025-07-16 20:28:12.512979: Epoch time: 205.93 s\n",
            "2025-07-16 20:28:13.788725: \n",
            "2025-07-16 20:28:13.791396: Epoch 61\n",
            "2025-07-16 20:28:13.794077: Current learning rate: 0.00945\n",
            "2025-07-16 20:31:39.734885: train_loss -0.517\n",
            "2025-07-16 20:31:39.738234: val_loss -0.4701\n",
            "2025-07-16 20:31:39.741206: Pseudo dice [np.float32(0.6582)]\n",
            "2025-07-16 20:31:39.744093: Epoch time: 205.95 s\n",
            "2025-07-16 20:31:41.048711: \n",
            "2025-07-16 20:31:41.051822: Epoch 62\n",
            "2025-07-16 20:31:41.053975: Current learning rate: 0.00944\n",
            "2025-07-16 20:35:06.860537: train_loss -0.5222\n",
            "2025-07-16 20:35:06.877673: val_loss -0.4176\n",
            "2025-07-16 20:35:06.879834: Pseudo dice [np.float32(0.6744)]\n",
            "2025-07-16 20:35:06.881970: Epoch time: 205.82 s\n",
            "2025-07-16 20:35:06.884791: Yayy! New best EMA pseudo Dice: 0.6628999710083008\n",
            "2025-07-16 20:35:10.739665: \n",
            "2025-07-16 20:35:10.742220: Epoch 63\n",
            "2025-07-16 20:35:10.744267: Current learning rate: 0.00943\n",
            "2025-07-16 20:38:49.614396: train_loss -0.4943\n",
            "2025-07-16 20:38:49.617215: val_loss -0.3828\n",
            "2025-07-16 20:38:49.619055: Pseudo dice [np.float32(0.68)]\n",
            "2025-07-16 20:38:49.620848: Epoch time: 218.88 s\n",
            "2025-07-16 20:38:49.622796: Yayy! New best EMA pseudo Dice: 0.6646000146865845\n",
            "2025-07-16 20:38:53.610779: \n",
            "2025-07-16 20:38:53.613889: Epoch 64\n",
            "2025-07-16 20:38:53.616320: Current learning rate: 0.00942\n",
            "2025-07-16 20:42:32.172565: train_loss -0.5094\n",
            "2025-07-16 20:42:32.176417: val_loss -0.4726\n",
            "2025-07-16 20:42:32.178424: Pseudo dice [np.float32(0.6815)]\n",
            "2025-07-16 20:42:32.180511: Epoch time: 218.57 s\n",
            "2025-07-16 20:42:32.194789: Yayy! New best EMA pseudo Dice: 0.6662999987602234\n",
            "2025-07-16 20:42:36.057053: \n",
            "2025-07-16 20:42:36.060144: Epoch 65\n",
            "2025-07-16 20:42:36.062358: Current learning rate: 0.00941\n",
            "2025-07-16 20:46:14.629034: train_loss -0.4923\n",
            "2025-07-16 20:46:14.632514: val_loss -0.4412\n",
            "2025-07-16 20:46:14.635360: Pseudo dice [np.float32(0.6398)]\n",
            "2025-07-16 20:46:14.638144: Epoch time: 218.58 s\n",
            "2025-07-16 20:46:16.011852: \n",
            "2025-07-16 20:46:16.014233: Epoch 66\n",
            "2025-07-16 20:46:16.016152: Current learning rate: 0.0094\n",
            "2025-07-16 20:49:41.657689: train_loss -0.5265\n",
            "2025-07-16 20:49:41.661176: val_loss -0.4845\n",
            "2025-07-16 20:49:41.663261: Pseudo dice [np.float32(0.675)]\n",
            "2025-07-16 20:49:41.665219: Epoch time: 205.65 s\n",
            "2025-07-16 20:49:44.150028: \n",
            "2025-07-16 20:49:44.152485: Epoch 67\n",
            "2025-07-16 20:49:44.154608: Current learning rate: 0.00939\n",
            "2025-07-16 20:53:10.105658: train_loss -0.5324\n",
            "2025-07-16 20:53:10.108672: val_loss -0.461\n",
            "2025-07-16 20:53:10.110637: Pseudo dice [np.float32(0.6605)]\n",
            "2025-07-16 20:53:10.112675: Epoch time: 205.96 s\n",
            "2025-07-16 20:53:11.439580: \n",
            "2025-07-16 20:53:11.442429: Epoch 68\n",
            "2025-07-16 20:53:11.445178: Current learning rate: 0.00939\n",
            "2025-07-16 20:56:37.360343: train_loss -0.4855\n",
            "2025-07-16 20:56:37.363964: val_loss -0.4896\n",
            "2025-07-16 20:56:37.366210: Pseudo dice [np.float32(0.69)]\n",
            "2025-07-16 20:56:37.368670: Epoch time: 205.92 s\n",
            "2025-07-16 20:56:37.371058: Yayy! New best EMA pseudo Dice: 0.6668999791145325\n",
            "2025-07-16 20:56:41.400740: \n",
            "2025-07-16 20:56:41.403145: Epoch 69\n",
            "2025-07-16 20:56:41.404935: Current learning rate: 0.00938\n",
            "2025-07-16 21:00:17.253563: train_loss -0.5332\n",
            "2025-07-16 21:00:17.257083: val_loss -0.4357\n",
            "2025-07-16 21:00:17.259464: Pseudo dice [np.float32(0.6265)]\n",
            "2025-07-16 21:00:17.261659: Epoch time: 215.86 s\n",
            "2025-07-16 21:00:18.647719: \n",
            "2025-07-16 21:00:18.651093: Epoch 70\n",
            "2025-07-16 21:00:18.654050: Current learning rate: 0.00937\n",
            "2025-07-16 21:03:44.751441: train_loss -0.5014\n",
            "2025-07-16 21:03:44.755055: val_loss -0.4051\n",
            "2025-07-16 21:03:44.757097: Pseudo dice [np.float32(0.6403)]\n",
            "2025-07-16 21:03:44.771680: Epoch time: 206.11 s\n",
            "2025-07-16 21:03:46.146906: \n",
            "2025-07-16 21:03:46.149472: Epoch 71\n",
            "2025-07-16 21:03:46.151522: Current learning rate: 0.00936\n",
            "2025-07-16 21:07:11.510255: train_loss -0.5027\n",
            "2025-07-16 21:07:11.513504: val_loss -0.3347\n",
            "2025-07-16 21:07:11.515760: Pseudo dice [np.float32(0.6422)]\n",
            "2025-07-16 21:07:11.519269: Epoch time: 205.37 s\n",
            "2025-07-16 21:07:12.803029: \n",
            "2025-07-16 21:07:12.805974: Epoch 72\n",
            "2025-07-16 21:07:12.808091: Current learning rate: 0.00935\n",
            "2025-07-16 21:10:38.187982: train_loss -0.4988\n",
            "2025-07-16 21:10:38.191251: val_loss -0.3951\n",
            "2025-07-16 21:10:38.193352: Pseudo dice [np.float32(0.6848)]\n",
            "2025-07-16 21:10:38.195347: Epoch time: 205.39 s\n",
            "2025-07-16 21:10:39.470568: \n",
            "2025-07-16 21:10:39.473433: Epoch 73\n",
            "2025-07-16 21:10:39.475700: Current learning rate: 0.00934\n",
            "2025-07-16 21:14:04.423972: train_loss -0.4655\n",
            "2025-07-16 21:14:04.427109: val_loss -0.4275\n",
            "2025-07-16 21:14:04.429996: Pseudo dice [np.float32(0.6387)]\n",
            "2025-07-16 21:14:04.431911: Epoch time: 204.96 s\n",
            "2025-07-16 21:14:05.779456: \n",
            "2025-07-16 21:14:05.782124: Epoch 74\n",
            "2025-07-16 21:14:05.784120: Current learning rate: 0.00933\n",
            "2025-07-16 21:17:30.876691: train_loss -0.503\n",
            "2025-07-16 21:17:30.879890: val_loss -0.437\n",
            "2025-07-16 21:17:30.881716: Pseudo dice [np.float32(0.6602)]\n",
            "2025-07-16 21:17:30.897006: Epoch time: 205.1 s\n",
            "2025-07-16 21:17:32.221176: \n",
            "2025-07-16 21:17:32.223516: Epoch 75\n",
            "2025-07-16 21:17:32.225621: Current learning rate: 0.00932\n",
            "2025-07-16 21:20:57.401604: train_loss -0.4965\n",
            "2025-07-16 21:20:57.405669: val_loss -0.371\n",
            "2025-07-16 21:20:57.408168: Pseudo dice [np.float32(0.6045)]\n",
            "2025-07-16 21:20:57.410539: Epoch time: 205.18 s\n",
            "2025-07-16 21:20:58.803412: \n",
            "2025-07-16 21:20:58.806102: Epoch 76\n",
            "2025-07-16 21:20:58.808255: Current learning rate: 0.00931\n",
            "2025-07-16 21:24:24.127317: train_loss -0.4986\n",
            "2025-07-16 21:24:24.130682: val_loss -0.4923\n",
            "2025-07-16 21:24:24.132387: Pseudo dice [np.float32(0.704)]\n",
            "2025-07-16 21:24:24.134407: Epoch time: 205.33 s\n",
            "2025-07-16 21:24:25.459855: \n",
            "2025-07-16 21:24:25.462435: Epoch 77\n",
            "2025-07-16 21:24:25.465299: Current learning rate: 0.0093\n",
            "2025-07-16 21:27:50.121513: train_loss -0.501\n",
            "2025-07-16 21:27:50.124619: val_loss -0.4469\n",
            "2025-07-16 21:27:50.126615: Pseudo dice [np.float32(0.6365)]\n",
            "2025-07-16 21:27:50.128427: Epoch time: 204.67 s\n",
            "2025-07-16 21:27:51.430408: \n",
            "2025-07-16 21:27:51.432837: Epoch 78\n",
            "2025-07-16 21:27:51.434685: Current learning rate: 0.0093\n",
            "2025-07-16 21:31:15.856954: train_loss -0.5039\n",
            "2025-07-16 21:31:15.860090: val_loss -0.4963\n",
            "2025-07-16 21:31:15.862955: Pseudo dice [np.float32(0.6867)]\n",
            "2025-07-16 21:31:15.864703: Epoch time: 204.43 s\n",
            "2025-07-16 21:31:17.169026: \n",
            "2025-07-16 21:31:17.172100: Epoch 79\n",
            "2025-07-16 21:31:17.174458: Current learning rate: 0.00929\n",
            "2025-07-16 21:34:42.160685: train_loss -0.4896\n",
            "2025-07-16 21:34:42.164112: val_loss -0.4688\n",
            "2025-07-16 21:34:42.166017: Pseudo dice [np.float32(0.6775)]\n",
            "2025-07-16 21:34:42.168424: Epoch time: 205.0 s\n",
            "2025-07-16 21:34:43.486096: \n",
            "2025-07-16 21:34:43.488642: Epoch 80\n",
            "2025-07-16 21:34:43.505807: Current learning rate: 0.00928\n",
            "2025-07-16 21:38:08.688160: train_loss -0.52\n",
            "2025-07-16 21:38:08.691440: val_loss -0.4665\n",
            "2025-07-16 21:38:08.693439: Pseudo dice [np.float32(0.6572)]\n",
            "2025-07-16 21:38:08.695457: Epoch time: 205.21 s\n",
            "2025-07-16 21:38:10.026983: \n",
            "2025-07-16 21:38:10.030099: Epoch 81\n",
            "2025-07-16 21:38:10.032763: Current learning rate: 0.00927\n",
            "2025-07-16 21:41:34.539722: train_loss -0.5067\n",
            "2025-07-16 21:41:34.543111: val_loss -0.4654\n",
            "2025-07-16 21:41:34.545343: Pseudo dice [np.float32(0.6902)]\n",
            "2025-07-16 21:41:34.547381: Epoch time: 204.52 s\n",
            "2025-07-16 21:41:35.856946: \n",
            "2025-07-16 21:41:35.859840: Epoch 82\n",
            "2025-07-16 21:41:35.861830: Current learning rate: 0.00926\n",
            "2025-07-16 21:45:00.975816: train_loss -0.5207\n",
            "2025-07-16 21:45:00.979077: val_loss -0.4628\n",
            "2025-07-16 21:45:00.996898: Pseudo dice [np.float32(0.6667)]\n",
            "2025-07-16 21:45:00.999484: Epoch time: 205.12 s\n",
            "2025-07-16 21:45:02.327794: \n",
            "2025-07-16 21:45:02.330203: Epoch 83\n",
            "2025-07-16 21:45:02.332616: Current learning rate: 0.00925\n",
            "2025-07-16 21:48:27.253766: train_loss -0.4889\n",
            "2025-07-16 21:48:27.256866: val_loss -0.5123\n",
            "2025-07-16 21:48:27.259921: Pseudo dice [np.float32(0.6946)]\n",
            "2025-07-16 21:48:27.262908: Epoch time: 204.93 s\n",
            "2025-07-16 21:48:27.265143: Yayy! New best EMA pseudo Dice: 0.6672000288963318\n",
            "2025-07-16 21:48:31.138392: \n",
            "2025-07-16 21:48:31.141060: Epoch 84\n",
            "2025-07-16 21:48:31.144609: Current learning rate: 0.00924\n",
            "2025-07-16 21:52:05.676304: train_loss -0.5115\n",
            "2025-07-16 21:52:05.680097: val_loss -0.3495\n",
            "2025-07-16 21:52:05.683674: Pseudo dice [np.float32(0.6331)]\n",
            "2025-07-16 21:52:05.687099: Epoch time: 214.54 s\n",
            "2025-07-16 21:52:07.040955: \n",
            "2025-07-16 21:52:07.057626: Epoch 85\n",
            "2025-07-16 21:52:07.059738: Current learning rate: 0.00923\n",
            "2025-07-16 21:55:32.674771: train_loss -0.5145\n",
            "2025-07-16 21:55:32.678387: val_loss -0.4426\n",
            "2025-07-16 21:55:32.680675: Pseudo dice [np.float32(0.6364)]\n",
            "2025-07-16 21:55:32.683090: Epoch time: 205.64 s\n",
            "2025-07-16 21:55:34.044080: \n",
            "2025-07-16 21:55:34.046558: Epoch 86\n",
            "2025-07-16 21:55:34.061506: Current learning rate: 0.00922\n",
            "2025-07-16 21:58:59.922499: train_loss -0.5279\n",
            "2025-07-16 21:58:59.925887: val_loss -0.3865\n",
            "2025-07-16 21:58:59.928127: Pseudo dice [np.float32(0.6501)]\n",
            "2025-07-16 21:58:59.930628: Epoch time: 205.88 s\n",
            "2025-07-16 21:59:01.272444: \n",
            "2025-07-16 21:59:01.275994: Epoch 87\n",
            "2025-07-16 21:59:01.278715: Current learning rate: 0.00921\n",
            "2025-07-16 22:02:26.955848: train_loss -0.5018\n",
            "2025-07-16 22:02:26.959581: val_loss -0.4257\n",
            "2025-07-16 22:02:26.962056: Pseudo dice [np.float32(0.6342)]\n",
            "2025-07-16 22:02:26.964536: Epoch time: 205.69 s\n",
            "2025-07-16 22:02:28.321686: \n",
            "2025-07-16 22:02:28.324652: Epoch 88\n",
            "2025-07-16 22:02:28.326911: Current learning rate: 0.0092\n",
            "2025-07-16 22:05:53.796097: train_loss -0.5305\n",
            "2025-07-16 22:05:53.799916: val_loss -0.451\n",
            "2025-07-16 22:05:53.802339: Pseudo dice [np.float32(0.6979)]\n",
            "2025-07-16 22:05:53.820420: Epoch time: 205.48 s\n",
            "2025-07-16 22:05:55.175106: \n",
            "2025-07-16 22:05:55.178050: Epoch 89\n",
            "2025-07-16 22:05:55.180917: Current learning rate: 0.0092\n",
            "2025-07-16 22:09:20.529719: train_loss -0.5266\n",
            "2025-07-16 22:09:20.532880: val_loss -0.4527\n",
            "2025-07-16 22:09:20.535861: Pseudo dice [np.float32(0.6771)]\n",
            "2025-07-16 22:09:20.538310: Epoch time: 205.36 s\n",
            "2025-07-16 22:09:21.952606: \n",
            "2025-07-16 22:09:21.968601: Epoch 90\n",
            "2025-07-16 22:09:21.970939: Current learning rate: 0.00919\n",
            "2025-07-16 22:12:47.233684: train_loss -0.5394\n",
            "2025-07-16 22:12:47.236944: val_loss -0.4213\n",
            "2025-07-16 22:12:47.238945: Pseudo dice [np.float32(0.6784)]\n",
            "2025-07-16 22:12:47.241059: Epoch time: 205.29 s\n",
            "2025-07-16 22:12:48.591516: \n",
            "2025-07-16 22:12:48.594177: Epoch 91\n",
            "2025-07-16 22:12:48.596110: Current learning rate: 0.00918\n",
            "2025-07-16 22:16:14.714905: train_loss -0.558\n",
            "2025-07-16 22:16:14.718526: val_loss -0.3811\n",
            "2025-07-16 22:16:14.722341: Pseudo dice [np.float32(0.6534)]\n",
            "2025-07-16 22:16:14.724788: Epoch time: 206.13 s\n",
            "2025-07-16 22:16:16.093618: \n",
            "2025-07-16 22:16:16.096431: Epoch 92\n",
            "2025-07-16 22:16:16.099145: Current learning rate: 0.00917\n",
            "2025-07-16 22:19:41.766456: train_loss -0.5342\n",
            "2025-07-16 22:19:41.769756: val_loss -0.3945\n",
            "2025-07-16 22:19:41.782753: Pseudo dice [np.float32(0.6269)]\n",
            "2025-07-16 22:19:41.786371: Epoch time: 205.68 s\n",
            "2025-07-16 22:19:43.086907: \n",
            "2025-07-16 22:19:43.090025: Epoch 93\n",
            "2025-07-16 22:19:43.092575: Current learning rate: 0.00916\n",
            "2025-07-16 22:23:08.681548: train_loss -0.509\n",
            "2025-07-16 22:23:08.685572: val_loss -0.3859\n",
            "2025-07-16 22:23:08.687948: Pseudo dice [np.float32(0.6438)]\n",
            "2025-07-16 22:23:08.690639: Epoch time: 205.6 s\n",
            "2025-07-16 22:23:10.021294: \n",
            "2025-07-16 22:23:10.039566: Epoch 94\n",
            "2025-07-16 22:23:10.042698: Current learning rate: 0.00915\n",
            "2025-07-16 22:26:36.059554: train_loss -0.5178\n",
            "2025-07-16 22:26:36.062641: val_loss -0.4535\n",
            "2025-07-16 22:26:36.064667: Pseudo dice [np.float32(0.6762)]\n",
            "2025-07-16 22:26:36.066518: Epoch time: 206.04 s\n",
            "2025-07-16 22:26:37.357145: \n",
            "2025-07-16 22:26:37.359554: Epoch 95\n",
            "2025-07-16 22:26:37.361714: Current learning rate: 0.00914\n",
            "2025-07-16 22:30:02.883021: train_loss -0.4761\n",
            "2025-07-16 22:30:02.885977: val_loss -0.4607\n",
            "2025-07-16 22:30:02.887765: Pseudo dice [np.float32(0.6805)]\n",
            "2025-07-16 22:30:02.890285: Epoch time: 205.53 s\n",
            "2025-07-16 22:30:04.133596: \n",
            "2025-07-16 22:30:04.136351: Epoch 96\n",
            "2025-07-16 22:30:04.138264: Current learning rate: 0.00913\n",
            "2025-07-16 22:33:28.921453: train_loss -0.537\n",
            "2025-07-16 22:33:28.937949: val_loss -0.3744\n",
            "2025-07-16 22:33:28.941912: Pseudo dice [np.float32(0.6562)]\n",
            "2025-07-16 22:33:28.944619: Epoch time: 204.79 s\n",
            "2025-07-16 22:33:30.202658: \n",
            "2025-07-16 22:33:30.205169: Epoch 97\n",
            "2025-07-16 22:33:30.206916: Current learning rate: 0.00912\n",
            "2025-07-16 22:36:54.947777: train_loss -0.5225\n",
            "2025-07-16 22:36:54.951127: val_loss -0.4082\n",
            "2025-07-16 22:36:54.953735: Pseudo dice [np.float32(0.6638)]\n",
            "2025-07-16 22:36:54.956532: Epoch time: 204.75 s\n",
            "2025-07-16 22:36:56.250659: \n",
            "2025-07-16 22:36:56.266241: Epoch 98\n",
            "2025-07-16 22:36:56.268654: Current learning rate: 0.00911\n",
            "2025-07-16 22:40:20.685279: train_loss -0.5166\n",
            "2025-07-16 22:40:20.688260: val_loss -0.4076\n",
            "2025-07-16 22:40:20.690068: Pseudo dice [np.float32(0.6882)]\n",
            "2025-07-16 22:40:20.692221: Epoch time: 204.44 s\n",
            "2025-07-16 22:40:21.954091: \n",
            "2025-07-16 22:40:21.956767: Epoch 99\n",
            "2025-07-16 22:40:21.959075: Current learning rate: 0.0091\n",
            "2025-07-16 22:43:46.528342: train_loss -0.5096\n",
            "2025-07-16 22:43:46.531793: val_loss -0.4451\n",
            "2025-07-16 22:43:46.534961: Pseudo dice [np.float32(0.6556)]\n",
            "2025-07-16 22:43:46.538187: Epoch time: 204.58 s\n",
            "2025-07-16 22:43:50.344481: \n",
            "2025-07-16 22:43:50.347186: Epoch 100\n",
            "2025-07-16 22:43:50.349391: Current learning rate: 0.0091\n",
            "2025-07-16 22:47:26.402849: train_loss -0.4872\n",
            "2025-07-16 22:47:26.405977: val_loss -0.5019\n",
            "2025-07-16 22:47:26.424493: Pseudo dice [np.float32(0.7015)]\n",
            "2025-07-16 22:47:26.426738: Epoch time: 216.06 s\n",
            "2025-07-16 22:47:26.428948: Yayy! New best EMA pseudo Dice: 0.6672999858856201\n",
            "2025-07-16 22:47:30.283238: \n",
            "2025-07-16 22:47:30.285789: Epoch 101\n",
            "2025-07-16 22:47:30.287846: Current learning rate: 0.00909\n",
            "2025-07-16 22:51:04.386903: train_loss -0.5175\n",
            "2025-07-16 22:51:04.390168: val_loss -0.4653\n",
            "2025-07-16 22:51:04.392073: Pseudo dice [np.float32(0.662)]\n",
            "2025-07-16 22:51:04.393874: Epoch time: 214.11 s\n",
            "2025-07-16 22:51:05.657896: \n",
            "2025-07-16 22:51:05.660676: Epoch 102\n",
            "2025-07-16 22:51:05.662937: Current learning rate: 0.00908\n",
            "2025-07-16 22:54:29.953867: train_loss -0.4863\n",
            "2025-07-16 22:54:29.957102: val_loss -0.5333\n",
            "2025-07-16 22:54:29.959429: Pseudo dice [np.float32(0.707)]\n",
            "2025-07-16 22:54:29.961995: Epoch time: 204.3 s\n",
            "2025-07-16 22:54:29.963805: Yayy! New best EMA pseudo Dice: 0.670799970626831\n",
            "2025-07-16 22:54:33.757771: \n",
            "2025-07-16 22:54:33.760411: Epoch 103\n",
            "2025-07-16 22:54:33.763099: Current learning rate: 0.00907\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úÖ ADICIONAR NOVA C√âLULA 7B - Modificar Planos para Performance:\n",
        "\n",
        "import json\n",
        "import os\n",
        "\n",
        "# Caminho do arquivo de planos\n",
        "plans_file = \"/content/drive/MyDrive/nnunet_data/nnUNet_preprocessed/Dataset001_AorticValve/nnUNetResEncUNetLPlans.json\"\n",
        "\n",
        "if os.path.exists(plans_file):\n",
        "    print(\"üìù Modificando planos para melhor performance...\")\n",
        "\n",
        "    with open(plans_file, 'r') as f:\n",
        "        plans = json.load(f)\n",
        "\n",
        "    # MELHORIAS CR√çTICAS:\n",
        "    if '3d_fullres' in plans['configurations']:\n",
        "        config = plans['configurations']['3d_fullres']\n",
        "\n",
        "        # 1. Aumentar √©pocas\n",
        "        config['num_epochs'] = 1000  # Era ~250, agora 1000\n",
        "\n",
        "        # 2. Reduzir batch size se OOM\n",
        "        if config.get('batch_size', 2) > 1:\n",
        "            config['batch_size'] = 1\n",
        "            print(\"  ‚úì Batch size reduzido para 1\")\n",
        "\n",
        "        # 3. Otimizar patch size para sua GPU\n",
        "        original_patch = config.get('patch_size', [32, 384, 384])\n",
        "        config['patch_size'] = [24, 320, 320]  # Menor para evitar OOM\n",
        "        print(f\"  ‚úì Patch size: {original_patch} ‚Üí {config['patch_size']}\")\n",
        "\n",
        "        # 4. Adicionar early stopping mais agressivo\n",
        "        config['patience'] = 50  # Parar ap√≥s 50 √©pocas sem melhoria\n",
        "\n",
        "    # Salvar modifica√ß√µes\n",
        "    with open(plans_file, 'w') as f:\n",
        "        json.dump(plans, f, indent=2)\n",
        "\n",
        "    print(\"‚úÖ Planos modificados com sucesso!\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Arquivo de planos n√£o encontrado. Execute primeiro o plan_and_preprocess.\")"
      ],
      "metadata": {
        "id": "XDyGA46XbZpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "V7Lg8iLebfqM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üîÆ C√âLULA 8 ‚Äî Predi√ß√£o nos dados de teste"
      ],
      "metadata": {
        "id": "iG_mMB-UPMHJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úÖ Predi√ß√£o nos dados de teste\n",
        "import os\n",
        "\n",
        "DATASET_ID = \"001\"\n",
        "DATASET_NAME = \"AorticValve\"\n",
        "TRAINER_PLANS = \"nnUNetTrainer_plans_nnUNetResEncUNetLPlans\"\n",
        "CONFIGURATION = \"3d_fullres\"\n",
        "FOLD = \"0\"\n",
        "\n",
        "INPUT_TEST_DIR = f\"/content/drive/MyDrive/nnunet_data/nnUNet_raw/Dataset{DATASET_ID}_{DATASET_NAME}/imagesTs\"\n",
        "OUTPUT_PREDS_DIR_TEST = f\"/content/drive/MyDrive/nnunet_data/nnUNet_results/Dataset{DATASET_ID}_{DATASET_NAME}/{TRAINER_PLANS}_{CONFIGURATION}/fold_{FOLD}/predictionsTs_ResEncL\"\n",
        "\n",
        "print(f\"Iniciando predi√ß√£o nos dados de teste de {INPUT_TEST_DIR}...\")\n",
        "print(f\"Resultados ser√£o salvos em: {OUTPUT_PREDS_DIR_TEST}\")\n",
        "\n",
        "# O nnUNetv2_predict criar√° os diret√≥rios de sa√≠da automaticamente.\n",
        "!nnUNetv2_predict \\\n",
        "  -d {DATASET_ID} \\\n",
        "  -i {INPUT_TEST_DIR} \\\n",
        "  -o {OUTPUT_PREDS_DIR_TEST} \\\n",
        "  -c {CONFIGURATION} \\\n",
        "  -f {FOLD} \\\n",
        "  -p nnUNetResEncUNetLPlans \\\n",
        "  -chk checkpoint_best.pth # Usa o melhor checkpoint de valida√ß√£o"
      ],
      "metadata": {
        "id": "4iHn0DPIPOG_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üîÆ C√âLULA 9 ‚Äî Predi√ß√£o nos dados de treino (valida√ß√£o cruzada)"
      ],
      "metadata": {
        "id": "rv4OIvyQPQm0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úÖ Predi√ß√£o nos dados de treino (valida√ß√£o cruzada)\n",
        "import os\n",
        "\n",
        "DATASET_ID = \"001\"\n",
        "DATASET_NAME = \"AorticValve\"\n",
        "TRAINER_PLANS = \"nnUNetTrainer_plans_nnUNetResEncUNetLPlans\"\n",
        "CONFIGURATION = \"3d_fullres\"\n",
        "FOLD = \"0\"\n",
        "\n",
        "INPUT_TRAIN_DIR = f\"/content/drive/MyDrive/nnunet_data/nnUNet_raw/Dataset{DATASET_ID}_{DATASET_NAME}/imagesTr\"\n",
        "OUTPUT_PREDS_DIR_TRAIN = f\"/content/drive/MyDrive/nnunet_data/nnUNet_results/Dataset{DATASET_ID}_{DATASET_NAME}/{TRAINER_PLANS}_{CONFIGURATION}/fold_{FOLD}/predictionsTr_ResEncL\"\n",
        "\n",
        "print(f\"Iniciando predi√ß√£o nos dados de treino (valida√ß√£o) de {INPUT_TRAIN_DIR}...\")\n",
        "print(f\"Resultados ser√£o salvos em: {OUTPUT_PREDS_DIR_TRAIN}\")\n",
        "\n",
        "!nnUNetv2_predict \\\n",
        "  -d {DATASET_ID} \\\n",
        "  -i {INPUT_TRAIN_DIR} \\\n",
        "  -o {OUTPUT_PREDS_DIR_TRAIN} \\\n",
        "  -c {CONFIGURATION} \\\n",
        "  -f {FOLD} \\\n",
        "  -p nnUNetResEncUNetLPlans \\\n",
        "  -chk checkpoint_best.pth # Usa o melhor checkpoint de valida√ß√£o"
      ],
      "metadata": {
        "id": "ORXAB-rdPTgu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üß† C√âLULA 10 ‚Äî Identificar c√°lcio apenas dentro da v√°lvula predita (para os dados de teste)"
      ],
      "metadata": {
        "id": "Z4vP9n2pPVIB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úÖ Extrair apenas o c√°lcio dentro da v√°lvula nas predi√ß√µes dos casos de teste\n",
        "import nibabel as nib\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "DATASET_ID = \"001\"\n",
        "DATASET_NAME = \"AorticValve\"\n",
        "TRAINER_PLANS = \"nnUNetTrainer_plans_nnUNetResEncUNetLPlans\"\n",
        "CONFIGURATION = \"3d_fullres\"\n",
        "FOLD = \"0\"\n",
        "\n",
        "BASE_NNUNET_RAW_DIR = f\"/content/drive/MyDrive/nnunet_data/nnUNet_raw/Dataset{DATASET_ID}_{DATASET_NAME}\"\n",
        "BASE_NNUNET_RESULTS_DIR = f\"/content/drive/MyDrive/nnunet_data/nnUNet_results/Dataset{DATASET_ID}_{DATASET_NAME}/{TRAINER_PLANS}_{CONFIGURATION}/fold_{FOLD}\"\n",
        "\n",
        "images_dir = os.path.join(BASE_NNUNET_RAW_DIR, 'imagesTs')\n",
        "preds_valve_dir = os.path.join(BASE_NNUNET_RESULTS_DIR, 'predictionsTs_ResEncL') # Predi√ß√µes da v√°lvula a√≥rtica\n",
        "output_calcium_dir = os.path.join(BASE_NNUNET_RESULTS_DIR, 'predictionsTs_calcio_na_valvula') # Onde o c√°lcio filtrado ser√° salvo\n",
        "\n",
        "os.makedirs(output_calcium_dir, exist_ok=True)\n",
        "\n",
        "HU_LIMIAR_CALCIFICACAO = 130  # Limiar de unidades Hounsfield para identificar c√°lcio\n",
        "\n",
        "print(f\"Processando imagens de: {images_dir}\")\n",
        "print(f\"Usando predi√ß√µes da v√°lvula de: {preds_valve_dir}\")\n",
        "print(f\"Salvando resultados do c√°lcio filtrado em: {output_calcium_dir}\n",
        "\")\n",
        "\n",
        "processed_count = 0\n",
        "for fname in sorted(os.listdir(images_dir)):\n",
        "    if not fname.endswith('_0000.nii.gz'): # Busca arquivos de imagem (terminam com _0000.nii.gz)\n",
        "        continue\n",
        "\n",
        "    # Extrai o ID do paciente (ex: 'AorticValve_083' de 'AorticValve_083_0000.nii.gz')\n",
        "    id_paciente_base = fname.replace('_0000.nii.gz', '')\n",
        "    print(f\"üîç Processando paciente: {id_paciente_base}\")\n",
        "\n",
        "    try:\n",
        "        img_path = os.path.join(images_dir, fname)\n",
        "        pred_valve_path = os.path.join(preds_valve_dir, f\"{id_paciente_base}.nii.gz\")\n",
        "\n",
        "        if not os.path.exists(img_path):\n",
        "            print(f\"  AVISO: Imagem '{img_path}' n√£o encontrada. Pulando este paciente.\")\n",
        "            continue\n",
        "        if not os.path.exists(pred_valve_path):\n",
        "            print(f\"  AVISO: Predi√ß√£o da v√°lvula '{pred_valve_path}' n√£o encontrada. Pulando este paciente.\")\n",
        "            continue\n",
        "\n",
        "        img = nib.load(img_path)\n",
        "        img_data = img.get_fdata()\n",
        "        pred_valve = nib.load(pred_valve_path).get_fdata()\n",
        "\n",
        "        valve_mask = pred_valve > 0 # Cria m√°scara booleana da v√°lvula predita (valores > 0 s√£o v√°lvula)\n",
        "        calc_mask = img_data > HU_LIMIAR_CALCIFICACAO # Cria m√°scara booleana do c√°lcio na imagem original\n",
        "\n",
        "        # O c√°lcio dentro da v√°lvula √© a intersec√ß√£o das duas m√°scaras\n",
        "        calc_dentro_valvula = np.logical_and(valve_mask, calc_mask)\n",
        "\n",
        "        # Salva o resultado como um novo arquivo NIfTI\n",
        "        nifti_out = nib.Nifti1Image(calc_dentro_valvula.astype(np.uint8), img.affine, img.header)\n",
        "        nib.save(nifti_out, os.path.join(output_calcium_dir, f\"{id_paciente_base}.nii.gz\"))\n",
        "        processed_count += 1\n",
        "    except Exception as e:\n",
        "        print(f\"  ‚ùå Erro ao processar paciente {id_paciente_base}: {e}\")\n",
        "\n",
        "print(f\"\\n‚úÖ C√°lcio na v√°lvula (teste) salvo em: {output_calcium_dir} ({processed_count} arquivos processados)\")"
      ],
      "metadata": {
        "id": "QXeBwNaJPW6u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üß† C√âLULA 11 ‚Äî Avalia√ß√£o Dice + HD95 dos c√°lcios detectados (em teste)"
      ],
      "metadata": {
        "id": "EviXI5aLPYWO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úÖ Avalia√ß√£o Dice e HD95 do c√°lcio detectado na v√°lvula predita (em compara√ß√£o com o r√≥tulo da v√°lvula)\n",
        "# NOTA IMPORTANTE: Esta m√©trica compara a m√°scara de c√°lcio *predita dentro da v√°lvula predita*\n",
        "# com o *r√≥tulo original da v√°lvula*. Se voc√™ possui r√≥tulos espec√≠ficos para o c√°lcio (ground truth)\n",
        "# eles deveriam ser usados para uma avalia√ß√£o direta da segmenta√ß√£o de c√°lcio.\n",
        "\n",
        "import pandas as pd\n",
        "from medpy.metric import binary\n",
        "import nibabel as nib\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "DATASET_ID = \"001\"\n",
        "DATASET_NAME = \"AorticValve\"\n",
        "TRAINER_PLANS = \"nnUNetTrainer_plans_nnUNetResEncUNetLPlans\"\n",
        "CONFIGURATION = \"3d_fullres\"\n",
        "FOLD = \"0\"\n",
        "\n",
        "BASE_NNUNET_RAW_DIR = f\"/content/drive/MyDrive/nnunet_data/nnUNet_raw/Dataset{DATASET_ID}_{DATASET_NAME}\"\n",
        "BASE_NNUNET_RESULTS_DIR = f\"/content/drive/MyDrive/nnunet_data/nnUNet_results/Dataset{DATASET_ID}_{DATASET_NAME}/{TRAINER_PLANS}_{CONFIGURATION}/fold_{FOLD}\"\n",
        "\n",
        "labels_valve_dir = os.path.join(BASE_NNUNET_RAW_DIR, 'labelsTs') # R√≥tulos originais da v√°lvula\n",
        "preds_calcium_in_valve_dir = os.path.join(BASE_NNUNET_RESULTS_DIR, 'predictionsTs_calcio_na_valvula') # Predi√ß√µes do c√°lcio filtrado\n",
        "\n",
        "resultados = []\n",
        "print(f\"Avaliando predi√ß√µes de c√°lcio na v√°lvula em: {preds_calcium_in_valve_dir}\")\n",
        "print(f\"Comparando com r√≥tulos da v√°lvula em: {labels_valve_dir}\n",
        "\")\n",
        "\n",
        "evaluated_count = 0\n",
        "for fname in sorted(os.listdir(preds_calcium_in_valve_dir)):\n",
        "    if not fname.endswith('.nii.gz'):\n",
        "        continue\n",
        "\n",
        "    patient_id = fname.replace('.nii.gz', '')\n",
        "    print(f\"üìä Avaliando paciente: {patient_id}\")\n",
        "\n",
        "    try:\n",
        "        pred_calcium_path = os.path.join(preds_calcium_in_valve_dir, fname)\n",
        "        label_valve_path = os.path.join(labels_valve_dir, fname) # Assume o mesmo nome de arquivo\n",
        "\n",
        "        if not os.path.exists(pred_calcium_path):\n",
        "            print(f\"  AVISO: Predi√ß√£o de c√°lcio {pred_calcium_path} n√£o encontrada. Pulando.\")\n",
        "            continue\n",
        "        if not os.path.exists(label_valve_path):\n",
        "            print(f\"  AVISO: R√≥tulo da v√°lvula {label_valve_path} n√£o encontrado. Pulando.\")\n",
        "            continue\n",
        "\n",
        "        pred_mask = nib.load(pred_calcium_path).get_fdata() > 0\n",
        "        label_mask = nib.load(label_valve_path).get_fdata() > 0\n",
        "\n",
        "        dice = float('nan')\n",
        "        hd95 = float('nan')\n",
        "\n",
        "        # Lida com casos onde uma ou ambas as m√°scaras podem estar vazias\n",
        "        if np.sum(pred_mask) == 0 and np.sum(label_mask) == 0:\n",
        "            dice = 1.0 # Perfeita concord√¢ncia se ambos vazios\n",
        "            hd95 = 0.0 # Dist√¢ncia zero se ambos vazios\n",
        "        elif np.sum(pred_mask) == 0 or np.sum(label_mask) == 0:\n",
        "            dice = 0.0 # Um vazio e o outro n√£o, sem sobreposi√ß√£o\n",
        "            # HD95 para um conjunto vazio e outro n√£o √© geralmente 'inf' ou um valor grande.\n",
        "            # MedPy levanta erro para isso, ent√£o tratamos.\n",
        "            hd95 = float('inf')\n",
        "        else:\n",
        "            try:\n",
        "                dice = binary.dc(pred_mask, label_mask)\n",
        "                hd95 = binary.hd95(pred_mask, label_mask)\n",
        "            except Exception as metric_e:\n",
        "                print(f\"  AVISO: Erro ao calcular m√©tricas para {patient_id}: {metric_e}. Definindo como NaN.\")\n",
        "\n",
        "        resultados.append({'Paciente': patient_id, 'Dice': round(dice, 4), 'HD95': round(hd95, 2)})\n",
        "        evaluated_count += 1\n",
        "    except Exception as e:\n",
        "        print(f\"  ‚ùå Erro geral ao processar paciente {patient_id} para m√©tricas: {e}\")\n",
        "        resultados.append({'Paciente': patient_id, 'Dice': float('nan'), 'HD95': float('nan')})\n",
        "\n",
        "df = pd.DataFrame(resultados)\n",
        "output_csv_path = os.path.join(preds_calcium_in_valve_dir, 'metrics_summary_calcio.csv')\n",
        "df.to_csv(output_csv_path, index=False)\n",
        "\n",
        "print(f\"\\n‚úÖ Sum√°rio das m√©tricas salvo em: {output_csv_path} ({evaluated_count} pacientes avaliados)\")\n",
        "print(\"\\nDataFrame de resultados:\")\n",
        "print(df)\n"
      ],
      "metadata": {
        "id": "TS_ysEXkPZ7a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üìä C√âLULA 12 ‚Äî Visualiza√ß√µes gr√°ficas (Boxplot + Histograma)"
      ],
      "metadata": {
        "id": "VCofDqH8PbSE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úÖ Visualiza√ß√µes gr√°ficas das m√©tricas de desempenho\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "DATASET_ID = \"001\"\n",
        "DATASET_NAME = \"AorticValve\"\n",
        "TRAINER_PLANS = \"nnUNetTrainer_plans_nnUNetResEncUNetLPlans\"\n",
        "CONFIGURATION = \"3d_fullres\"\n",
        "FOLD = \"0\"\n",
        "\n",
        "BASE_NNUNET_RESULTS_DIR = f\"/content/drive/MyDrive/nnunet_data/nnUNet_results/Dataset{DATASET_ID}_{DATASET_NAME}/{TRAINER_PLANS}_{CONFIGURATION}/fold_{FOLD}\"\n",
        "preds_calcium_in_valve_dir = os.path.join(BASE_NNUNET_RESULTS_DIR, 'predictionsTs_calcio_na_valvula')\n",
        "input_csv_path = os.path.join(preds_calcium_in_valve_dir, 'metrics_summary_calcio.csv')\n",
        "\n",
        "print(f\"Tentando carregar dados de: {input_csv_path}\")\n",
        "\n",
        "if not os.path.exists(input_csv_path):\n",
        "    print(f\"ERRO: Arquivo de m√©tricas '{input_csv_path}' n√£o encontrado.\\nCertifique-se de que a C√©lula 11 foi executada com sucesso e gerou o CSV.\")\n",
        "else:\n",
        "    df = pd.read_csv(input_csv_path)\n",
        "\n",
        "    if df.empty:\n",
        "        print(\"AVISO: O DataFrame de m√©tricas est√° vazio. N√£o h√° dados para plotar.\")\n",
        "    else:\n",
        "        sns.set(style=\"whitegrid\")\n",
        "\n",
        "        # Boxplot Dice\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        sns.boxplot(y=df['Dice'])\n",
        "        plt.title('Distribui√ß√£o do Coeficiente Dice (C√°lcio na V√°lvula Predita)', fontsize=14)\n",
        "        plt.ylabel('Coeficiente Dice', fontsize=12)\n",
        "        plt.grid(True, linestyle='--', alpha=0.7)\n",
        "        plt.show()\n",
        "\n",
        "        # Boxplot HD95\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        # Remover NaNs para o boxplot, se houver, pois o HD95 pode ser 'inf' para m√°scaras vazias\n",
        "        hd95_data = df['HD95'].replace([np.inf, -np.inf], np.nan).dropna() # Remove inf e NaN\n",
        "        if hd95_data.empty:\n",
        "            print(\"AVISO: N√£o h√° dados v√°lidos para plotar o Boxplot HD95.\")\n",
        "        else:\n",
        "            sns.boxplot(y=hd95_data)\n",
        "            plt.title('Distribui√ß√£o da Dist√¢ncia de Hausdorff 95 (C√°lcio na V√°lvula Predita)', fontsize=14)\n",
        "            plt.ylabel('HD95 (mm)', fontsize=12)\n",
        "            plt.grid(True, linestyle='--', alpha=0.7)\n",
        "            plt.show()\n",
        "\n",
        "        # Histograma Dice\n",
        "        plt.figure(figsize=(10, 7))\n",
        "        sns.histplot(df['Dice'], bins=15, kde=True, color='skyblue', edgecolor='black')\n",
        "        plt.title('Histograma do Coeficiente Dice (C√°lcio na V√°lvula Predita)', fontsize=14)\n",
        "        plt.xlabel('Coeficiente Dice', fontsize=12)\n",
        "        plt.ylabel('Frequ√™ncia', fontsize=12)\n",
        "        plt.grid(True, linestyle='--', alpha=0.7)\n",
        "        plt.show()\n",
        "\n",
        "        print(\"\\n‚úÖ Visualiza√ß√µes geradas com sucesso!\")\n"
      ],
      "metadata": {
        "id": "Ud6EdT9YPc5j"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}