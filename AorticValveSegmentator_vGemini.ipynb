{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mardenlelis/ConfusionMatrixFunction/blob/main/AorticValveSegmentator_vGemini.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AorticValveSegmentator - Notebook Melhorado\n",
        "\n",
        "Este notebook foi aprimorado com base nas sugestões de melhoria para o pipeline de segmentação da válvula aórtica usando nnU-Netv2, incluindo correções de caminho, mensagens mais claras e preparação para lidar com erros de memória durante o treinamento."
      ],
      "metadata": {
        "id": "new-notebook-intro"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "🚀 CÉLULA 1 — Montar Google Drive"
      ],
      "metadata": {
        "id": "-uNH1mHlNfBB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 📦 Montar Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N9VGP8MPNZa1",
        "outputId": "7c5f5ac2-2cca-49ef-ffcf-bb7f3bd562ff"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "🔧 Célula 2 — Instalar dependências"
      ],
      "metadata": {
        "id": "aAcl4Z7NNivs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 📌 Instalar dependências\n",
        "# ✅ Instalar bibliotecas essenciais e nnU-Net v2\n",
        "!pip install --upgrade pip\n",
        "!pip install numpy pandas nibabel tqdm SimpleITK openpyxl --quiet\n",
        "!pip install medpy --quiet\n",
        "!pip install nnunetv2 --quiet\n",
        "# A flag --index-url é crucial para garantir a instalação de uma versão de torch compatível com a GPU (cu118)\n",
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118 --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cH6XBSicNlL-",
        "outputId": "cd1b9c2b-0786-4c70-eaa1-1646f3dfdd20"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.11/dist-packages (24.1.2)\n",
            "Collecting pip\n",
            "  Downloading pip-25.1.1-py3-none-any.whl.metadata (3.6 kB)\n",
            "Downloading pip-25.1.1-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m75.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pip\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 24.1.2\n",
            "    Uninstalling pip-24.1.2:\n",
            "      Successfully uninstalled pip-24.1.2\n",
            "Successfully installed pip-25.1.1\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.6/52.6 MB\u001b[0m \u001b[31m121.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[33m  DEPRECATION: Building 'medpy' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'medpy'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
            "\u001b[0m  Building wheel for medpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m59.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m152.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m149.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m51.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m43.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m64.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m124.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m130.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m74.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m149.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m93.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.6/45.6 MB\u001b[0m \u001b[31m110.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for nnunetv2 (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[33m  DEPRECATION: Building 'acvl-utils' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'acvl-utils'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
            "\u001b[0m  Building wheel for acvl-utils (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[33m  DEPRECATION: Building 'dynamic-network-architectures' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'dynamic-network-architectures'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
            "\u001b[0m  Building wheel for dynamic-network-architectures (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[33m  DEPRECATION: Building 'batchgenerators' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'batchgenerators'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
            "\u001b[0m  Building wheel for batchgenerators (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for batchgeneratorsv2 (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23/23\u001b[0m [nnunetv2]\n",
            "\u001b[1A\u001b[2K"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "⚙️ CÉLULA 3 — Verificar GPU e dispositivo"
      ],
      "metadata": {
        "id": "lBKYMcMsNniZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ⚙️ Verificar disponibilidade de GPU\n",
        "import torch\n",
        "print(\"CUDA disponível:\", torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print(\"Dispositivo:\", torch.cuda.get_device_name(0))\n",
        "else:\n",
        "    print(\"AVISO: Nenhuma GPU CUDA detectada. O treinamento será muito lento.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zcH031XlNszp",
        "outputId": "0c7449d4-973d-4b6e-d30b-6e1b4a8bb3ec"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA disponível: True\n",
            "Dispositivo: Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "🧹 Célula 4 — Limpar diretórios de origem de imagens e rótulos (opcional)"
      ],
      "metadata": {
        "id": "Xswwn7A1Sh5q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# AVISO: Esta célula apagará **TODOS** os arquivos nas pastas de imagens e rótulos originais\n",
        "# dentro da estrutura do nnU-Net. Use com cautela se você já tiver dados lá.\n",
        "# É útil para recomeçar do zero, mas pode ser comentada se os dados já estiverem preparados.\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "dir_originais_imagens = '/content/drive/MyDrive/nnunet_data/nnUNet_raw/Dataset001_AorticValve/originais/imagens'\n",
        "dir_originais_rotulos = '/content/drive/MyDrive/nnunet_data/nnUNet_raw/Dataset001_AorticValve/originais/rotulos'\n",
        "\n",
        "print(\"Limpando diretórios de origem para Dataset001_AorticValve/originais/imagens e rotulos...\")\n",
        "\n",
        "for d in [dir_originais_imagens, dir_originais_rotulos]:\n",
        "    if os.path.exists(d):\n",
        "        # Remove o conteúdo, mas mantém o diretório para recriação se necessário\n",
        "        for f in os.listdir(d):\n",
        "            file_path = os.path.join(d, f)\n",
        "            try:\n",
        "                if os.path.isfile(file_path) or os.path.islink(file_path):\n",
        "                    os.unlink(file_path)\n",
        "                elif os.path.isdir(file_path):\n",
        "                    shutil.rmtree(file_path)\n",
        "            except Exception as e:\n",
        "                print(f'Falha ao deletar {file_path}. Razão: {e}')\n",
        "    else:\n",
        "        print(f'Diretório não encontrado: {d}. Criando...')\n",
        "        os.makedirs(d, exist_ok=True)\n",
        "\n",
        "print(\"Limpeza concluída.\")\n"
      ],
      "metadata": {
        "id": "5vGv35lXS039",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2cd06f1f-4bc3-46b8-f334-92dc99599bc2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Limpando diretórios de origem para Dataset001_AorticValve/originais/imagens e rotulos...\n",
            "Limpeza concluída.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "📁 Célula 5 - Copia os arquivos do diretório origem e transfere para a estrutura de arquivos nnU-Net"
      ],
      "metadata": {
        "id": "r-q3Z5XkS4aN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Essa célula copia os arquivos do diretório origem e transfere para a estrutura de arquivos\n",
        "# utilizadas pelas células de preparação do dataset, com renomeação específica.\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# --- Caminhos ---\n",
        "# ATENÇÃO: Ajuste 'dir_origem' para o local dos seus arquivos NRRD de origem.\n",
        "dir_origem = '/content/drive/MyDrive/TC_DICOM/nrrd'\n",
        "\n",
        "# Diretórios de destino para as imagens e os rótulos (labels) na estrutura nnU-Net_raw\n",
        "dir_destino_imagens = '/content/drive/MyDrive/nnunet_data/nnUNet_raw/Dataset001_AorticValve/originais/imagens'\n",
        "dir_destino_labels = '/content/drive/MyDrive/nnunet_data/nnUNet_raw/Dataset001_AorticValve/originais/rotulos'\n",
        "\n",
        "# Garante que os diretórios de destino existam\n",
        "os.makedirs(dir_destino_imagens, exist_ok=True)\n",
        "os.makedirs(dir_destino_labels, exist_ok=True)\n",
        "\n",
        "print(\"Iniciando a busca, cópia e RENOMEAÇÃO com critérios específicos...\")\n",
        "\n",
        "if not os.path.isdir(dir_origem):\n",
        "    print(f\"\\nERRO: O diretório de origem '{dir_origem}' não foi encontrado. Por favor, verifique o caminho e a montagem do Drive.\")\n",
        "else:\n",
        "    found_any_file = False\n",
        "    for raiz, subpastas, arquivos in os.walk(dir_origem):\n",
        "        if raiz == dir_origem: # Pula o diretório raiz em si\n",
        "            continue\n",
        "\n",
        "        id_paciente_str = os.path.basename(raiz)\n",
        "        if not id_paciente_str.isdigit():\n",
        "            print(f\"  AVISO: Ignorando pasta '{raiz}' pois o nome não é um ID de paciente numérico válido.\")\n",
        "            continue\n",
        "\n",
        "        id_paciente_formatado = id_paciente_str.zfill(3)\n",
        "        print(f\"\\nAnalisando pasta do paciente: {id_paciente_formatado}\")\n",
        "\n",
        "        label_copiado = False\n",
        "        imagem_copiada = False\n",
        "\n",
        "        for nome_arquivo in arquivos:\n",
        "            caminho_completo_origem = os.path.join(raiz, nome_arquivo)\n",
        "\n",
        "            # 1. Procura pelo arquivo de rótulo que termina com '.seg.nrrd'\n",
        "            if nome_arquivo.endswith('.seg.nrrd') and not label_copiado:\n",
        "                novo_nome_label = f\"{id_paciente_formatado}.nrrd\"\n",
        "                caminho_completo_destino = os.path.join(dir_destino_labels, novo_nome_label)\n",
        "                print(f\"  -> RÓTULO encontrado e renomeado para: {novo_nome_label}\")\n",
        "                shutil.copy2(caminho_completo_origem, caminho_completo_destino)\n",
        "                label_copiado = True\n",
        "                found_any_file = True\n",
        "\n",
        "            # 2. Procura pelo arquivo de imagem com nome exato '120 KV.nrrd'\n",
        "            if nome_arquivo.endswith('120 KV.nrrd') and not imagem_copiada:\n",
        "                novo_nome_imagem = f\"{id_paciente_formatado}.nrrd\"\n",
        "                caminho_completo_destino = os.path.join(dir_destino_imagens, novo_nome_imagem)\n",
        "                print(f\"  -> IMAGEM encontrada e renomeada para: {novo_nome_imagem}\")\n",
        "                shutil.copy2(caminho_completo_origem, caminho_completo_destino)\n",
        "                imagem_copiada = True\n",
        "                found_any_file = True\n",
        "\n",
        "        if not label_copiado:\n",
        "            print(f\"  -> AVISO: Nenhum arquivo de RÓTULO terminando com '.seg.nrrd' foi encontrado para o paciente {id_paciente_formatado}.\")\n",
        "        if not imagem_copiada:\n",
        "            print(f\"  -> AVISO: Nenhum arquivo de IMAGEM terminando com '120 KV.nrrd' foi encontrado para o paciente {id_paciente_formatado}.\")\n",
        "\n",
        "    if not found_any_file:\n",
        "        print(\"\\nAVISO: Nenhuma imagem ou rótulo válido foi encontrado no diretório de origem ou subdiretórios.\\nCertifique-se de que o caminho está correto e os arquivos seguem o padrão esperado.\")\n",
        "\n",
        "    print(\"\\n\\nProcesso de cópia e renomeação concluído!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TUC7RpfbS7rK",
        "outputId": "82e2870c-f6bf-49c8-b1d0-c5b933f4e98f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iniciando a busca, cópia e RENOMEAÇÃO com critérios específicos...\n",
            "\n",
            "Analisando pasta do paciente: 083\n",
            "  -> IMAGEM encontrada e renomeada para: 083.nrrd\n",
            "  -> RÓTULO encontrado e renomeado para: 083.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 084\n",
            "  -> RÓTULO encontrado e renomeado para: 084.nrrd\n",
            "  -> IMAGEM encontrada e renomeada para: 084.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 091\n",
            "  -> IMAGEM encontrada e renomeada para: 091.nrrd\n",
            "  -> RÓTULO encontrado e renomeado para: 091.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 089\n",
            "  -> RÓTULO encontrado e renomeado para: 089.nrrd\n",
            "  -> IMAGEM encontrada e renomeada para: 089.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 090\n",
            "  -> RÓTULO encontrado e renomeado para: 090.nrrd\n",
            "  -> IMAGEM encontrada e renomeada para: 090.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 092\n",
            "  -> IMAGEM encontrada e renomeada para: 092.nrrd\n",
            "  -> RÓTULO encontrado e renomeado para: 092.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 087\n",
            "  -> RÓTULO encontrado e renomeado para: 087.nrrd\n",
            "  -> IMAGEM encontrada e renomeada para: 087.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 085\n",
            "  -> IMAGEM encontrada e renomeada para: 085.nrrd\n",
            "  -> RÓTULO encontrado e renomeado para: 085.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 088\n",
            "  -> IMAGEM encontrada e renomeada para: 088.nrrd\n",
            "  -> RÓTULO encontrado e renomeado para: 088.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 086\n",
            "  -> IMAGEM encontrada e renomeada para: 086.nrrd\n",
            "  -> RÓTULO encontrado e renomeado para: 086.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 093\n",
            "  -> IMAGEM encontrada e renomeada para: 093.nrrd\n",
            "  -> RÓTULO encontrado e renomeado para: 093.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 094\n",
            "  -> IMAGEM encontrada e renomeada para: 094.nrrd\n",
            "  -> RÓTULO encontrado e renomeado para: 094.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 095\n",
            "  -> RÓTULO encontrado e renomeado para: 095.nrrd\n",
            "  -> IMAGEM encontrada e renomeada para: 095.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 096\n",
            "  -> IMAGEM encontrada e renomeada para: 096.nrrd\n",
            "  -> RÓTULO encontrado e renomeado para: 096.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 098\n",
            "  -> RÓTULO encontrado e renomeado para: 098.nrrd\n",
            "  -> IMAGEM encontrada e renomeada para: 098.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 004\n",
            "  -> RÓTULO encontrado e renomeado para: 004.nrrd\n",
            "  -> IMAGEM encontrada e renomeada para: 004.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 100\n",
            "  -> RÓTULO encontrado e renomeado para: 100.nrrd\n",
            "  -> IMAGEM encontrada e renomeada para: 100.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 003\n",
            "  -> RÓTULO encontrado e renomeado para: 003.nrrd\n",
            "  -> IMAGEM encontrada e renomeada para: 003.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 097\n",
            "  -> IMAGEM encontrada e renomeada para: 097.nrrd\n",
            "  -> RÓTULO encontrado e renomeado para: 097.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 001\n",
            "  -> RÓTULO encontrado e renomeado para: 001.nrrd\n",
            "  -> IMAGEM encontrada e renomeada para: 001.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 002\n",
            "  -> IMAGEM encontrada e renomeada para: 002.nrrd\n",
            "  -> RÓTULO encontrado e renomeado para: 002.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 099\n",
            "  -> RÓTULO encontrado e renomeado para: 099.nrrd\n",
            "  -> IMAGEM encontrada e renomeada para: 099.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 005\n",
            "  -> RÓTULO encontrado e renomeado para: 005.nrrd\n",
            "  -> IMAGEM encontrada e renomeada para: 005.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 006\n",
            "  -> IMAGEM encontrada e renomeada para: 006.nrrd\n",
            "  -> RÓTULO encontrado e renomeado para: 006.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 007\n",
            "  -> RÓTULO encontrado e renomeado para: 007.nrrd\n",
            "  -> IMAGEM encontrada e renomeada para: 007.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 008\n",
            "  -> IMAGEM encontrada e renomeada para: 008.nrrd\n",
            "  -> RÓTULO encontrado e renomeado para: 008.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 011\n",
            "  -> IMAGEM encontrada e renomeada para: 011.nrrd\n",
            "  -> RÓTULO encontrado e renomeado para: 011.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 009\n",
            "  -> IMAGEM encontrada e renomeada para: 009.nrrd\n",
            "  -> RÓTULO encontrado e renomeado para: 009.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 014\n",
            "  -> IMAGEM encontrada e renomeada para: 014.nrrd\n",
            "  -> RÓTULO encontrado e renomeado para: 014.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 013\n",
            "  -> RÓTULO encontrado e renomeado para: 013.nrrd\n",
            "  -> IMAGEM encontrada e renomeada para: 013.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 012\n",
            "  -> IMAGEM encontrada e renomeada para: 012.nrrd\n",
            "  -> RÓTULO encontrado e renomeado para: 012.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 016\n",
            "  -> RÓTULO encontrado e renomeado para: 016.nrrd\n",
            "  -> IMAGEM encontrada e renomeada para: 016.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 010\n",
            "  -> RÓTULO encontrado e renomeado para: 010.nrrd\n",
            "  -> IMAGEM encontrada e renomeada para: 010.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 015\n",
            "  -> IMAGEM encontrada e renomeada para: 015.nrrd\n",
            "  -> RÓTULO encontrado e renomeado para: 015.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 017\n",
            "  -> IMAGEM encontrada e renomeada para: 017.nrrd\n",
            "  -> RÓTULO encontrado e renomeado para: 017.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 018\n",
            "  -> RÓTULO encontrado e renomeado para: 018.nrrd\n",
            "  -> IMAGEM encontrada e renomeada para: 018.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 020\n",
            "  -> IMAGEM encontrada e renomeada para: 020.nrrd\n",
            "  -> RÓTULO encontrado e renomeado para: 020.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 019\n",
            "  -> IMAGEM encontrada e renomeada para: 019.nrrd\n",
            "  -> RÓTULO encontrado e renomeado para: 019.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 028\n",
            "  -> RÓTULO encontrado e renomeado para: 028.nrrd\n",
            "  -> IMAGEM encontrada e renomeada para: 028.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 027\n",
            "  -> IMAGEM encontrada e renomeada para: 027.nrrd\n",
            "  -> RÓTULO encontrado e renomeado para: 027.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 021\n",
            "  -> IMAGEM encontrada e renomeada para: 021.nrrd\n",
            "  -> RÓTULO encontrado e renomeado para: 021.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 026\n",
            "  -> IMAGEM encontrada e renomeada para: 026.nrrd\n",
            "  -> RÓTULO encontrado e renomeado para: 026.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 024\n",
            "  -> IMAGEM encontrada e renomeada para: 024.nrrd\n",
            "  -> RÓTULO encontrado e renomeado para: 024.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 023\n",
            "  -> IMAGEM encontrada e renomeada para: 023.nrrd\n",
            "  -> RÓTULO encontrado e renomeado para: 023.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 025\n",
            "  -> IMAGEM encontrada e renomeada para: 025.nrrd\n",
            "  -> RÓTULO encontrado e renomeado para: 025.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 022\n",
            "  -> IMAGEM encontrada e renomeada para: 022.nrrd\n",
            "  -> RÓTULO encontrado e renomeado para: 022.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 029\n",
            "  -> IMAGEM encontrada e renomeada para: 029.nrrd\n",
            "  -> RÓTULO encontrado e renomeado para: 029.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 030\n",
            "  -> RÓTULO encontrado e renomeado para: 030.nrrd\n",
            "  -> IMAGEM encontrada e renomeada para: 030.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 032\n",
            "  -> IMAGEM encontrada e renomeada para: 032.nrrd\n",
            "  -> RÓTULO encontrado e renomeado para: 032.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 031\n",
            "  -> RÓTULO encontrado e renomeado para: 031.nrrd\n",
            "  -> IMAGEM encontrada e renomeada para: 031.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 034\n",
            "  -> IMAGEM encontrada e renomeada para: 034.nrrd\n",
            "  -> RÓTULO encontrado e renomeado para: 034.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 040\n",
            "  -> IMAGEM encontrada e renomeada para: 040.nrrd\n",
            "  -> RÓTULO encontrado e renomeado para: 040.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 033\n",
            "  -> RÓTULO encontrado e renomeado para: 033.nrrd\n",
            "  -> IMAGEM encontrada e renomeada para: 033.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 036\n",
            "  -> RÓTULO encontrado e renomeado para: 036.nrrd\n",
            "  -> IMAGEM encontrada e renomeada para: 036.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 035\n",
            "  -> IMAGEM encontrada e renomeada para: 035.nrrd\n",
            "  -> RÓTULO encontrado e renomeado para: 035.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 037\n",
            "  -> IMAGEM encontrada e renomeada para: 037.nrrd\n",
            "  -> RÓTULO encontrado e renomeado para: 037.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 038\n",
            "  -> IMAGEM encontrada e renomeada para: 038.nrrd\n",
            "  -> RÓTULO encontrado e renomeado para: 038.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 039\n",
            "  -> IMAGEM encontrada e renomeada para: 039.nrrd\n",
            "  -> RÓTULO encontrado e renomeado para: 039.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 042\n",
            "  -> IMAGEM encontrada e renomeada para: 042.nrrd\n",
            "  -> RÓTULO encontrado e renomeado para: 042.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 041\n",
            "  -> IMAGEM encontrada e renomeada para: 041.nrrd\n",
            "  -> RÓTULO encontrado e renomeado para: 041.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 043\n",
            "  -> RÓTULO encontrado e renomeado para: 043.nrrd\n",
            "  -> IMAGEM encontrada e renomeada para: 043.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 044\n",
            "  -> IMAGEM encontrada e renomeada para: 044.nrrd\n",
            "  -> RÓTULO encontrado e renomeado para: 044.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 051\n",
            "  -> RÓTULO encontrado e renomeado para: 051.nrrd\n",
            "  -> IMAGEM encontrada e renomeada para: 051.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 048\n",
            "  -> IMAGEM encontrada e renomeada para: 048.nrrd\n",
            "  -> RÓTULO encontrado e renomeado para: 048.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 049\n",
            "  -> RÓTULO encontrado e renomeado para: 049.nrrd\n",
            "  -> IMAGEM encontrada e renomeada para: 049.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 045\n",
            "  -> RÓTULO encontrado e renomeado para: 045.nrrd\n",
            "  -> IMAGEM encontrada e renomeada para: 045.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 050\n",
            "  -> IMAGEM encontrada e renomeada para: 050.nrrd\n",
            "  -> RÓTULO encontrado e renomeado para: 050.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 046\n",
            "  -> RÓTULO encontrado e renomeado para: 046.nrrd\n",
            "  -> IMAGEM encontrada e renomeada para: 046.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 052\n",
            "  -> IMAGEM encontrada e renomeada para: 052.nrrd\n",
            "  -> RÓTULO encontrado e renomeado para: 052.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 047\n",
            "  -> RÓTULO encontrado e renomeado para: 047.nrrd\n",
            "  -> IMAGEM encontrada e renomeada para: 047.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 054\n",
            "  -> IMAGEM encontrada e renomeada para: 054.nrrd\n",
            "  -> RÓTULO encontrado e renomeado para: 054.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 053\n",
            "  -> RÓTULO encontrado e renomeado para: 053.nrrd\n",
            "  -> IMAGEM encontrada e renomeada para: 053.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 056\n",
            "  -> RÓTULO encontrado e renomeado para: 056.nrrd\n",
            "  -> IMAGEM encontrada e renomeada para: 056.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 055\n",
            "  -> RÓTULO encontrado e renomeado para: 055.nrrd\n",
            "  -> IMAGEM encontrada e renomeada para: 055.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 063\n",
            "  -> IMAGEM encontrada e renomeada para: 063.nrrd\n",
            "  -> RÓTULO encontrado e renomeado para: 063.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 058\n",
            "  -> IMAGEM encontrada e renomeada para: 058.nrrd\n",
            "  -> RÓTULO encontrado e renomeado para: 058.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 061\n",
            "  -> IMAGEM encontrada e renomeada para: 061.nrrd\n",
            "  -> RÓTULO encontrado e renomeado para: 061.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 057\n",
            "  -> RÓTULO encontrado e renomeado para: 057.nrrd\n",
            "  -> IMAGEM encontrada e renomeada para: 057.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 062\n",
            "  -> IMAGEM encontrada e renomeada para: 062.nrrd\n",
            "  -> RÓTULO encontrado e renomeado para: 062.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 060\n",
            "  -> IMAGEM encontrada e renomeada para: 060.nrrd\n",
            "  -> RÓTULO encontrado e renomeado para: 060.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 059\n",
            "  -> IMAGEM encontrada e renomeada para: 059.nrrd\n",
            "  -> RÓTULO encontrado e renomeado para: 059.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 064\n",
            "  -> RÓTULO encontrado e renomeado para: 064.nrrd\n",
            "  -> IMAGEM encontrada e renomeada para: 064.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 065\n",
            "  -> IMAGEM encontrada e renomeada para: 065.nrrd\n",
            "  -> RÓTULO encontrado e renomeado para: 065.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 066\n",
            "  -> RÓTULO encontrado e renomeado para: 066.nrrd\n",
            "  -> IMAGEM encontrada e renomeada para: 066.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 067\n",
            "  -> IMAGEM encontrada e renomeada para: 067.nrrd\n",
            "  -> RÓTULO encontrado e renomeado para: 067.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 068\n",
            "  -> RÓTULO encontrado e renomeado para: 068.nrrd\n",
            "  -> IMAGEM encontrada e renomeada para: 068.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 076\n",
            "  -> RÓTULO encontrado e renomeado para: 076.nrrd\n",
            "  -> IMAGEM encontrada e renomeada para: 076.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 075\n",
            "  -> IMAGEM encontrada e renomeada para: 075.nrrd\n",
            "  -> RÓTULO encontrado e renomeado para: 075.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 071\n",
            "  -> RÓTULO encontrado e renomeado para: 071.nrrd\n",
            "  -> IMAGEM encontrada e renomeada para: 071.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 072\n",
            "  -> IMAGEM encontrada e renomeada para: 072.nrrd\n",
            "  -> RÓTULO encontrado e renomeado para: 072.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 070\n",
            "  -> RÓTULO encontrado e renomeado para: 070.nrrd\n",
            "  -> IMAGEM encontrada e renomeada para: 070.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 073\n",
            "  -> IMAGEM encontrada e renomeada para: 073.nrrd\n",
            "  -> RÓTULO encontrado e renomeado para: 073.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 074\n",
            "  -> IMAGEM encontrada e renomeada para: 074.nrrd\n",
            "  -> RÓTULO encontrado e renomeado para: 074.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 069\n",
            "  -> IMAGEM encontrada e renomeada para: 069.nrrd\n",
            "  -> RÓTULO encontrado e renomeado para: 069.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 078\n",
            "  -> IMAGEM encontrada e renomeada para: 078.nrrd\n",
            "  -> RÓTULO encontrado e renomeado para: 078.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 077\n",
            "  -> RÓTULO encontrado e renomeado para: 077.nrrd\n",
            "  -> IMAGEM encontrada e renomeada para: 077.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 080\n",
            "  -> IMAGEM encontrada e renomeada para: 080.nrrd\n",
            "  -> RÓTULO encontrado e renomeado para: 080.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 079\n",
            "  -> IMAGEM encontrada e renomeada para: 079.nrrd\n",
            "  -> RÓTULO encontrado e renomeado para: 079.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 082\n",
            "  -> RÓTULO encontrado e renomeado para: 082.nrrd\n",
            "  -> IMAGEM encontrada e renomeada para: 082.nrrd\n",
            "\n",
            "Analisando pasta do paciente: 081\n",
            "  -> RÓTULO encontrado e renomeado para: 081.nrrd\n",
            "  -> IMAGEM encontrada e renomeada para: 081.nrrd\n",
            "\n",
            "\n",
            "Processo de cópia e renomeação concluído!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "📁 Célula 6 — Gerar dataset.json automaticamente e estruturar o dataset para nnU-Net"
      ],
      "metadata": {
        "id": "Tl5IlrYfNvXU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ Preparação COMPLETA do dataset nnUNetv2 com limpeza e estrutura correta\n",
        "!pip install -q SimpleITK\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "import json\n",
        "import SimpleITK as sitk\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ============================================================================\n",
        "# 1. CONFIGURAÇÃO DE DIRETÓRIOS E LIMPEZA DOS DIRETÓRIOS DE DESTINO\n",
        "# ============================================================================\n",
        "\n",
        "DATASET_NAME = \"AorticValve\"\n",
        "DATASET_ID = \"001\"\n",
        "BASE_DIR = f\"/content/drive/MyDrive/nnunet_data/nnUNet_raw/Dataset{DATASET_ID}_{DATASET_NAME}\"\n",
        "\n",
        "SOURCE_IMAGES_DIR = os.path.join(BASE_DIR, \"originais\", \"imagens\")\n",
        "SOURCE_LABELS_DIR = os.path.join(BASE_DIR, \"originais\", \"rotulos\")\n",
        "\n",
        "IMAGES_TR_DIR = os.path.join(BASE_DIR, \"imagesTr\")\n",
        "LABELS_TR_DIR = os.path.join(BASE_DIR, \"labelsTr\")\n",
        "IMAGES_TS_DIR = os.path.join(BASE_DIR, \"imagesTs\")\n",
        "LABELS_TS_DIR = os.path.join(BASE_DIR, \"labelsTs\")  # Pasta para rótulos do conjunto de teste (para avaliação)\n",
        "\n",
        "# 🔄 LIMPEZA DOS DIRETÓRIOS nnU-Net\n",
        "print(\"🧹 Limpando e recriando diretórios de destino nnU-Net...\")\n",
        "for path in [IMAGES_TR_DIR, LABELS_TR_DIR, IMAGES_TS_DIR, LABELS_TS_DIR]:\n",
        "    if os.path.exists(path): # Remove o diretório se já existir\n",
        "        shutil.rmtree(path)\n",
        "    os.makedirs(path, exist_ok=True) # Cria o diretório limpo\n",
        "\n",
        "# ============================================================================\n",
        "# 2. FUNÇÃO DE CONVERSÃO CORRETA PARA .nii.gz COM SimpleITK\n",
        "# ============================================================================\n",
        "\n",
        "def convert_and_save_gzipped(source_path, dest_path):\n",
        "    if not os.path.exists(source_path):\n",
        "        print(f\"  ⚠️ Arquivo não encontrado: {os.path.basename(source_path)}. Pulando a conversão.\")\n",
        "        return False\n",
        "    try:\n",
        "        img = sitk.ReadImage(source_path)\n",
        "        sitk.WriteImage(img, dest_path, useCompression=True)\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"  ❌ Erro ao converter {os.path.basename(source_path)} para NIfTI: {e}\")\n",
        "        return False\n",
        "\n",
        "# ============================================================================\n",
        "# 3. CONVERSÃO DAS IMAGENS E DIVISÃO TREINO/TESTE\n",
        "# ============================================================================\n",
        "\n",
        "print(\"🔎 Lendo arquivos de origem...\")\n",
        "image_files_unsorted = [f for f in os.listdir(SOURCE_IMAGES_DIR) if not f.startswith('.')]\n",
        "label_files_unsorted = [f for f in os.listdir(SOURCE_LABELS_DIR) if not f.startswith('.')]\n",
        "\n",
        "# Cria uma lista de tuplas (id_paciente, nome_arquivo_imagem, nome_arquivo_label)\n",
        "paired_files = []\n",
        "for img_fname in image_files_unsorted:\n",
        "    base_name = img_fname.split('.')[0] # Assume que o nome base é o ID do paciente\n",
        "    label_fname = f\"{base_name}.nrrd\"\n",
        "    if label_fname in label_files_unsorted:\n",
        "        paired_files.append((base_name, img_fname, label_fname))\n",
        "    else:\n",
        "        print(f\"  AVISO: Rótulo '{label_fname}' não encontrado para a imagem '{img_fname}'. Ignorando este par.\")\n",
        "\n",
        "paired_files.sort(key=lambda x: x[0]) # Ordena para reprodutibilidade\n",
        "\n",
        "num_total_valid_files = len(paired_files)\n",
        "if num_total_valid_files == 0:\n",
        "    print(\"🚨 ERRO: Nenhum par de imagem/rótulo válido encontrado nos diretórios de origem. Certifique-se de que a Célula 5 foi executada corretamente.\")\n",
        "    exit() # Sai do script se não houver dados\n",
        "\n",
        "num_train_desired = min(80, num_total_valid_files)\n",
        "num_test_desired = num_total_valid_files - num_train_desired # O restante vai para teste\n",
        "\n",
        "num_train_files = 0\n",
        "num_test_files = 0\n",
        "train_metadata = []\n",
        "test_metadata = []\n",
        "\n",
        "print(f\"🚀 Processando {num_train_desired} casos para treino...\")\n",
        "for i in tqdm(range(num_train_desired)):\n",
        "    base_name, original_image_fname, original_label_fname = paired_files[i]\n",
        "    original_image_path = os.path.join(SOURCE_IMAGES_DIR, original_image_fname)\n",
        "    original_label_path = os.path.join(SOURCE_LABELS_DIR, original_label_fname)\n",
        "\n",
        "    case_id_nnunet = f\"{DATASET_NAME}_{base_name}\" # ID no formato nnU-Net\n",
        "\n",
        "    image_out_path = os.path.join(IMAGES_TR_DIR, f\"{case_id_nnunet}_0000.nii.gz\")\n",
        "    label_out_path = os.path.join(LABELS_TR_DIR, f\"{case_id_nnunet}.nii.gz\")\n",
        "\n",
        "    img_ok = convert_and_save_gzipped(original_image_path, image_out_path)\n",
        "    lbl_ok = convert_and_save_gzipped(original_label_path, label_out_path)\n",
        "\n",
        "    if img_ok and lbl_ok:\n",
        "        num_train_files += 1\n",
        "        train_metadata.append({\n",
        "            \"image\": f\"./imagesTr/{case_id_nnunet}_0000.nii.gz\",\n",
        "            \"label\": f\"./labelsTr/{case_id_nnunet}.nii.gz\"\n",
        "        })\n",
        "    else:\n",
        "        print(f\"  AVISO: Caso {base_name} não foi incluído no treino devido a erro na conversão.\")\n",
        "\n",
        "print(f\"\\n🧪 Processando {num_test_desired} casos para teste...\")\n",
        "for i in tqdm(range(num_train_desired, num_total_valid_files)):\n",
        "    base_name, original_image_fname, original_label_fname = paired_files[i]\n",
        "    original_image_path = os.path.join(SOURCE_IMAGES_DIR, original_image_fname)\n",
        "    original_label_path = os.path.join(SOURCE_LABELS_DIR, original_label_fname)\n",
        "\n",
        "    case_id_nnunet = f\"{DATASET_NAME}_{base_name}\"\n",
        "\n",
        "    image_out_path = os.path.join(IMAGES_TS_DIR, f\"{case_id_nnunet}_0000.nii.gz\")\n",
        "    label_out_path = os.path.join(LABELS_TS_DIR, f\"{case_id_nnunet}.nii.gz\") # Copiar label para labelsTs para avaliação posterior\n",
        "\n",
        "    img_ok = convert_and_save_gzipped(original_image_path, image_out_path)\n",
        "    lbl_ok = convert_and_save_gzipped(original_label_path, label_out_path)\n",
        "\n",
        "    if img_ok:\n",
        "        num_test_files += 1\n",
        "        test_metadata.append(f\"./imagesTs/{case_id_nnunet}_0000.nii.gz\")\n",
        "    else:\n",
        "        print(f\"  AVISO: Caso {base_name} não foi incluído no teste devido a erro na conversão.\")\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# 4. GERAR dataset.json COMPLETO (VERSÃO FINAL nnUNetv2)\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n📝 Gerando dataset.json...\")\n",
        "\n",
        "dataset_info = {\n",
        "    \"channel_names\": {\n",
        "        \"0\": \"CT\" # Nome do canal de entrada (CT, MR, etc.)\n",
        "    },\n",
        "    \"labels\": {\n",
        "        \"background\": \"0\",\n",
        "        \"aortic_valve\": \"1\" # Mapeamento do rótulo 1 para 'aortic_valve'\n",
        "    },\n",
        "    \"modality\": {\n",
        "        \"0\": \"CT\"\n",
        "    },\n",
        "    \"file_ending\": \".nii.gz\",\n",
        "    \"numTraining\": num_train_files,\n",
        "    \"training\": train_metadata,\n",
        "    \"test\": test_metadata,\n",
        "    \"name\": DATASET_NAME, # Nome do dataset\n",
        "    \"description\": \"Segmentação da Válvula Aórtica em Tomografia Computadorizada\",\n",
        "    \"reference\": \"N/A\",\n",
        "    \"licence\": \"N/A\",\n",
        "    \"release\": \"0.0\"\n",
        "}\n",
        "\n",
        "with open(os.path.join(BASE_DIR, \"dataset.json\"), \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(dataset_info, f, indent=4)\n",
        "\n",
        "print(\"\\n✅ Dataset estruturado com sucesso!\")\n",
        "print(f\"✔️ Treino: {num_train_files} casos\")\n",
        "print(f\"✔️ Teste : {num_test_files} casos (com labelsTs para avaliação)\")\n",
        "print(f\"📁 Arquivo JSON: {os.path.join(BASE_DIR, 'dataset.json')}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DQxQblz1Nx-a",
        "outputId": "d170c6fd-79c6-466c-a730-8899d9935777"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧹 Limpando e recriando diretórios de destino nnU-Net...\n",
            "🔎 Lendo arquivos de origem...\n",
            "🚀 Processando 80 casos para treino...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 80/80 [01:10<00:00,  1.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🧪 Processando 20 casos para teste...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 20/20 [00:17<00:00,  1.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "📝 Gerando dataset.json...\n",
            "\n",
            "✅ Dataset estruturado com sucesso!\n",
            "✔️ Treino: 80 casos\n",
            "✔️ Teste : 20 casos (com labelsTs para avaliação)\n",
            "📁 Arquivo JSON: /content/drive/MyDrive/nnunet_data/nnUNet_raw/Dataset001_AorticValve/dataset.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ ADICIONAR NOVA CÉLULA 6B - Augmentation Avançada:\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "import numpy as np\n",
        "import SimpleITK as sitk\n",
        "from scipy.ndimage import rotate, gaussian_filter\n",
        "\n",
        "def advanced_augmentation(image_path, label_path, output_dir_img, output_dir_lbl, base_name):\n",
        "    \"\"\"Aplica augmentations avançadas específicas para válvula aórtica\"\"\"\n",
        "\n",
        "    # Carregar imagem e label\n",
        "    img = sitk.ReadImage(image_path)\n",
        "    lbl = sitk.ReadImage(label_path)\n",
        "\n",
        "    img_array = sitk.GetArrayFromImage(img)\n",
        "    lbl_array = sitk.GetArrayFromImage(lbl)\n",
        "\n",
        "    augmented_pairs = []\n",
        "\n",
        "    # 1. Rotações pequenas (válvula aórtica é sensível a rotação)\n",
        "    for angle in [-5, 5, -10, 10]:\n",
        "        img_rot = rotate(img_array, angle, axes=(1, 2), reshape=False, order=1)\n",
        "        lbl_rot = rotate(lbl_array, angle, axes=(1, 2), reshape=False, order=0)\n",
        "        augmented_pairs.append((img_rot, lbl_rot, f\"{base_name}_rot{angle}\"))\n",
        "\n",
        "    # 2. Flip horizontal/vertical\n",
        "    img_flip_h = np.flip(img_array, axis=2)\n",
        "    lbl_flip_h = np.flip(lbl_array, axis=2)\n",
        "    augmented_pairs.append((img_flip_h, lbl_flip_h, f\"{base_name}_flipH\"))\n",
        "\n",
        "    # 3. Gaussian noise (simula ruído de CT)\n",
        "    noise_std = np.std(img_array) * 0.05\n",
        "    img_noise = img_array + np.random.normal(0, noise_std, img_array.shape)\n",
        "    augmented_pairs.append((img_noise, lbl_array, f\"{base_name}_noise\"))\n",
        "\n",
        "    # 4. Contraste ajustado (importante para CT)\n",
        "    img_contrast = np.clip(img_array * 1.2 - 50, img_array.min(), img_array.max())\n",
        "    augmented_pairs.append((img_contrast, lbl_array, f\"{base_name}_contrast\"))\n",
        "\n",
        "    # Salvar todas as versões aumentadas\n",
        "    for img_aug, lbl_aug, suffix in augmented_pairs:\n",
        "        # Criar novas imagens SimpleITK\n",
        "        img_aug_sitk = sitk.GetImageFromArray(img_aug)\n",
        "        lbl_aug_sitk = sitk.GetImageFromArray(lbl_aug.astype(np.uint8))\n",
        "\n",
        "        # Copiar metadados originais\n",
        "        img_aug_sitk.CopyInformation(img)\n",
        "        lbl_aug_sitk.CopyInformation(lbl)\n",
        "\n",
        "        # Salvar\n",
        "        sitk.WriteImage(img_aug_sitk, os.path.join(output_dir_img, f\"{suffix}_0000.nii.gz\"), useCompression=True)\n",
        "        sitk.WriteImage(lbl_aug_sitk, os.path.join(output_dir_lbl, f\"{suffix}.nii.gz\"), useCompression=True)\n",
        "\n",
        "# Aplicar augmentation apenas aos dados de treino\n",
        "print(\"🔄 Aplicando data augmentation avançada...\")\n",
        "\n",
        "SOURCE_IMAGES_DIR = os.path.join(BASE_DIR, \"originais\", \"imagens\")\n",
        "SOURCE_LABELS_DIR = os.path.join(BASE_DIR, \"originais\", \"rotulos\")\n",
        "\n",
        "# Pegar apenas os primeiros 80% para treino (mesma lógica da célula original)\n",
        "image_files = sorted([f for f in os.listdir(SOURCE_IMAGES_DIR) if not f.startswith('.')])\n",
        "num_train = min(80, len(image_files))\n",
        "\n",
        "for i in range(num_train):\n",
        "    fname = image_files[i]\n",
        "    base_name = fname.split('.')[0]\n",
        "\n",
        "    img_path = os.path.join(SOURCE_IMAGES_DIR, fname)\n",
        "    lbl_path = os.path.join(SOURCE_LABELS_DIR, fname)\n",
        "\n",
        "    if os.path.exists(img_path) and os.path.exists(lbl_path):\n",
        "        try:\n",
        "            advanced_augmentation(img_path, lbl_path, IMAGES_TR_DIR, LABELS_TR_DIR, f\"AorticValve_{base_name}\")\n",
        "            print(f\"  ✓ Augmentado: {base_name}\")\n",
        "        except Exception as e:\n",
        "            print(f\"  ❌ Erro no {base_name}: {e}\")\n",
        "\n",
        "print(\"✅ Data augmentation concluída!\")"
      ],
      "metadata": {
        "id": "YvcVy2qnbnjL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "🔁 CÉLULA 7 — Preparação e Treinamento do nnU-Netv2"
      ],
      "metadata": {
        "id": "Q_vWnbw3PAY1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ Definindo os paths globais do nnUNetv2\n",
        "import os\n",
        "import torch # Necessário para configurar variáveis de ambiente PyTorch\n",
        "\n",
        "os.environ[\"nnUNet_raw\"] = \"/content/drive/MyDrive/nnunet_data/nnUNet_raw\"\n",
        "os.environ[\"nnUNet_preprocessed\"] = \"/content/drive/MyDrive/nnunet_data/nnUNet_preprocessed\"\n",
        "os.environ[\"nnUNet_results\"] = \"/content/drive/MyDrive/nnunet_data/nnUNet_results\"\n",
        "\n",
        "# =========================================================================\n",
        "# SOLUÇÃO PARA OUT OF MEMORY (OOM):\n",
        "# 1. Tentar alocação de memória expansível (Pytorch)\n",
        "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
        "# 2. Se o erro de OOM persistir, você DEVE reduzir o batch_size e/ou patch_size.\n",
        "#    Para fazer isso, execute a célula abaixo para gerar os planos (se ainda não fez).\n",
        "#    Em seguida, VÁ MANUALMENTE ao arquivo:\n",
        "#    /content/drive/MyDrive/nnunet_data/nnUNet_preprocessed/Dataset001_AorticValve/nnUNetResEncUNetLPlans.json\n",
        "#    Edite a seção '3d_fullres' e altere:\n",
        "#    \"batch_size\": 2  ->  \"batch_size\": 1  (ou menos, se necessário)\n",
        "#    \"patch_size\": [32, 384, 384] -> [24, 320, 320] (ou tamanhos menores, se necessário)\n",
        "#    APÓS A EDIÇÃO, NÃO EXECUTE NOVAMENTE a etapa de 'plan_and_preprocess' pois ela sobrescreverá suas alterações!\n",
        "# =========================================================================\n",
        "\n",
        "print(\"--- Etapa de Planejamento e Pré-processamento ---\")\n",
        "print(\"Isso gerará os planos de treinamento e pré-processará os dados.\")\n",
        "print(\"Se você já editou o arquivo de planos manualmente (para OOM), NÃO execute esta linha novamente.\")\n",
        "\n",
        "# Gerar planos e pré-processar o dataset\n",
        "# -d 001: ID do dataset\n",
        "# -pl nnUNetPlannerResEncL: Usa o plano de arquitetura Residual Encoder UNet L\n",
        "!nnUNetv2_plan_and_preprocess -d 001 -pl nnUNetPlannerResEncL\n",
        "\n",
        "print(\"\\n--- Início do Treinamento nnUNetv2 ---\")\n",
        "print(\"Certifique-se de que o arquivo nnUNetResEncUNetLPlans.json foi ajustado se encontrou OutOfMemoryError.\")\n",
        "\n",
        "# Iniciar o treinamento\n",
        "# 001: ID do dataset\n",
        "# 3d_fullres: Configuração de treinamento (3D full resolution)\n",
        "# 0: Fold de validação cruzada (usando a fold 0). Para treinamento completo, idealmente, você treinaria 5 folds.\n",
        "# -p nnUNetResEncUNetLPlans: Especifica o plano de treinamento a ser usado\n",
        "!nnUNetv2_train 001 3d_fullres 0 -p nnUNetResEncUNetLPlans\n",
        "\n",
        "print(\"\\nTreinamento concluído (ou falhou devido a OOM - verifique a saída acima)!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m9nA2By7PKBc",
        "outputId": "b38ce231-7cd8-4599-d032-595b6da39fc8"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Etapa de Planejamento e Pré-processamento ---\n",
            "Isso gerará os planos de treinamento e pré-processará os dados.\n",
            "Se você já editou o arquivo de planos manualmente (para OOM), NÃO execute esta linha novamente.\n",
            "Fingerprint extraction...\n",
            "Dataset001_AorticValve\n",
            "Experiment planning...\n",
            "Dropping 3d_lowres config because the image size difference to 3d_fullres is too small. 3d_fullres: [ 50. 512. 512.], 3d_lowres: [50, 512, 512]\n",
            "2D U-Net configuration:\n",
            "{'data_identifier': 'nnUNetPlans_2d', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 35, 'patch_size': (np.int64(512), np.int64(512)), 'median_image_size_in_voxels': array([512., 512.]), 'spacing': array([0.35058594, 0.35058594]), 'normalization_schemes': ['CTNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.ResidualEncoderUNet', 'arch_kwargs': {'n_stages': 8, 'features_per_stage': (32, 64, 128, 256, 512, 512, 512, 512), 'conv_op': 'torch.nn.modules.conv.Conv2d', 'kernel_sizes': ((3, 3), (3, 3), (3, 3), (3, 3), (3, 3), (3, 3), (3, 3), (3, 3)), 'strides': ((1, 1), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2)), 'n_blocks_per_stage': (1, 3, 4, 6, 6, 6, 6, 6), 'n_conv_per_stage_decoder': (1, 1, 1, 1, 1, 1, 1), 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm2d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}}, '_kw_requires_import': ('conv_op', 'norm_op', 'dropout_op', 'nonlin')}, 'batch_dice': True}\n",
            "\n",
            "Using <class 'nnunetv2.imageio.simpleitk_reader_writer.SimpleITKIO'> as reader/writer\n",
            "3D fullres U-Net configuration:\n",
            "{'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 2, 'patch_size': (np.int64(32), np.int64(384), np.int64(384)), 'median_image_size_in_voxels': array([ 50., 512., 512.]), 'spacing': array([2.5       , 0.35058594, 0.35058594]), 'normalization_schemes': ['CTNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.ResidualEncoderUNet', 'arch_kwargs': {'n_stages': 7, 'features_per_stage': (32, 64, 128, 256, 320, 320, 320), 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': ((1, 3, 3), (1, 3, 3), (3, 3, 3), (3, 3, 3), (3, 3, 3), (3, 3, 3), (3, 3, 3)), 'strides': ((1, 1, 1), (1, 2, 2), (1, 2, 2), (2, 2, 2), (2, 2, 2), (2, 2, 2), (1, 2, 2)), 'n_blocks_per_stage': (1, 3, 4, 6, 6, 6, 6), 'n_conv_per_stage_decoder': (1, 1, 1, 1, 1, 1), 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}}, '_kw_requires_import': ('conv_op', 'norm_op', 'dropout_op', 'nonlin')}, 'batch_dice': False}\n",
            "\n",
            "Plans were saved to /content/drive/MyDrive/nnunet_data/nnUNet_preprocessed/Dataset001_AorticValve/nnUNetResEncUNetLPlans.json\n",
            "Preprocessing...\n",
            "Preprocessing dataset Dataset001_AorticValve\n",
            "Configuration: 2d...\n",
            "100% 80/80 [04:21<00:00,  3.27s/it]\n",
            "Configuration: 3d_fullres...\n",
            "100% 80/80 [05:05<00:00,  3.82s/it]\n",
            "Configuration: 3d_lowres...\n",
            "INFO: Configuration 3d_lowres not found in plans file nnUNetResEncUNetLPlans.json of dataset Dataset001_AorticValve. Skipping.\n",
            "\n",
            "--- Início do Treinamento nnUNetv2 ---\n",
            "Certifique-se de que o arquivo nnUNetResEncUNetLPlans.json foi ajustado se encontrou OutOfMemoryError.\n",
            "Using device: cuda:0\n",
            "\n",
            "#######################################################################\n",
            "Please cite the following paper when using nnU-Net:\n",
            "Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.\n",
            "#######################################################################\n",
            "\n",
            "2025-07-16 16:17:31.375121: Using torch.compile...\n",
            "2025-07-16 16:17:33.231574: do_dummy_2d_data_aug: True\n",
            "2025-07-16 16:17:33.237743: Using splits from existing split file: /content/drive/MyDrive/nnunet_data/nnUNet_preprocessed/Dataset001_AorticValve/splits_final.json\n",
            "2025-07-16 16:17:33.947770: The split file contains 5 splits.\n",
            "2025-07-16 16:17:33.963364: Desired fold for training: 0\n",
            "2025-07-16 16:17:33.966480: This split has 64 training and 16 validation cases.\n",
            "using pin_memory on device 0\n",
            "using pin_memory on device 0\n",
            "\n",
            "This is the configuration used by this training:\n",
            "Configuration name: 3d_fullres\n",
            " {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 2, 'patch_size': [32, 384, 384], 'median_image_size_in_voxels': [50.0, 512.0, 512.0], 'spacing': [2.5, 0.3505859375, 0.3505859375], 'normalization_schemes': ['CTNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.ResidualEncoderUNet', 'arch_kwargs': {'n_stages': 7, 'features_per_stage': [32, 64, 128, 256, 320, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[1, 3, 3], [1, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [1, 2, 2], [1, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_blocks_per_stage': [1, 3, 4, 6, 6, 6, 6], 'n_conv_per_stage_decoder': [1, 1, 1, 1, 1, 1], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': False} \n",
            "\n",
            "These are the global plan.json settings:\n",
            " {'dataset_name': 'Dataset001_AorticValve', 'plans_name': 'nnUNetResEncUNetLPlans', 'original_median_spacing_after_transp': [2.5000005960464478, 0.3505859375, 0.3505859375], 'original_median_shape_after_transp': [49, 512, 512], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'nnUNetPlannerResEncL', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 991.0, 'mean': 32.83921813964844, 'median': 35.0, 'min': -210.0, 'percentile_00_5': -82.0, 'percentile_99_5': 105.0, 'std': 29.420087814331055}}} \n",
            "\n",
            "2025-07-16 16:17:39.766026: Unable to plot network architecture: nnUNet_compile is enabled!\n",
            "2025-07-16 16:17:40.515191: \n",
            "2025-07-16 16:17:40.525194: Epoch 0\n",
            "2025-07-16 16:17:40.541355: Current learning rate: 0.01\n",
            "W0716 16:18:14.162000 81786 torch/_inductor/utils.py:1137] [0/0] Not enough SMs to use max_autotune_gemm mode\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/nnUNetv2_train\", line 8, in <module>\n",
            "    sys.exit(run_training_entry())\n",
            "             ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/nnunetv2/run/run_training.py\", line 266, in run_training_entry\n",
            "    run_training(args.dataset_name_or_id, args.configuration, args.fold, args.tr, args.p, args.pretrained_weights,\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/nnunetv2/run/run_training.py\", line 207, in run_training\n",
            "    nnunet_trainer.run_training()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/nnunetv2/training/nnUNetTrainer/nnUNetTrainer.py\", line 1371, in run_training\n",
            "    train_outputs.append(self.train_step(next(self.dataloader_train)))\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/nnunetv2/training/nnUNetTrainer/nnUNetTrainer.py\", line 989, in train_step\n",
            "    output = self.network(data)\n",
            "             ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py\", line 574, in _fn\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/dynamic_network_architectures/architectures/unet.py\", line 179, in forward\n",
            "    def forward(self, x):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py\", line 745, in _fn\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/_functorch/aot_autograd.py\", line 1184, in forward\n",
            "    return compiled_fn(full_args)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py\", line 310, in runtime_wrapper\n",
            "    all_outs = call_func_at_runtime_with_args(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/_functorch/_aot_autograd/utils.py\", line 126, in call_func_at_runtime_with_args\n",
            "    out = normalize_as_list(f(args))\n",
            "                            ^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/_functorch/_aot_autograd/utils.py\", line 100, in g\n",
            "    return f(*args)\n",
            "           ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/autograd/function.py\", line 575, in apply\n",
            "    return super().apply(*args, **kwargs)  # type: ignore[misc]\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py\", line 1585, in forward\n",
            "    fw_outs = call_func_at_runtime_with_args(\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/_functorch/_aot_autograd/utils.py\", line 126, in call_func_at_runtime_with_args\n",
            "    out = normalize_as_list(f(args))\n",
            "                            ^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py\", line 490, in wrapper\n",
            "    return compiled_fn(runtime_args)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py\", line 672, in inner_fn\n",
            "    outs = compiled_fn(args)\n",
            "           ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/_inductor/output_code.py\", line 466, in __call__\n",
            "    return self.current_callable(inputs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/_inductor/utils.py\", line 2128, in run\n",
            "    return model(new_inputs)\n",
            "           ^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/torchinductor_root/kr/ckrtimwlx4cfhfd4xxa3gylvav4c5urn7nca7n4wjamc5tvkhucr.py\", line 6548, in call\n",
            "    buf855 = empty_strided_cuda((2, 64, 32, 384, 384), (301989888, 4718592, 147456, 384, 1), torch.float16)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.12 GiB. GPU 0 has a total capacity of 14.74 GiB of which 632.12 MiB is free. Process 196324 has 14.12 GiB memory in use. Of the allocated memory 13.98 GiB is allocated by PyTorch, and 19.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Exception in thread Thread-2 (results_loop):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.11/threading.py\", line 1045, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.11/threading.py\", line 982, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/batchgenerators/dataloading/nondet_multi_threaded_augmenter.py\", line 125, in results_loop\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/batchgenerators/dataloading/nondet_multi_threaded_augmenter.py\", line 103, in results_loop\n",
            "    raise RuntimeError(\"One or more background workers are no longer alive. Exiting. Please check the \"\n",
            "RuntimeError: One or more background workers are no longer alive. Exiting. Please check the print statements above for the actual error message\n",
            "Exception in thread Thread-3 (results_loop):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.11/threading.py\", line 1045, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.11/threading.py\", line 982, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/batchgenerators/dataloading/nondet_multi_threaded_augmenter.py\", line 125, in results_loop\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/batchgenerators/dataloading/nondet_multi_threaded_augmenter.py\", line 103, in results_loop\n",
            "    raise RuntimeError(\"One or more background workers are no longer alive. Exiting. Please check the \"\n",
            "RuntimeError: One or more background workers are no longer alive. Exiting. Please check the print statements above for the actual error message\n",
            "\n",
            "Treinamento concluído (ou falhou devido a OOM - verifique a saída acima)!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Iniciar o treinamento\n",
        "# 001: ID do dataset\n",
        "# 3d_fullres: Configuração de treinamento (3D full resolution)\n",
        "# 0: Fold de validação cruzada (usando a fold 0). Para treinamento completo, idealmente, você treinaria 5 folds.\n",
        "# -p nnUNetResEncUNetLPlans: Especifica o plano de treinamento a ser usado\n",
        "!nnUNetv2_train 001 3d_fullres 0 -p nnUNetResEncUNetLPlans"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MhFKnTa-Hnki",
        "outputId": "d6f868f8-61be-4884-914e-84eea7adbfe0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda:0\n",
            "\n",
            "#######################################################################\n",
            "Please cite the following paper when using nnU-Net:\n",
            "Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.\n",
            "#######################################################################\n",
            "\n",
            "2025-07-16 16:43:26.086724: Using torch.compile...\n",
            "2025-07-16 16:43:27.030293: do_dummy_2d_data_aug: True\n",
            "2025-07-16 16:43:27.036246: Using splits from existing split file: /content/drive/MyDrive/nnunet_data/nnUNet_preprocessed/Dataset001_AorticValve/splits_final.json\n",
            "2025-07-16 16:43:27.040446: The split file contains 5 splits.\n",
            "2025-07-16 16:43:27.042759: Desired fold for training: 0\n",
            "2025-07-16 16:43:27.045249: This split has 64 training and 16 validation cases.\n",
            "using pin_memory on device 0\n",
            "using pin_memory on device 0\n",
            "\n",
            "This is the configuration used by this training:\n",
            "Configuration name: 3d_fullres\n",
            " {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 1, 'patch_size': [24, 320, 320], 'median_image_size_in_voxels': [50.0, 512.0, 512.0], 'spacing': [2.5, 0.3505859375, 0.3505859375], 'normalization_schemes': ['CTNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.ResidualEncoderUNet', 'arch_kwargs': {'n_stages': 7, 'features_per_stage': [32, 64, 128, 256, 320, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[1, 3, 3], [1, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [1, 2, 2], [1, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_blocks_per_stage': [1, 3, 4, 6, 6, 6, 6], 'n_conv_per_stage_decoder': [1, 1, 1, 1, 1, 1], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': False} \n",
            "\n",
            "These are the global plan.json settings:\n",
            " {'dataset_name': 'Dataset001_AorticValve', 'plans_name': 'nnUNetResEncUNetLPlans', 'original_median_spacing_after_transp': [2.5000005960464478, 0.3505859375, 0.3505859375], 'original_median_shape_after_transp': [49, 512, 512], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'nnUNetPlannerResEncL', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 991.0, 'mean': 32.83921813964844, 'median': 35.0, 'min': -210.0, 'percentile_00_5': -82.0, 'percentile_99_5': 105.0, 'std': 29.420087814331055}}} \n",
            "\n",
            "2025-07-16 16:43:29.174848: Unable to plot network architecture: nnUNet_compile is enabled!\n",
            "2025-07-16 16:43:29.228820: \n",
            "2025-07-16 16:43:29.241778: Epoch 0\n",
            "2025-07-16 16:43:29.279576: Current learning rate: 0.01\n",
            "W0716 16:43:51.683000 89299 torch/_inductor/utils.py:1137] [0/0] Not enough SMs to use max_autotune_gemm mode\n",
            "2025-07-16 16:49:06.257597: train_loss 0.0522\n",
            "2025-07-16 16:49:06.261341: val_loss 0.019\n",
            "2025-07-16 16:49:06.263656: Pseudo dice [np.float32(0.0)]\n",
            "2025-07-16 16:49:06.266381: Epoch time: 337.04 s\n",
            "2025-07-16 16:49:06.269270: Yayy! New best EMA pseudo Dice: 0.0\n",
            "2025-07-16 16:49:09.877315: \n",
            "2025-07-16 16:49:09.879750: Epoch 1\n",
            "2025-07-16 16:49:09.881649: Current learning rate: 0.00999\n",
            "2025-07-16 16:52:33.184851: train_loss 0.0155\n",
            "2025-07-16 16:52:33.187908: val_loss -0.0085\n",
            "2025-07-16 16:52:33.189773: Pseudo dice [np.float32(0.0)]\n",
            "2025-07-16 16:52:33.191755: Epoch time: 203.31 s\n",
            "2025-07-16 16:52:34.494228: \n",
            "2025-07-16 16:52:34.497118: Epoch 2\n",
            "2025-07-16 16:52:34.499063: Current learning rate: 0.00998\n",
            "2025-07-16 16:55:59.071829: train_loss -0.058\n",
            "2025-07-16 16:55:59.074980: val_loss -0.0688\n",
            "2025-07-16 16:55:59.077101: Pseudo dice [np.float32(0.1701)]\n",
            "2025-07-16 16:55:59.079099: Epoch time: 204.58 s\n",
            "2025-07-16 16:55:59.080935: Yayy! New best EMA pseudo Dice: 0.017000000923871994\n",
            "2025-07-16 16:56:02.986854: \n",
            "2025-07-16 16:56:02.989468: Epoch 3\n",
            "2025-07-16 16:56:02.992174: Current learning rate: 0.00997\n",
            "2025-07-16 16:59:40.462542: train_loss -0.1294\n",
            "2025-07-16 16:59:40.465771: val_loss -0.1727\n",
            "2025-07-16 16:59:40.468891: Pseudo dice [np.float32(0.3087)]\n",
            "2025-07-16 16:59:40.470880: Epoch time: 217.48 s\n",
            "2025-07-16 16:59:40.473045: Yayy! New best EMA pseudo Dice: 0.04619999974966049\n",
            "2025-07-16 16:59:45.746823: \n",
            "2025-07-16 16:59:45.749290: Epoch 4\n",
            "2025-07-16 16:59:45.751070: Current learning rate: 0.00996\n",
            "2025-07-16 17:03:23.320672: train_loss -0.1235\n",
            "2025-07-16 17:03:23.323715: val_loss -0.1273\n",
            "2025-07-16 17:03:23.325863: Pseudo dice [np.float32(0.2549)]\n",
            "2025-07-16 17:03:23.327907: Epoch time: 217.58 s\n",
            "2025-07-16 17:03:23.330267: Yayy! New best EMA pseudo Dice: 0.06700000166893005\n",
            "2025-07-16 17:03:27.152709: \n",
            "2025-07-16 17:03:27.155594: Epoch 5\n",
            "2025-07-16 17:03:27.865802: Current learning rate: 0.00995\n",
            "2025-07-16 17:07:07.298495: train_loss -0.2257\n",
            "2025-07-16 17:07:07.301542: val_loss -0.186\n",
            "2025-07-16 17:07:07.303356: Pseudo dice [np.float32(0.3431)]\n",
            "2025-07-16 17:07:07.305092: Epoch time: 220.15 s\n",
            "2025-07-16 17:07:07.306816: Yayy! New best EMA pseudo Dice: 0.09470000118017197\n",
            "2025-07-16 17:07:11.238206: \n",
            "2025-07-16 17:07:11.240740: Epoch 6\n",
            "2025-07-16 17:07:11.242698: Current learning rate: 0.00995\n",
            "2025-07-16 17:10:51.925495: train_loss -0.2576\n",
            "2025-07-16 17:10:51.929119: val_loss -0.272\n",
            "2025-07-16 17:10:51.931843: Pseudo dice [np.float32(0.4063)]\n",
            "2025-07-16 17:10:51.934455: Epoch time: 220.69 s\n",
            "2025-07-16 17:10:51.937144: Yayy! New best EMA pseudo Dice: 0.1257999986410141\n",
            "2025-07-16 17:10:55.826450: \n",
            "2025-07-16 17:10:55.829020: Epoch 7\n",
            "2025-07-16 17:10:55.831063: Current learning rate: 0.00994\n",
            "2025-07-16 17:14:35.099772: train_loss -0.2768\n",
            "2025-07-16 17:14:35.102806: val_loss -0.3455\n",
            "2025-07-16 17:14:35.105062: Pseudo dice [np.float32(0.5223)]\n",
            "2025-07-16 17:14:35.107753: Epoch time: 219.28 s\n",
            "2025-07-16 17:14:35.110246: Yayy! New best EMA pseudo Dice: 0.1655000001192093\n",
            "2025-07-16 17:14:39.020462: \n",
            "2025-07-16 17:14:39.023690: Epoch 8\n",
            "2025-07-16 17:14:39.025620: Current learning rate: 0.00993\n",
            "2025-07-16 17:18:17.399302: train_loss -0.3058\n",
            "2025-07-16 17:18:17.403065: val_loss -0.2421\n",
            "2025-07-16 17:18:17.405352: Pseudo dice [np.float32(0.4121)]\n",
            "2025-07-16 17:18:17.408181: Epoch time: 218.38 s\n",
            "2025-07-16 17:18:17.410101: Yayy! New best EMA pseudo Dice: 0.19009999930858612\n",
            "2025-07-16 17:18:21.217261: \n",
            "2025-07-16 17:18:21.219769: Epoch 9\n",
            "2025-07-16 17:18:21.221778: Current learning rate: 0.00992\n",
            "2025-07-16 17:21:58.064483: train_loss -0.3196\n",
            "2025-07-16 17:21:58.067598: val_loss -0.37\n",
            "2025-07-16 17:21:58.069442: Pseudo dice [np.float32(0.517)]\n",
            "2025-07-16 17:21:58.071253: Epoch time: 216.85 s\n",
            "2025-07-16 17:21:58.073197: Yayy! New best EMA pseudo Dice: 0.22280000150203705\n",
            "2025-07-16 17:22:02.762275: \n",
            "2025-07-16 17:22:02.765100: Epoch 10\n",
            "2025-07-16 17:22:02.767521: Current learning rate: 0.00991\n",
            "2025-07-16 17:25:40.255215: train_loss -0.4083\n",
            "2025-07-16 17:25:40.258399: val_loss -0.3131\n",
            "2025-07-16 17:25:40.260240: Pseudo dice [np.float32(0.5607)]\n",
            "2025-07-16 17:25:40.262031: Epoch time: 217.5 s\n",
            "2025-07-16 17:25:40.263807: Yayy! New best EMA pseudo Dice: 0.2565999925136566\n",
            "2025-07-16 17:25:43.990595: \n",
            "2025-07-16 17:25:43.993539: Epoch 11\n",
            "2025-07-16 17:25:43.996243: Current learning rate: 0.0099\n",
            "2025-07-16 17:29:23.108934: train_loss -0.3775\n",
            "2025-07-16 17:29:23.111645: val_loss -0.2582\n",
            "2025-07-16 17:29:23.113456: Pseudo dice [np.float32(0.5457)]\n",
            "2025-07-16 17:29:23.127931: Epoch time: 219.12 s\n",
            "2025-07-16 17:29:23.131488: Yayy! New best EMA pseudo Dice: 0.2854999899864197\n",
            "2025-07-16 17:29:26.894687: \n",
            "2025-07-16 17:29:26.897552: Epoch 12\n",
            "2025-07-16 17:29:26.899642: Current learning rate: 0.00989\n",
            "2025-07-16 17:33:07.611405: train_loss -0.4192\n",
            "2025-07-16 17:33:07.614528: val_loss -0.4041\n",
            "2025-07-16 17:33:07.617816: Pseudo dice [np.float32(0.6547)]\n",
            "2025-07-16 17:33:07.619776: Epoch time: 220.72 s\n",
            "2025-07-16 17:33:07.636710: Yayy! New best EMA pseudo Dice: 0.3224000036716461\n",
            "2025-07-16 17:33:11.490237: \n",
            "2025-07-16 17:33:11.492758: Epoch 13\n",
            "2025-07-16 17:33:11.494817: Current learning rate: 0.00988\n",
            "2025-07-16 17:36:48.647212: train_loss -0.4018\n",
            "2025-07-16 17:36:48.650232: val_loss -0.4168\n",
            "2025-07-16 17:36:48.653133: Pseudo dice [np.float32(0.6472)]\n",
            "2025-07-16 17:36:48.655103: Epoch time: 217.16 s\n",
            "2025-07-16 17:36:48.656880: Yayy! New best EMA pseudo Dice: 0.3549000024795532\n",
            "2025-07-16 17:36:52.468811: \n",
            "2025-07-16 17:36:52.471670: Epoch 14\n",
            "2025-07-16 17:36:52.473719: Current learning rate: 0.00987\n",
            "2025-07-16 17:40:33.360626: train_loss -0.385\n",
            "2025-07-16 17:40:33.364264: val_loss -0.3497\n",
            "2025-07-16 17:40:33.367321: Pseudo dice [np.float32(0.5879)]\n",
            "2025-07-16 17:40:33.369342: Epoch time: 220.9 s\n",
            "2025-07-16 17:40:33.385053: Yayy! New best EMA pseudo Dice: 0.3781999945640564\n",
            "2025-07-16 17:40:37.267128: \n",
            "2025-07-16 17:40:37.269497: Epoch 15\n",
            "2025-07-16 17:40:37.271807: Current learning rate: 0.00986\n",
            "2025-07-16 17:44:16.264630: train_loss -0.3987\n",
            "2025-07-16 17:44:16.267525: val_loss -0.3801\n",
            "2025-07-16 17:44:16.269646: Pseudo dice [np.float32(0.6338)]\n",
            "2025-07-16 17:44:16.272342: Epoch time: 219.0 s\n",
            "2025-07-16 17:44:16.274275: Yayy! New best EMA pseudo Dice: 0.40380001068115234\n",
            "2025-07-16 17:44:20.810227: \n",
            "2025-07-16 17:44:20.812770: Epoch 16\n",
            "2025-07-16 17:44:20.814649: Current learning rate: 0.00986\n",
            "2025-07-16 17:48:00.068247: train_loss -0.4053\n",
            "2025-07-16 17:48:00.071137: val_loss -0.414\n",
            "2025-07-16 17:48:00.073139: Pseudo dice [np.float32(0.6276)]\n",
            "2025-07-16 17:48:00.075036: Epoch time: 219.26 s\n",
            "2025-07-16 17:48:00.077014: Yayy! New best EMA pseudo Dice: 0.4262000024318695\n",
            "2025-07-16 17:48:03.842387: \n",
            "2025-07-16 17:48:03.845313: Epoch 17\n",
            "2025-07-16 17:48:04.534153: Current learning rate: 0.00985\n",
            "2025-07-16 17:51:43.615384: train_loss -0.4316\n",
            "2025-07-16 17:51:43.618438: val_loss -0.4333\n",
            "2025-07-16 17:51:43.620346: Pseudo dice [np.float32(0.6137)]\n",
            "2025-07-16 17:51:43.622928: Epoch time: 219.78 s\n",
            "2025-07-16 17:51:43.625666: Yayy! New best EMA pseudo Dice: 0.4449000060558319\n",
            "2025-07-16 17:51:48.418500: \n",
            "2025-07-16 17:51:48.420870: Epoch 18\n",
            "2025-07-16 17:51:48.423670: Current learning rate: 0.00984\n",
            "2025-07-16 17:55:26.669877: train_loss -0.4235\n",
            "2025-07-16 17:55:26.672886: val_loss -0.3614\n",
            "2025-07-16 17:55:26.675041: Pseudo dice [np.float32(0.5566)]\n",
            "2025-07-16 17:55:26.677837: Epoch time: 218.25 s\n",
            "2025-07-16 17:55:26.680576: Yayy! New best EMA pseudo Dice: 0.4560999870300293\n",
            "2025-07-16 17:55:30.441834: \n",
            "2025-07-16 17:55:30.444475: Epoch 19\n",
            "2025-07-16 17:55:30.446395: Current learning rate: 0.00983\n",
            "2025-07-16 17:59:08.061543: train_loss -0.3811\n",
            "2025-07-16 17:59:08.065113: val_loss -0.2929\n",
            "2025-07-16 17:59:08.067477: Pseudo dice [np.float32(0.4764)]\n",
            "2025-07-16 17:59:08.070037: Epoch time: 217.62 s\n",
            "2025-07-16 17:59:08.072201: Yayy! New best EMA pseudo Dice: 0.45809999108314514\n",
            "2025-07-16 17:59:11.989632: \n",
            "2025-07-16 17:59:11.992277: Epoch 20\n",
            "2025-07-16 17:59:11.995228: Current learning rate: 0.00982\n",
            "2025-07-16 18:02:49.407480: train_loss -0.4266\n",
            "2025-07-16 18:02:49.410943: val_loss -0.4671\n",
            "2025-07-16 18:02:49.413748: Pseudo dice [np.float32(0.6311)]\n",
            "2025-07-16 18:02:49.416481: Epoch time: 217.42 s\n",
            "2025-07-16 18:02:49.419101: Yayy! New best EMA pseudo Dice: 0.47540000081062317\n",
            "2025-07-16 18:02:53.317034: \n",
            "2025-07-16 18:02:53.320140: Epoch 21\n",
            "2025-07-16 18:02:53.322626: Current learning rate: 0.00981\n",
            "2025-07-16 18:06:35.011628: train_loss -0.4534\n",
            "2025-07-16 18:06:35.014874: val_loss -0.4741\n",
            "2025-07-16 18:06:35.016785: Pseudo dice [np.float32(0.6654)]\n",
            "2025-07-16 18:06:35.018829: Epoch time: 221.7 s\n",
            "2025-07-16 18:06:35.021692: Yayy! New best EMA pseudo Dice: 0.4943999946117401\n",
            "2025-07-16 18:06:38.925866: \n",
            "2025-07-16 18:06:38.928522: Epoch 22\n",
            "2025-07-16 18:06:38.930766: Current learning rate: 0.0098\n",
            "2025-07-16 18:10:19.889326: train_loss -0.4654\n",
            "2025-07-16 18:10:19.892584: val_loss -0.4247\n",
            "2025-07-16 18:10:19.894327: Pseudo dice [np.float32(0.6543)]\n",
            "2025-07-16 18:10:19.896559: Epoch time: 220.97 s\n",
            "2025-07-16 18:10:19.898524: Yayy! New best EMA pseudo Dice: 0.5103999972343445\n",
            "2025-07-16 18:10:23.764718: \n",
            "2025-07-16 18:10:23.767637: Epoch 23\n",
            "2025-07-16 18:10:23.769521: Current learning rate: 0.00979\n",
            "2025-07-16 18:14:01.405361: train_loss -0.4283\n",
            "2025-07-16 18:14:01.408726: val_loss -0.4104\n",
            "2025-07-16 18:14:01.411837: Pseudo dice [np.float32(0.6223)]\n",
            "2025-07-16 18:14:01.414685: Epoch time: 217.64 s\n",
            "2025-07-16 18:14:01.418067: Yayy! New best EMA pseudo Dice: 0.5216000080108643\n",
            "2025-07-16 18:14:05.264071: \n",
            "2025-07-16 18:14:05.266575: Epoch 24\n",
            "2025-07-16 18:14:05.268520: Current learning rate: 0.00978\n",
            "2025-07-16 18:17:44.777666: train_loss -0.4424\n",
            "2025-07-16 18:17:44.780841: val_loss -0.2916\n",
            "2025-07-16 18:17:44.782743: Pseudo dice [np.float32(0.5156)]\n",
            "2025-07-16 18:17:44.784514: Epoch time: 219.52 s\n",
            "2025-07-16 18:17:46.052855: \n",
            "2025-07-16 18:17:46.055361: Epoch 25\n",
            "2025-07-16 18:17:46.057085: Current learning rate: 0.00977\n",
            "2025-07-16 18:21:11.371463: train_loss -0.4012\n",
            "2025-07-16 18:21:11.374920: val_loss -0.3563\n",
            "2025-07-16 18:21:11.377074: Pseudo dice [np.float32(0.543)]\n",
            "2025-07-16 18:21:11.379005: Epoch time: 205.32 s\n",
            "2025-07-16 18:21:11.380985: Yayy! New best EMA pseudo Dice: 0.5231999754905701\n",
            "2025-07-16 18:21:15.435377: \n",
            "2025-07-16 18:21:15.438448: Epoch 26\n",
            "2025-07-16 18:21:15.441046: Current learning rate: 0.00977\n",
            "2025-07-16 18:24:52.844896: train_loss -0.4346\n",
            "2025-07-16 18:24:52.848675: val_loss -0.4235\n",
            "2025-07-16 18:24:52.851149: Pseudo dice [np.float32(0.5684)]\n",
            "2025-07-16 18:24:52.853429: Epoch time: 217.41 s\n",
            "2025-07-16 18:24:52.855480: Yayy! New best EMA pseudo Dice: 0.5277000069618225\n",
            "2025-07-16 18:24:56.832671: \n",
            "2025-07-16 18:24:56.835601: Epoch 27\n",
            "2025-07-16 18:24:56.837752: Current learning rate: 0.00976\n",
            "2025-07-16 18:28:36.671444: train_loss -0.4657\n",
            "2025-07-16 18:28:36.675047: val_loss -0.4026\n",
            "2025-07-16 18:28:36.677248: Pseudo dice [np.float32(0.6307)]\n",
            "2025-07-16 18:28:36.680209: Epoch time: 219.84 s\n",
            "2025-07-16 18:28:36.681968: Yayy! New best EMA pseudo Dice: 0.5379999876022339\n",
            "2025-07-16 18:28:40.516849: \n",
            "2025-07-16 18:28:40.519209: Epoch 28\n",
            "2025-07-16 18:28:40.521082: Current learning rate: 0.00975\n",
            "2025-07-16 18:32:19.465820: train_loss -0.4692\n",
            "2025-07-16 18:32:19.469284: val_loss -0.4834\n",
            "2025-07-16 18:32:19.472972: Pseudo dice [np.float32(0.6485)]\n",
            "2025-07-16 18:32:19.475134: Epoch time: 218.95 s\n",
            "2025-07-16 18:32:19.477376: Yayy! New best EMA pseudo Dice: 0.5490999817848206\n",
            "2025-07-16 18:32:23.279730: \n",
            "2025-07-16 18:32:23.282897: Epoch 29\n",
            "2025-07-16 18:32:23.285141: Current learning rate: 0.00974\n",
            "2025-07-16 18:36:01.407851: train_loss -0.459\n",
            "2025-07-16 18:36:01.423405: val_loss -0.4251\n",
            "2025-07-16 18:36:01.425397: Pseudo dice [np.float32(0.6262)]\n",
            "2025-07-16 18:36:01.427980: Epoch time: 218.13 s\n",
            "2025-07-16 18:36:01.430403: Yayy! New best EMA pseudo Dice: 0.5568000078201294\n",
            "2025-07-16 18:36:05.350262: \n",
            "2025-07-16 18:36:05.352855: Epoch 30\n",
            "2025-07-16 18:36:05.355589: Current learning rate: 0.00973\n",
            "2025-07-16 18:39:43.388405: train_loss -0.5117\n",
            "2025-07-16 18:39:43.391924: val_loss -0.4267\n",
            "2025-07-16 18:39:43.407487: Pseudo dice [np.float32(0.6599)]\n",
            "2025-07-16 18:39:43.409395: Epoch time: 218.04 s\n",
            "2025-07-16 18:39:43.411165: Yayy! New best EMA pseudo Dice: 0.5670999884605408\n",
            "2025-07-16 18:39:48.778713: \n",
            "2025-07-16 18:39:48.781202: Epoch 31\n",
            "2025-07-16 18:39:48.783163: Current learning rate: 0.00972\n",
            "2025-07-16 18:43:28.281221: train_loss -0.4734\n",
            "2025-07-16 18:43:28.284271: val_loss -0.4535\n",
            "2025-07-16 18:43:28.285995: Pseudo dice [np.float32(0.6881)]\n",
            "2025-07-16 18:43:28.287709: Epoch time: 219.51 s\n",
            "2025-07-16 18:43:28.289511: Yayy! New best EMA pseudo Dice: 0.579200029373169\n",
            "2025-07-16 18:43:32.099758: \n",
            "2025-07-16 18:43:32.102860: Epoch 32\n",
            "2025-07-16 18:43:32.105197: Current learning rate: 0.00971\n",
            "2025-07-16 18:47:11.897536: train_loss -0.4771\n",
            "2025-07-16 18:47:11.900453: val_loss -0.4112\n",
            "2025-07-16 18:47:11.902416: Pseudo dice [np.float32(0.6736)]\n",
            "2025-07-16 18:47:11.904336: Epoch time: 219.8 s\n",
            "2025-07-16 18:47:11.906604: Yayy! New best EMA pseudo Dice: 0.5885999798774719\n",
            "2025-07-16 18:47:17.376064: \n",
            "2025-07-16 18:47:17.378557: Epoch 33\n",
            "2025-07-16 18:47:17.381939: Current learning rate: 0.0097\n",
            "2025-07-16 18:50:58.500175: train_loss -0.4568\n",
            "2025-07-16 18:50:58.504059: val_loss -0.4725\n",
            "2025-07-16 18:50:58.506464: Pseudo dice [np.float32(0.6726)]\n",
            "2025-07-16 18:50:58.508977: Epoch time: 221.13 s\n",
            "2025-07-16 18:50:58.511578: Yayy! New best EMA pseudo Dice: 0.597000002861023\n",
            "2025-07-16 18:51:03.602731: \n",
            "2025-07-16 18:51:03.605671: Epoch 34\n",
            "2025-07-16 18:51:03.608085: Current learning rate: 0.00969\n",
            "2025-07-16 18:54:41.653932: train_loss -0.498\n",
            "2025-07-16 18:54:41.657344: val_loss -0.3859\n",
            "2025-07-16 18:54:41.659414: Pseudo dice [np.float32(0.6383)]\n",
            "2025-07-16 18:54:41.661602: Epoch time: 218.06 s\n",
            "2025-07-16 18:54:41.663304: Yayy! New best EMA pseudo Dice: 0.6011000275611877\n",
            "2025-07-16 18:54:45.817947: \n",
            "2025-07-16 18:54:45.820811: Epoch 35\n",
            "2025-07-16 18:54:45.822968: Current learning rate: 0.00968\n",
            "2025-07-16 18:58:26.353520: train_loss -0.4652\n",
            "2025-07-16 18:58:26.357093: val_loss -0.5114\n",
            "2025-07-16 18:58:26.360236: Pseudo dice [np.float32(0.6864)]\n",
            "2025-07-16 18:58:26.362950: Epoch time: 220.54 s\n",
            "2025-07-16 18:58:26.364839: Yayy! New best EMA pseudo Dice: 0.6097000241279602\n",
            "2025-07-16 18:58:30.355052: \n",
            "2025-07-16 18:58:30.357843: Epoch 36\n",
            "2025-07-16 18:58:30.360253: Current learning rate: 0.00968\n",
            "2025-07-16 19:02:09.503657: train_loss -0.4623\n",
            "2025-07-16 19:02:09.507017: val_loss -0.3438\n",
            "2025-07-16 19:02:09.510127: Pseudo dice [np.float32(0.6662)]\n",
            "2025-07-16 19:02:09.513036: Epoch time: 219.15 s\n",
            "2025-07-16 19:02:09.516053: Yayy! New best EMA pseudo Dice: 0.6152999997138977\n",
            "2025-07-16 19:02:13.436781: \n",
            "2025-07-16 19:02:13.439642: Epoch 37\n",
            "2025-07-16 19:02:13.441857: Current learning rate: 0.00967\n",
            "2025-07-16 19:05:52.429286: train_loss -0.4842\n",
            "2025-07-16 19:05:52.432925: val_loss -0.4957\n",
            "2025-07-16 19:05:52.435281: Pseudo dice [np.float32(0.6676)]\n",
            "2025-07-16 19:05:52.437401: Epoch time: 219.0 s\n",
            "2025-07-16 19:05:52.439364: Yayy! New best EMA pseudo Dice: 0.6205000281333923\n",
            "2025-07-16 19:05:56.345871: \n",
            "2025-07-16 19:05:56.348769: Epoch 38\n",
            "2025-07-16 19:05:56.350838: Current learning rate: 0.00966\n",
            "2025-07-16 19:09:33.318690: train_loss -0.4572\n",
            "2025-07-16 19:09:33.321838: val_loss -0.5115\n",
            "2025-07-16 19:09:33.324424: Pseudo dice [np.float32(0.6635)]\n",
            "2025-07-16 19:09:33.326874: Epoch time: 216.98 s\n",
            "2025-07-16 19:09:33.329205: Yayy! New best EMA pseudo Dice: 0.6248000264167786\n",
            "2025-07-16 19:09:38.724758: \n",
            "2025-07-16 19:09:38.727849: Epoch 39\n",
            "2025-07-16 19:09:38.729916: Current learning rate: 0.00965\n",
            "2025-07-16 19:13:18.833867: train_loss -0.4852\n",
            "2025-07-16 19:13:18.836973: val_loss -0.3705\n",
            "2025-07-16 19:13:18.839036: Pseudo dice [np.float32(0.6173)]\n",
            "2025-07-16 19:13:18.840988: Epoch time: 220.11 s\n",
            "2025-07-16 19:13:20.174585: \n",
            "2025-07-16 19:13:20.177074: Epoch 40\n",
            "2025-07-16 19:13:20.179070: Current learning rate: 0.00964\n",
            "2025-07-16 19:16:45.190168: train_loss -0.4794\n",
            "2025-07-16 19:16:45.193785: val_loss -0.4664\n",
            "2025-07-16 19:16:45.195767: Pseudo dice [np.float32(0.6689)]\n",
            "2025-07-16 19:16:45.197703: Epoch time: 205.02 s\n",
            "2025-07-16 19:16:45.199761: Yayy! New best EMA pseudo Dice: 0.628600001335144\n",
            "2025-07-16 19:16:49.605718: \n",
            "2025-07-16 19:16:49.608289: Epoch 41\n",
            "2025-07-16 19:16:49.610362: Current learning rate: 0.00963\n",
            "2025-07-16 19:20:28.414127: train_loss -0.4886\n",
            "2025-07-16 19:20:28.417019: val_loss -0.487\n",
            "2025-07-16 19:20:28.418899: Pseudo dice [np.float32(0.7035)]\n",
            "2025-07-16 19:20:28.420622: Epoch time: 218.81 s\n",
            "2025-07-16 19:20:28.422286: Yayy! New best EMA pseudo Dice: 0.6360999941825867\n",
            "2025-07-16 19:20:32.326374: \n",
            "2025-07-16 19:20:32.329347: Epoch 42\n",
            "2025-07-16 19:20:32.331499: Current learning rate: 0.00962\n",
            "2025-07-16 19:24:09.836708: train_loss -0.4671\n",
            "2025-07-16 19:24:09.839750: val_loss -0.3817\n",
            "2025-07-16 19:24:09.841579: Pseudo dice [np.float32(0.5851)]\n",
            "2025-07-16 19:24:09.859336: Epoch time: 217.51 s\n",
            "2025-07-16 19:24:11.120362: \n",
            "2025-07-16 19:24:11.123115: Epoch 43\n",
            "2025-07-16 19:24:11.125455: Current learning rate: 0.00961\n",
            "2025-07-16 19:27:36.280566: train_loss -0.461\n",
            "2025-07-16 19:27:36.283586: val_loss -0.4132\n",
            "2025-07-16 19:27:36.286895: Pseudo dice [np.float32(0.6091)]\n",
            "2025-07-16 19:27:36.289792: Epoch time: 205.16 s\n",
            "2025-07-16 19:27:37.586566: \n",
            "2025-07-16 19:27:37.589396: Epoch 44\n",
            "2025-07-16 19:27:37.591330: Current learning rate: 0.0096\n",
            "2025-07-16 19:31:02.233568: train_loss -0.4399\n",
            "2025-07-16 19:31:02.236304: val_loss -0.5135\n",
            "2025-07-16 19:31:02.237931: Pseudo dice [np.float32(0.6803)]\n",
            "2025-07-16 19:31:02.239736: Epoch time: 204.65 s\n",
            "2025-07-16 19:31:03.515772: \n",
            "2025-07-16 19:31:03.518274: Epoch 45\n",
            "2025-07-16 19:31:03.520234: Current learning rate: 0.00959\n",
            "2025-07-16 19:34:28.405008: train_loss -0.4906\n",
            "2025-07-16 19:34:28.408600: val_loss -0.5043\n",
            "2025-07-16 19:34:28.411100: Pseudo dice [np.float32(0.7104)]\n",
            "2025-07-16 19:34:28.413265: Epoch time: 204.89 s\n",
            "2025-07-16 19:34:28.416262: Yayy! New best EMA pseudo Dice: 0.6416000127792358\n",
            "2025-07-16 19:34:32.348977: \n",
            "2025-07-16 19:34:32.351824: Epoch 46\n",
            "2025-07-16 19:34:32.354206: Current learning rate: 0.00959\n",
            "2025-07-16 19:38:10.753350: train_loss -0.4613\n",
            "2025-07-16 19:38:10.756683: val_loss -0.3804\n",
            "2025-07-16 19:38:10.758943: Pseudo dice [np.float32(0.6516)]\n",
            "2025-07-16 19:38:10.761090: Epoch time: 218.41 s\n",
            "2025-07-16 19:38:10.763037: Yayy! New best EMA pseudo Dice: 0.6425999999046326\n",
            "2025-07-16 19:38:14.581113: \n",
            "2025-07-16 19:38:14.584116: Epoch 47\n",
            "2025-07-16 19:38:14.586535: Current learning rate: 0.00958\n",
            "2025-07-16 19:41:51.663059: train_loss -0.4791\n",
            "2025-07-16 19:41:51.666661: val_loss -0.4321\n",
            "2025-07-16 19:41:51.668931: Pseudo dice [np.float32(0.639)]\n",
            "2025-07-16 19:41:51.671319: Epoch time: 217.09 s\n",
            "2025-07-16 19:41:53.012035: \n",
            "2025-07-16 19:41:53.014562: Epoch 48\n",
            "2025-07-16 19:41:53.017451: Current learning rate: 0.00957\n",
            "2025-07-16 19:45:18.123521: train_loss -0.4725\n",
            "2025-07-16 19:45:18.127102: val_loss -0.4731\n",
            "2025-07-16 19:45:18.129432: Pseudo dice [np.float32(0.707)]\n",
            "2025-07-16 19:45:18.131621: Epoch time: 205.12 s\n",
            "2025-07-16 19:45:18.133845: Yayy! New best EMA pseudo Dice: 0.6486999988555908\n",
            "2025-07-16 19:45:23.469674: \n",
            "2025-07-16 19:45:23.472017: Epoch 49\n",
            "2025-07-16 19:45:23.474400: Current learning rate: 0.00956\n",
            "2025-07-16 19:49:00.562215: train_loss -0.497\n",
            "2025-07-16 19:49:00.565075: val_loss -0.3871\n",
            "2025-07-16 19:49:00.566824: Pseudo dice [np.float32(0.5868)]\n",
            "2025-07-16 19:49:00.568877: Epoch time: 217.1 s\n",
            "2025-07-16 19:49:04.314087: \n",
            "2025-07-16 19:49:04.317036: Epoch 50\n",
            "2025-07-16 19:49:04.319110: Current learning rate: 0.00955\n",
            "2025-07-16 19:52:29.947948: train_loss -0.487\n",
            "2025-07-16 19:52:29.951702: val_loss -0.4021\n",
            "2025-07-16 19:52:29.953562: Pseudo dice [np.float32(0.627)]\n",
            "2025-07-16 19:52:29.955357: Epoch time: 205.64 s\n",
            "2025-07-16 19:52:31.264013: \n",
            "2025-07-16 19:52:31.266602: Epoch 51\n",
            "2025-07-16 19:52:31.268512: Current learning rate: 0.00954\n",
            "2025-07-16 19:55:57.105130: train_loss -0.5148\n",
            "2025-07-16 19:55:57.108777: val_loss -0.3726\n",
            "2025-07-16 19:55:57.111197: Pseudo dice [np.float32(0.6087)]\n",
            "2025-07-16 19:55:57.113620: Epoch time: 205.85 s\n",
            "2025-07-16 19:55:58.373763: \n",
            "2025-07-16 19:55:58.376198: Epoch 52\n",
            "2025-07-16 19:55:58.377894: Current learning rate: 0.00953\n",
            "2025-07-16 19:59:24.399223: train_loss -0.4743\n",
            "2025-07-16 19:59:24.417506: val_loss -0.4121\n",
            "2025-07-16 19:59:24.419282: Pseudo dice [np.float32(0.6686)]\n",
            "2025-07-16 19:59:24.421166: Epoch time: 206.03 s\n",
            "2025-07-16 19:59:25.683415: \n",
            "2025-07-16 19:59:25.685812: Epoch 53\n",
            "2025-07-16 19:59:25.687611: Current learning rate: 0.00952\n",
            "2025-07-16 20:02:51.665216: train_loss -0.5132\n",
            "2025-07-16 20:02:51.668573: val_loss -0.3943\n",
            "2025-07-16 20:02:51.670588: Pseudo dice [np.float32(0.6856)]\n",
            "2025-07-16 20:02:51.672527: Epoch time: 205.99 s\n",
            "2025-07-16 20:02:52.945582: \n",
            "2025-07-16 20:02:52.948156: Epoch 54\n",
            "2025-07-16 20:02:52.962770: Current learning rate: 0.00951\n",
            "2025-07-16 20:06:18.499079: train_loss -0.4892\n",
            "2025-07-16 20:06:18.502340: val_loss -0.4151\n",
            "2025-07-16 20:06:18.505445: Pseudo dice [np.float32(0.6542)]\n",
            "2025-07-16 20:06:18.508521: Epoch time: 205.56 s\n",
            "2025-07-16 20:06:19.882646: \n",
            "2025-07-16 20:06:19.886226: Epoch 55\n",
            "2025-07-16 20:06:19.889613: Current learning rate: 0.0095\n",
            "2025-07-16 20:09:45.526703: train_loss -0.5487\n",
            "2025-07-16 20:09:45.530011: val_loss -0.4687\n",
            "2025-07-16 20:09:45.531961: Pseudo dice [np.float32(0.6784)]\n",
            "2025-07-16 20:09:45.533807: Epoch time: 205.65 s\n",
            "2025-07-16 20:09:45.535594: Yayy! New best EMA pseudo Dice: 0.649399995803833\n",
            "2025-07-16 20:09:49.451176: \n",
            "2025-07-16 20:09:49.453794: Epoch 56\n",
            "2025-07-16 20:09:49.455707: Current learning rate: 0.00949\n",
            "2025-07-16 20:13:29.753946: train_loss -0.4889\n",
            "2025-07-16 20:13:29.757611: val_loss -0.4995\n",
            "2025-07-16 20:13:29.759834: Pseudo dice [np.float32(0.7172)]\n",
            "2025-07-16 20:13:29.762153: Epoch time: 220.31 s\n",
            "2025-07-16 20:13:29.764492: Yayy! New best EMA pseudo Dice: 0.6561999917030334\n",
            "2025-07-16 20:13:33.739622: \n",
            "2025-07-16 20:13:33.742808: Epoch 57\n",
            "2025-07-16 20:13:33.745008: Current learning rate: 0.00949\n",
            "2025-07-16 20:17:14.318838: train_loss -0.5018\n",
            "2025-07-16 20:17:14.322206: val_loss -0.3821\n",
            "2025-07-16 20:17:14.338954: Pseudo dice [np.float32(0.675)]\n",
            "2025-07-16 20:17:14.342473: Epoch time: 220.58 s\n",
            "2025-07-16 20:17:14.344782: Yayy! New best EMA pseudo Dice: 0.6581000089645386\n",
            "2025-07-16 20:17:19.250889: \n",
            "2025-07-16 20:17:19.253470: Epoch 58\n",
            "2025-07-16 20:17:19.256198: Current learning rate: 0.00948\n",
            "2025-07-16 20:21:01.064193: train_loss -0.5506\n",
            "2025-07-16 20:21:01.067272: val_loss -0.4655\n",
            "2025-07-16 20:21:01.069415: Pseudo dice [np.float32(0.7055)]\n",
            "2025-07-16 20:21:01.071623: Epoch time: 221.82 s\n",
            "2025-07-16 20:21:01.088108: Yayy! New best EMA pseudo Dice: 0.6628000140190125\n",
            "2025-07-16 20:21:04.975528: \n",
            "2025-07-16 20:21:04.978122: Epoch 59\n",
            "2025-07-16 20:21:04.980763: Current learning rate: 0.00947\n",
            "2025-07-16 20:24:45.315889: train_loss -0.4686\n",
            "2025-07-16 20:24:45.318823: val_loss -0.4283\n",
            "2025-07-16 20:24:45.320616: Pseudo dice [np.float32(0.6343)]\n",
            "2025-07-16 20:24:45.322425: Epoch time: 220.34 s\n",
            "2025-07-16 20:24:46.582119: \n",
            "2025-07-16 20:24:46.584523: Epoch 60\n",
            "2025-07-16 20:24:46.586493: Current learning rate: 0.00946\n",
            "2025-07-16 20:28:12.503515: train_loss -0.4795\n",
            "2025-07-16 20:28:12.506874: val_loss -0.4427\n",
            "2025-07-16 20:28:12.510225: Pseudo dice [np.float32(0.6802)]\n",
            "2025-07-16 20:28:12.512979: Epoch time: 205.93 s\n",
            "2025-07-16 20:28:13.788725: \n",
            "2025-07-16 20:28:13.791396: Epoch 61\n",
            "2025-07-16 20:28:13.794077: Current learning rate: 0.00945\n",
            "2025-07-16 20:31:39.734885: train_loss -0.517\n",
            "2025-07-16 20:31:39.738234: val_loss -0.4701\n",
            "2025-07-16 20:31:39.741206: Pseudo dice [np.float32(0.6582)]\n",
            "2025-07-16 20:31:39.744093: Epoch time: 205.95 s\n",
            "2025-07-16 20:31:41.048711: \n",
            "2025-07-16 20:31:41.051822: Epoch 62\n",
            "2025-07-16 20:31:41.053975: Current learning rate: 0.00944\n",
            "2025-07-16 20:35:06.860537: train_loss -0.5222\n",
            "2025-07-16 20:35:06.877673: val_loss -0.4176\n",
            "2025-07-16 20:35:06.879834: Pseudo dice [np.float32(0.6744)]\n",
            "2025-07-16 20:35:06.881970: Epoch time: 205.82 s\n",
            "2025-07-16 20:35:06.884791: Yayy! New best EMA pseudo Dice: 0.6628999710083008\n",
            "2025-07-16 20:35:10.739665: \n",
            "2025-07-16 20:35:10.742220: Epoch 63\n",
            "2025-07-16 20:35:10.744267: Current learning rate: 0.00943\n",
            "2025-07-16 20:38:49.614396: train_loss -0.4943\n",
            "2025-07-16 20:38:49.617215: val_loss -0.3828\n",
            "2025-07-16 20:38:49.619055: Pseudo dice [np.float32(0.68)]\n",
            "2025-07-16 20:38:49.620848: Epoch time: 218.88 s\n",
            "2025-07-16 20:38:49.622796: Yayy! New best EMA pseudo Dice: 0.6646000146865845\n",
            "2025-07-16 20:38:53.610779: \n",
            "2025-07-16 20:38:53.613889: Epoch 64\n",
            "2025-07-16 20:38:53.616320: Current learning rate: 0.00942\n",
            "2025-07-16 20:42:32.172565: train_loss -0.5094\n",
            "2025-07-16 20:42:32.176417: val_loss -0.4726\n",
            "2025-07-16 20:42:32.178424: Pseudo dice [np.float32(0.6815)]\n",
            "2025-07-16 20:42:32.180511: Epoch time: 218.57 s\n",
            "2025-07-16 20:42:32.194789: Yayy! New best EMA pseudo Dice: 0.6662999987602234\n",
            "2025-07-16 20:42:36.057053: \n",
            "2025-07-16 20:42:36.060144: Epoch 65\n",
            "2025-07-16 20:42:36.062358: Current learning rate: 0.00941\n",
            "2025-07-16 20:46:14.629034: train_loss -0.4923\n",
            "2025-07-16 20:46:14.632514: val_loss -0.4412\n",
            "2025-07-16 20:46:14.635360: Pseudo dice [np.float32(0.6398)]\n",
            "2025-07-16 20:46:14.638144: Epoch time: 218.58 s\n",
            "2025-07-16 20:46:16.011852: \n",
            "2025-07-16 20:46:16.014233: Epoch 66\n",
            "2025-07-16 20:46:16.016152: Current learning rate: 0.0094\n",
            "2025-07-16 20:49:41.657689: train_loss -0.5265\n",
            "2025-07-16 20:49:41.661176: val_loss -0.4845\n",
            "2025-07-16 20:49:41.663261: Pseudo dice [np.float32(0.675)]\n",
            "2025-07-16 20:49:41.665219: Epoch time: 205.65 s\n",
            "2025-07-16 20:49:44.150028: \n",
            "2025-07-16 20:49:44.152485: Epoch 67\n",
            "2025-07-16 20:49:44.154608: Current learning rate: 0.00939\n",
            "2025-07-16 20:53:10.105658: train_loss -0.5324\n",
            "2025-07-16 20:53:10.108672: val_loss -0.461\n",
            "2025-07-16 20:53:10.110637: Pseudo dice [np.float32(0.6605)]\n",
            "2025-07-16 20:53:10.112675: Epoch time: 205.96 s\n",
            "2025-07-16 20:53:11.439580: \n",
            "2025-07-16 20:53:11.442429: Epoch 68\n",
            "2025-07-16 20:53:11.445178: Current learning rate: 0.00939\n",
            "2025-07-16 20:56:37.360343: train_loss -0.4855\n",
            "2025-07-16 20:56:37.363964: val_loss -0.4896\n",
            "2025-07-16 20:56:37.366210: Pseudo dice [np.float32(0.69)]\n",
            "2025-07-16 20:56:37.368670: Epoch time: 205.92 s\n",
            "2025-07-16 20:56:37.371058: Yayy! New best EMA pseudo Dice: 0.6668999791145325\n",
            "2025-07-16 20:56:41.400740: \n",
            "2025-07-16 20:56:41.403145: Epoch 69\n",
            "2025-07-16 20:56:41.404935: Current learning rate: 0.00938\n",
            "2025-07-16 21:00:17.253563: train_loss -0.5332\n",
            "2025-07-16 21:00:17.257083: val_loss -0.4357\n",
            "2025-07-16 21:00:17.259464: Pseudo dice [np.float32(0.6265)]\n",
            "2025-07-16 21:00:17.261659: Epoch time: 215.86 s\n",
            "2025-07-16 21:00:18.647719: \n",
            "2025-07-16 21:00:18.651093: Epoch 70\n",
            "2025-07-16 21:00:18.654050: Current learning rate: 0.00937\n",
            "2025-07-16 21:03:44.751441: train_loss -0.5014\n",
            "2025-07-16 21:03:44.755055: val_loss -0.4051\n",
            "2025-07-16 21:03:44.757097: Pseudo dice [np.float32(0.6403)]\n",
            "2025-07-16 21:03:44.771680: Epoch time: 206.11 s\n",
            "2025-07-16 21:03:46.146906: \n",
            "2025-07-16 21:03:46.149472: Epoch 71\n",
            "2025-07-16 21:03:46.151522: Current learning rate: 0.00936\n",
            "2025-07-16 21:07:11.510255: train_loss -0.5027\n",
            "2025-07-16 21:07:11.513504: val_loss -0.3347\n",
            "2025-07-16 21:07:11.515760: Pseudo dice [np.float32(0.6422)]\n",
            "2025-07-16 21:07:11.519269: Epoch time: 205.37 s\n",
            "2025-07-16 21:07:12.803029: \n",
            "2025-07-16 21:07:12.805974: Epoch 72\n",
            "2025-07-16 21:07:12.808091: Current learning rate: 0.00935\n",
            "2025-07-16 21:10:38.187982: train_loss -0.4988\n",
            "2025-07-16 21:10:38.191251: val_loss -0.3951\n",
            "2025-07-16 21:10:38.193352: Pseudo dice [np.float32(0.6848)]\n",
            "2025-07-16 21:10:38.195347: Epoch time: 205.39 s\n",
            "2025-07-16 21:10:39.470568: \n",
            "2025-07-16 21:10:39.473433: Epoch 73\n",
            "2025-07-16 21:10:39.475700: Current learning rate: 0.00934\n",
            "2025-07-16 21:14:04.423972: train_loss -0.4655\n",
            "2025-07-16 21:14:04.427109: val_loss -0.4275\n",
            "2025-07-16 21:14:04.429996: Pseudo dice [np.float32(0.6387)]\n",
            "2025-07-16 21:14:04.431911: Epoch time: 204.96 s\n",
            "2025-07-16 21:14:05.779456: \n",
            "2025-07-16 21:14:05.782124: Epoch 74\n",
            "2025-07-16 21:14:05.784120: Current learning rate: 0.00933\n",
            "2025-07-16 21:17:30.876691: train_loss -0.503\n",
            "2025-07-16 21:17:30.879890: val_loss -0.437\n",
            "2025-07-16 21:17:30.881716: Pseudo dice [np.float32(0.6602)]\n",
            "2025-07-16 21:17:30.897006: Epoch time: 205.1 s\n",
            "2025-07-16 21:17:32.221176: \n",
            "2025-07-16 21:17:32.223516: Epoch 75\n",
            "2025-07-16 21:17:32.225621: Current learning rate: 0.00932\n",
            "2025-07-16 21:20:57.401604: train_loss -0.4965\n",
            "2025-07-16 21:20:57.405669: val_loss -0.371\n",
            "2025-07-16 21:20:57.408168: Pseudo dice [np.float32(0.6045)]\n",
            "2025-07-16 21:20:57.410539: Epoch time: 205.18 s\n",
            "2025-07-16 21:20:58.803412: \n",
            "2025-07-16 21:20:58.806102: Epoch 76\n",
            "2025-07-16 21:20:58.808255: Current learning rate: 0.00931\n",
            "2025-07-16 21:24:24.127317: train_loss -0.4986\n",
            "2025-07-16 21:24:24.130682: val_loss -0.4923\n",
            "2025-07-16 21:24:24.132387: Pseudo dice [np.float32(0.704)]\n",
            "2025-07-16 21:24:24.134407: Epoch time: 205.33 s\n",
            "2025-07-16 21:24:25.459855: \n",
            "2025-07-16 21:24:25.462435: Epoch 77\n",
            "2025-07-16 21:24:25.465299: Current learning rate: 0.0093\n",
            "2025-07-16 21:27:50.121513: train_loss -0.501\n",
            "2025-07-16 21:27:50.124619: val_loss -0.4469\n",
            "2025-07-16 21:27:50.126615: Pseudo dice [np.float32(0.6365)]\n",
            "2025-07-16 21:27:50.128427: Epoch time: 204.67 s\n",
            "2025-07-16 21:27:51.430408: \n",
            "2025-07-16 21:27:51.432837: Epoch 78\n",
            "2025-07-16 21:27:51.434685: Current learning rate: 0.0093\n",
            "2025-07-16 21:31:15.856954: train_loss -0.5039\n",
            "2025-07-16 21:31:15.860090: val_loss -0.4963\n",
            "2025-07-16 21:31:15.862955: Pseudo dice [np.float32(0.6867)]\n",
            "2025-07-16 21:31:15.864703: Epoch time: 204.43 s\n",
            "2025-07-16 21:31:17.169026: \n",
            "2025-07-16 21:31:17.172100: Epoch 79\n",
            "2025-07-16 21:31:17.174458: Current learning rate: 0.00929\n",
            "2025-07-16 21:34:42.160685: train_loss -0.4896\n",
            "2025-07-16 21:34:42.164112: val_loss -0.4688\n",
            "2025-07-16 21:34:42.166017: Pseudo dice [np.float32(0.6775)]\n",
            "2025-07-16 21:34:42.168424: Epoch time: 205.0 s\n",
            "2025-07-16 21:34:43.486096: \n",
            "2025-07-16 21:34:43.488642: Epoch 80\n",
            "2025-07-16 21:34:43.505807: Current learning rate: 0.00928\n",
            "2025-07-16 21:38:08.688160: train_loss -0.52\n",
            "2025-07-16 21:38:08.691440: val_loss -0.4665\n",
            "2025-07-16 21:38:08.693439: Pseudo dice [np.float32(0.6572)]\n",
            "2025-07-16 21:38:08.695457: Epoch time: 205.21 s\n",
            "2025-07-16 21:38:10.026983: \n",
            "2025-07-16 21:38:10.030099: Epoch 81\n",
            "2025-07-16 21:38:10.032763: Current learning rate: 0.00927\n",
            "2025-07-16 21:41:34.539722: train_loss -0.5067\n",
            "2025-07-16 21:41:34.543111: val_loss -0.4654\n",
            "2025-07-16 21:41:34.545343: Pseudo dice [np.float32(0.6902)]\n",
            "2025-07-16 21:41:34.547381: Epoch time: 204.52 s\n",
            "2025-07-16 21:41:35.856946: \n",
            "2025-07-16 21:41:35.859840: Epoch 82\n",
            "2025-07-16 21:41:35.861830: Current learning rate: 0.00926\n",
            "2025-07-16 21:45:00.975816: train_loss -0.5207\n",
            "2025-07-16 21:45:00.979077: val_loss -0.4628\n",
            "2025-07-16 21:45:00.996898: Pseudo dice [np.float32(0.6667)]\n",
            "2025-07-16 21:45:00.999484: Epoch time: 205.12 s\n",
            "2025-07-16 21:45:02.327794: \n",
            "2025-07-16 21:45:02.330203: Epoch 83\n",
            "2025-07-16 21:45:02.332616: Current learning rate: 0.00925\n",
            "2025-07-16 21:48:27.253766: train_loss -0.4889\n",
            "2025-07-16 21:48:27.256866: val_loss -0.5123\n",
            "2025-07-16 21:48:27.259921: Pseudo dice [np.float32(0.6946)]\n",
            "2025-07-16 21:48:27.262908: Epoch time: 204.93 s\n",
            "2025-07-16 21:48:27.265143: Yayy! New best EMA pseudo Dice: 0.6672000288963318\n",
            "2025-07-16 21:48:31.138392: \n",
            "2025-07-16 21:48:31.141060: Epoch 84\n",
            "2025-07-16 21:48:31.144609: Current learning rate: 0.00924\n",
            "2025-07-16 21:52:05.676304: train_loss -0.5115\n",
            "2025-07-16 21:52:05.680097: val_loss -0.3495\n",
            "2025-07-16 21:52:05.683674: Pseudo dice [np.float32(0.6331)]\n",
            "2025-07-16 21:52:05.687099: Epoch time: 214.54 s\n",
            "2025-07-16 21:52:07.040955: \n",
            "2025-07-16 21:52:07.057626: Epoch 85\n",
            "2025-07-16 21:52:07.059738: Current learning rate: 0.00923\n",
            "2025-07-16 21:55:32.674771: train_loss -0.5145\n",
            "2025-07-16 21:55:32.678387: val_loss -0.4426\n",
            "2025-07-16 21:55:32.680675: Pseudo dice [np.float32(0.6364)]\n",
            "2025-07-16 21:55:32.683090: Epoch time: 205.64 s\n",
            "2025-07-16 21:55:34.044080: \n",
            "2025-07-16 21:55:34.046558: Epoch 86\n",
            "2025-07-16 21:55:34.061506: Current learning rate: 0.00922\n",
            "2025-07-16 21:58:59.922499: train_loss -0.5279\n",
            "2025-07-16 21:58:59.925887: val_loss -0.3865\n",
            "2025-07-16 21:58:59.928127: Pseudo dice [np.float32(0.6501)]\n",
            "2025-07-16 21:58:59.930628: Epoch time: 205.88 s\n",
            "2025-07-16 21:59:01.272444: \n",
            "2025-07-16 21:59:01.275994: Epoch 87\n",
            "2025-07-16 21:59:01.278715: Current learning rate: 0.00921\n",
            "2025-07-16 22:02:26.955848: train_loss -0.5018\n",
            "2025-07-16 22:02:26.959581: val_loss -0.4257\n",
            "2025-07-16 22:02:26.962056: Pseudo dice [np.float32(0.6342)]\n",
            "2025-07-16 22:02:26.964536: Epoch time: 205.69 s\n",
            "2025-07-16 22:02:28.321686: \n",
            "2025-07-16 22:02:28.324652: Epoch 88\n",
            "2025-07-16 22:02:28.326911: Current learning rate: 0.0092\n",
            "2025-07-16 22:05:53.796097: train_loss -0.5305\n",
            "2025-07-16 22:05:53.799916: val_loss -0.451\n",
            "2025-07-16 22:05:53.802339: Pseudo dice [np.float32(0.6979)]\n",
            "2025-07-16 22:05:53.820420: Epoch time: 205.48 s\n",
            "2025-07-16 22:05:55.175106: \n",
            "2025-07-16 22:05:55.178050: Epoch 89\n",
            "2025-07-16 22:05:55.180917: Current learning rate: 0.0092\n",
            "2025-07-16 22:09:20.529719: train_loss -0.5266\n",
            "2025-07-16 22:09:20.532880: val_loss -0.4527\n",
            "2025-07-16 22:09:20.535861: Pseudo dice [np.float32(0.6771)]\n",
            "2025-07-16 22:09:20.538310: Epoch time: 205.36 s\n",
            "2025-07-16 22:09:21.952606: \n",
            "2025-07-16 22:09:21.968601: Epoch 90\n",
            "2025-07-16 22:09:21.970939: Current learning rate: 0.00919\n",
            "2025-07-16 22:12:47.233684: train_loss -0.5394\n",
            "2025-07-16 22:12:47.236944: val_loss -0.4213\n",
            "2025-07-16 22:12:47.238945: Pseudo dice [np.float32(0.6784)]\n",
            "2025-07-16 22:12:47.241059: Epoch time: 205.29 s\n",
            "2025-07-16 22:12:48.591516: \n",
            "2025-07-16 22:12:48.594177: Epoch 91\n",
            "2025-07-16 22:12:48.596110: Current learning rate: 0.00918\n",
            "2025-07-16 22:16:14.714905: train_loss -0.558\n",
            "2025-07-16 22:16:14.718526: val_loss -0.3811\n",
            "2025-07-16 22:16:14.722341: Pseudo dice [np.float32(0.6534)]\n",
            "2025-07-16 22:16:14.724788: Epoch time: 206.13 s\n",
            "2025-07-16 22:16:16.093618: \n",
            "2025-07-16 22:16:16.096431: Epoch 92\n",
            "2025-07-16 22:16:16.099145: Current learning rate: 0.00917\n",
            "2025-07-16 22:19:41.766456: train_loss -0.5342\n",
            "2025-07-16 22:19:41.769756: val_loss -0.3945\n",
            "2025-07-16 22:19:41.782753: Pseudo dice [np.float32(0.6269)]\n",
            "2025-07-16 22:19:41.786371: Epoch time: 205.68 s\n",
            "2025-07-16 22:19:43.086907: \n",
            "2025-07-16 22:19:43.090025: Epoch 93\n",
            "2025-07-16 22:19:43.092575: Current learning rate: 0.00916\n",
            "2025-07-16 22:23:08.681548: train_loss -0.509\n",
            "2025-07-16 22:23:08.685572: val_loss -0.3859\n",
            "2025-07-16 22:23:08.687948: Pseudo dice [np.float32(0.6438)]\n",
            "2025-07-16 22:23:08.690639: Epoch time: 205.6 s\n",
            "2025-07-16 22:23:10.021294: \n",
            "2025-07-16 22:23:10.039566: Epoch 94\n",
            "2025-07-16 22:23:10.042698: Current learning rate: 0.00915\n",
            "2025-07-16 22:26:36.059554: train_loss -0.5178\n",
            "2025-07-16 22:26:36.062641: val_loss -0.4535\n",
            "2025-07-16 22:26:36.064667: Pseudo dice [np.float32(0.6762)]\n",
            "2025-07-16 22:26:36.066518: Epoch time: 206.04 s\n",
            "2025-07-16 22:26:37.357145: \n",
            "2025-07-16 22:26:37.359554: Epoch 95\n",
            "2025-07-16 22:26:37.361714: Current learning rate: 0.00914\n",
            "2025-07-16 22:30:02.883021: train_loss -0.4761\n",
            "2025-07-16 22:30:02.885977: val_loss -0.4607\n",
            "2025-07-16 22:30:02.887765: Pseudo dice [np.float32(0.6805)]\n",
            "2025-07-16 22:30:02.890285: Epoch time: 205.53 s\n",
            "2025-07-16 22:30:04.133596: \n",
            "2025-07-16 22:30:04.136351: Epoch 96\n",
            "2025-07-16 22:30:04.138264: Current learning rate: 0.00913\n",
            "2025-07-16 22:33:28.921453: train_loss -0.537\n",
            "2025-07-16 22:33:28.937949: val_loss -0.3744\n",
            "2025-07-16 22:33:28.941912: Pseudo dice [np.float32(0.6562)]\n",
            "2025-07-16 22:33:28.944619: Epoch time: 204.79 s\n",
            "2025-07-16 22:33:30.202658: \n",
            "2025-07-16 22:33:30.205169: Epoch 97\n",
            "2025-07-16 22:33:30.206916: Current learning rate: 0.00912\n",
            "2025-07-16 22:36:54.947777: train_loss -0.5225\n",
            "2025-07-16 22:36:54.951127: val_loss -0.4082\n",
            "2025-07-16 22:36:54.953735: Pseudo dice [np.float32(0.6638)]\n",
            "2025-07-16 22:36:54.956532: Epoch time: 204.75 s\n",
            "2025-07-16 22:36:56.250659: \n",
            "2025-07-16 22:36:56.266241: Epoch 98\n",
            "2025-07-16 22:36:56.268654: Current learning rate: 0.00911\n",
            "2025-07-16 22:40:20.685279: train_loss -0.5166\n",
            "2025-07-16 22:40:20.688260: val_loss -0.4076\n",
            "2025-07-16 22:40:20.690068: Pseudo dice [np.float32(0.6882)]\n",
            "2025-07-16 22:40:20.692221: Epoch time: 204.44 s\n",
            "2025-07-16 22:40:21.954091: \n",
            "2025-07-16 22:40:21.956767: Epoch 99\n",
            "2025-07-16 22:40:21.959075: Current learning rate: 0.0091\n",
            "2025-07-16 22:43:46.528342: train_loss -0.5096\n",
            "2025-07-16 22:43:46.531793: val_loss -0.4451\n",
            "2025-07-16 22:43:46.534961: Pseudo dice [np.float32(0.6556)]\n",
            "2025-07-16 22:43:46.538187: Epoch time: 204.58 s\n",
            "2025-07-16 22:43:50.344481: \n",
            "2025-07-16 22:43:50.347186: Epoch 100\n",
            "2025-07-16 22:43:50.349391: Current learning rate: 0.0091\n",
            "2025-07-16 22:47:26.402849: train_loss -0.4872\n",
            "2025-07-16 22:47:26.405977: val_loss -0.5019\n",
            "2025-07-16 22:47:26.424493: Pseudo dice [np.float32(0.7015)]\n",
            "2025-07-16 22:47:26.426738: Epoch time: 216.06 s\n",
            "2025-07-16 22:47:26.428948: Yayy! New best EMA pseudo Dice: 0.6672999858856201\n",
            "2025-07-16 22:47:30.283238: \n",
            "2025-07-16 22:47:30.285789: Epoch 101\n",
            "2025-07-16 22:47:30.287846: Current learning rate: 0.00909\n",
            "2025-07-16 22:51:04.386903: train_loss -0.5175\n",
            "2025-07-16 22:51:04.390168: val_loss -0.4653\n",
            "2025-07-16 22:51:04.392073: Pseudo dice [np.float32(0.662)]\n",
            "2025-07-16 22:51:04.393874: Epoch time: 214.11 s\n",
            "2025-07-16 22:51:05.657896: \n",
            "2025-07-16 22:51:05.660676: Epoch 102\n",
            "2025-07-16 22:51:05.662937: Current learning rate: 0.00908\n",
            "2025-07-16 22:54:29.953867: train_loss -0.4863\n",
            "2025-07-16 22:54:29.957102: val_loss -0.5333\n",
            "2025-07-16 22:54:29.959429: Pseudo dice [np.float32(0.707)]\n",
            "2025-07-16 22:54:29.961995: Epoch time: 204.3 s\n",
            "2025-07-16 22:54:29.963805: Yayy! New best EMA pseudo Dice: 0.670799970626831\n",
            "2025-07-16 22:54:33.757771: \n",
            "2025-07-16 22:54:33.760411: Epoch 103\n",
            "2025-07-16 22:54:33.763099: Current learning rate: 0.00907\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ ADICIONAR NOVA CÉLULA 7B - Modificar Planos para Performance:\n",
        "\n",
        "import json\n",
        "import os\n",
        "\n",
        "# Caminho do arquivo de planos\n",
        "plans_file = \"/content/drive/MyDrive/nnunet_data/nnUNet_preprocessed/Dataset001_AorticValve/nnUNetResEncUNetLPlans.json\"\n",
        "\n",
        "if os.path.exists(plans_file):\n",
        "    print(\"📝 Modificando planos para melhor performance...\")\n",
        "\n",
        "    with open(plans_file, 'r') as f:\n",
        "        plans = json.load(f)\n",
        "\n",
        "    # MELHORIAS CRÍTICAS:\n",
        "    if '3d_fullres' in plans['configurations']:\n",
        "        config = plans['configurations']['3d_fullres']\n",
        "\n",
        "        # 1. Aumentar épocas\n",
        "        config['num_epochs'] = 1000  # Era ~250, agora 1000\n",
        "\n",
        "        # 2. Reduzir batch size se OOM\n",
        "        if config.get('batch_size', 2) > 1:\n",
        "            config['batch_size'] = 1\n",
        "            print(\"  ✓ Batch size reduzido para 1\")\n",
        "\n",
        "        # 3. Otimizar patch size para sua GPU\n",
        "        original_patch = config.get('patch_size', [32, 384, 384])\n",
        "        config['patch_size'] = [24, 320, 320]  # Menor para evitar OOM\n",
        "        print(f\"  ✓ Patch size: {original_patch} → {config['patch_size']}\")\n",
        "\n",
        "        # 4. Adicionar early stopping mais agressivo\n",
        "        config['patience'] = 50  # Parar após 50 épocas sem melhoria\n",
        "\n",
        "    # Salvar modificações\n",
        "    with open(plans_file, 'w') as f:\n",
        "        json.dump(plans, f, indent=2)\n",
        "\n",
        "    print(\"✅ Planos modificados com sucesso!\")\n",
        "else:\n",
        "    print(\"⚠️ Arquivo de planos não encontrado. Execute primeiro o plan_and_preprocess.\")"
      ],
      "metadata": {
        "id": "XDyGA46XbZpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "V7Lg8iLebfqM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "🔮 CÉLULA 8 — Predição nos dados de teste"
      ],
      "metadata": {
        "id": "iG_mMB-UPMHJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ Predição nos dados de teste\n",
        "import os\n",
        "\n",
        "DATASET_ID = \"001\"\n",
        "DATASET_NAME = \"AorticValve\"\n",
        "TRAINER_PLANS = \"nnUNetTrainer_plans_nnUNetResEncUNetLPlans\"\n",
        "CONFIGURATION = \"3d_fullres\"\n",
        "FOLD = \"0\"\n",
        "\n",
        "INPUT_TEST_DIR = f\"/content/drive/MyDrive/nnunet_data/nnUNet_raw/Dataset{DATASET_ID}_{DATASET_NAME}/imagesTs\"\n",
        "OUTPUT_PREDS_DIR_TEST = f\"/content/drive/MyDrive/nnunet_data/nnUNet_results/Dataset{DATASET_ID}_{DATASET_NAME}/{TRAINER_PLANS}_{CONFIGURATION}/fold_{FOLD}/predictionsTs_ResEncL\"\n",
        "\n",
        "print(f\"Iniciando predição nos dados de teste de {INPUT_TEST_DIR}...\")\n",
        "print(f\"Resultados serão salvos em: {OUTPUT_PREDS_DIR_TEST}\")\n",
        "\n",
        "# O nnUNetv2_predict criará os diretórios de saída automaticamente.\n",
        "!nnUNetv2_predict \\\n",
        "  -d {DATASET_ID} \\\n",
        "  -i {INPUT_TEST_DIR} \\\n",
        "  -o {OUTPUT_PREDS_DIR_TEST} \\\n",
        "  -c {CONFIGURATION} \\\n",
        "  -f {FOLD} \\\n",
        "  -p nnUNetResEncUNetLPlans \\\n",
        "  -chk checkpoint_best.pth # Usa o melhor checkpoint de validação"
      ],
      "metadata": {
        "id": "4iHn0DPIPOG_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "🔮 CÉLULA 9 — Predição nos dados de treino (validação cruzada)"
      ],
      "metadata": {
        "id": "rv4OIvyQPQm0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ Predição nos dados de treino (validação cruzada)\n",
        "import os\n",
        "\n",
        "DATASET_ID = \"001\"\n",
        "DATASET_NAME = \"AorticValve\"\n",
        "TRAINER_PLANS = \"nnUNetTrainer_plans_nnUNetResEncUNetLPlans\"\n",
        "CONFIGURATION = \"3d_fullres\"\n",
        "FOLD = \"0\"\n",
        "\n",
        "INPUT_TRAIN_DIR = f\"/content/drive/MyDrive/nnunet_data/nnUNet_raw/Dataset{DATASET_ID}_{DATASET_NAME}/imagesTr\"\n",
        "OUTPUT_PREDS_DIR_TRAIN = f\"/content/drive/MyDrive/nnunet_data/nnUNet_results/Dataset{DATASET_ID}_{DATASET_NAME}/{TRAINER_PLANS}_{CONFIGURATION}/fold_{FOLD}/predictionsTr_ResEncL\"\n",
        "\n",
        "print(f\"Iniciando predição nos dados de treino (validação) de {INPUT_TRAIN_DIR}...\")\n",
        "print(f\"Resultados serão salvos em: {OUTPUT_PREDS_DIR_TRAIN}\")\n",
        "\n",
        "!nnUNetv2_predict \\\n",
        "  -d {DATASET_ID} \\\n",
        "  -i {INPUT_TRAIN_DIR} \\\n",
        "  -o {OUTPUT_PREDS_DIR_TRAIN} \\\n",
        "  -c {CONFIGURATION} \\\n",
        "  -f {FOLD} \\\n",
        "  -p nnUNetResEncUNetLPlans \\\n",
        "  -chk checkpoint_best.pth # Usa o melhor checkpoint de validação"
      ],
      "metadata": {
        "id": "ORXAB-rdPTgu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "🧠 CÉLULA 10 — Identificar cálcio apenas dentro da válvula predita (para os dados de teste)"
      ],
      "metadata": {
        "id": "Z4vP9n2pPVIB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ Extrair apenas o cálcio dentro da válvula nas predições dos casos de teste\n",
        "import nibabel as nib\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "DATASET_ID = \"001\"\n",
        "DATASET_NAME = \"AorticValve\"\n",
        "TRAINER_PLANS = \"nnUNetTrainer_plans_nnUNetResEncUNetLPlans\"\n",
        "CONFIGURATION = \"3d_fullres\"\n",
        "FOLD = \"0\"\n",
        "\n",
        "BASE_NNUNET_RAW_DIR = f\"/content/drive/MyDrive/nnunet_data/nnUNet_raw/Dataset{DATASET_ID}_{DATASET_NAME}\"\n",
        "BASE_NNUNET_RESULTS_DIR = f\"/content/drive/MyDrive/nnunet_data/nnUNet_results/Dataset{DATASET_ID}_{DATASET_NAME}/{TRAINER_PLANS}_{CONFIGURATION}/fold_{FOLD}\"\n",
        "\n",
        "images_dir = os.path.join(BASE_NNUNET_RAW_DIR, 'imagesTs')\n",
        "preds_valve_dir = os.path.join(BASE_NNUNET_RESULTS_DIR, 'predictionsTs_ResEncL') # Predições da válvula aórtica\n",
        "output_calcium_dir = os.path.join(BASE_NNUNET_RESULTS_DIR, 'predictionsTs_calcio_na_valvula') # Onde o cálcio filtrado será salvo\n",
        "\n",
        "os.makedirs(output_calcium_dir, exist_ok=True)\n",
        "\n",
        "HU_LIMIAR_CALCIFICACAO = 130  # Limiar de unidades Hounsfield para identificar cálcio\n",
        "\n",
        "print(f\"Processando imagens de: {images_dir}\")\n",
        "print(f\"Usando predições da válvula de: {preds_valve_dir}\")\n",
        "print(f\"Salvando resultados do cálcio filtrado em: {output_calcium_dir}\n",
        "\")\n",
        "\n",
        "processed_count = 0\n",
        "for fname in sorted(os.listdir(images_dir)):\n",
        "    if not fname.endswith('_0000.nii.gz'): # Busca arquivos de imagem (terminam com _0000.nii.gz)\n",
        "        continue\n",
        "\n",
        "    # Extrai o ID do paciente (ex: 'AorticValve_083' de 'AorticValve_083_0000.nii.gz')\n",
        "    id_paciente_base = fname.replace('_0000.nii.gz', '')\n",
        "    print(f\"🔍 Processando paciente: {id_paciente_base}\")\n",
        "\n",
        "    try:\n",
        "        img_path = os.path.join(images_dir, fname)\n",
        "        pred_valve_path = os.path.join(preds_valve_dir, f\"{id_paciente_base}.nii.gz\")\n",
        "\n",
        "        if not os.path.exists(img_path):\n",
        "            print(f\"  AVISO: Imagem '{img_path}' não encontrada. Pulando este paciente.\")\n",
        "            continue\n",
        "        if not os.path.exists(pred_valve_path):\n",
        "            print(f\"  AVISO: Predição da válvula '{pred_valve_path}' não encontrada. Pulando este paciente.\")\n",
        "            continue\n",
        "\n",
        "        img = nib.load(img_path)\n",
        "        img_data = img.get_fdata()\n",
        "        pred_valve = nib.load(pred_valve_path).get_fdata()\n",
        "\n",
        "        valve_mask = pred_valve > 0 # Cria máscara booleana da válvula predita (valores > 0 são válvula)\n",
        "        calc_mask = img_data > HU_LIMIAR_CALCIFICACAO # Cria máscara booleana do cálcio na imagem original\n",
        "\n",
        "        # O cálcio dentro da válvula é a intersecção das duas máscaras\n",
        "        calc_dentro_valvula = np.logical_and(valve_mask, calc_mask)\n",
        "\n",
        "        # Salva o resultado como um novo arquivo NIfTI\n",
        "        nifti_out = nib.Nifti1Image(calc_dentro_valvula.astype(np.uint8), img.affine, img.header)\n",
        "        nib.save(nifti_out, os.path.join(output_calcium_dir, f\"{id_paciente_base}.nii.gz\"))\n",
        "        processed_count += 1\n",
        "    except Exception as e:\n",
        "        print(f\"  ❌ Erro ao processar paciente {id_paciente_base}: {e}\")\n",
        "\n",
        "print(f\"\\n✅ Cálcio na válvula (teste) salvo em: {output_calcium_dir} ({processed_count} arquivos processados)\")"
      ],
      "metadata": {
        "id": "QXeBwNaJPW6u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "🧠 CÉLULA 11 — Avaliação Dice + HD95 dos cálcios detectados (em teste)"
      ],
      "metadata": {
        "id": "EviXI5aLPYWO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ Avaliação Dice e HD95 do cálcio detectado na válvula predita (em comparação com o rótulo da válvula)\n",
        "# NOTA IMPORTANTE: Esta métrica compara a máscara de cálcio *predita dentro da válvula predita*\n",
        "# com o *rótulo original da válvula*. Se você possui rótulos específicos para o cálcio (ground truth)\n",
        "# eles deveriam ser usados para uma avaliação direta da segmentação de cálcio.\n",
        "\n",
        "import pandas as pd\n",
        "from medpy.metric import binary\n",
        "import nibabel as nib\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "DATASET_ID = \"001\"\n",
        "DATASET_NAME = \"AorticValve\"\n",
        "TRAINER_PLANS = \"nnUNetTrainer_plans_nnUNetResEncUNetLPlans\"\n",
        "CONFIGURATION = \"3d_fullres\"\n",
        "FOLD = \"0\"\n",
        "\n",
        "BASE_NNUNET_RAW_DIR = f\"/content/drive/MyDrive/nnunet_data/nnUNet_raw/Dataset{DATASET_ID}_{DATASET_NAME}\"\n",
        "BASE_NNUNET_RESULTS_DIR = f\"/content/drive/MyDrive/nnunet_data/nnUNet_results/Dataset{DATASET_ID}_{DATASET_NAME}/{TRAINER_PLANS}_{CONFIGURATION}/fold_{FOLD}\"\n",
        "\n",
        "labels_valve_dir = os.path.join(BASE_NNUNET_RAW_DIR, 'labelsTs') # Rótulos originais da válvula\n",
        "preds_calcium_in_valve_dir = os.path.join(BASE_NNUNET_RESULTS_DIR, 'predictionsTs_calcio_na_valvula') # Predições do cálcio filtrado\n",
        "\n",
        "resultados = []\n",
        "print(f\"Avaliando predições de cálcio na válvula em: {preds_calcium_in_valve_dir}\")\n",
        "print(f\"Comparando com rótulos da válvula em: {labels_valve_dir}\n",
        "\")\n",
        "\n",
        "evaluated_count = 0\n",
        "for fname in sorted(os.listdir(preds_calcium_in_valve_dir)):\n",
        "    if not fname.endswith('.nii.gz'):\n",
        "        continue\n",
        "\n",
        "    patient_id = fname.replace('.nii.gz', '')\n",
        "    print(f\"📊 Avaliando paciente: {patient_id}\")\n",
        "\n",
        "    try:\n",
        "        pred_calcium_path = os.path.join(preds_calcium_in_valve_dir, fname)\n",
        "        label_valve_path = os.path.join(labels_valve_dir, fname) # Assume o mesmo nome de arquivo\n",
        "\n",
        "        if not os.path.exists(pred_calcium_path):\n",
        "            print(f\"  AVISO: Predição de cálcio {pred_calcium_path} não encontrada. Pulando.\")\n",
        "            continue\n",
        "        if not os.path.exists(label_valve_path):\n",
        "            print(f\"  AVISO: Rótulo da válvula {label_valve_path} não encontrado. Pulando.\")\n",
        "            continue\n",
        "\n",
        "        pred_mask = nib.load(pred_calcium_path).get_fdata() > 0\n",
        "        label_mask = nib.load(label_valve_path).get_fdata() > 0\n",
        "\n",
        "        dice = float('nan')\n",
        "        hd95 = float('nan')\n",
        "\n",
        "        # Lida com casos onde uma ou ambas as máscaras podem estar vazias\n",
        "        if np.sum(pred_mask) == 0 and np.sum(label_mask) == 0:\n",
        "            dice = 1.0 # Perfeita concordância se ambos vazios\n",
        "            hd95 = 0.0 # Distância zero se ambos vazios\n",
        "        elif np.sum(pred_mask) == 0 or np.sum(label_mask) == 0:\n",
        "            dice = 0.0 # Um vazio e o outro não, sem sobreposição\n",
        "            # HD95 para um conjunto vazio e outro não é geralmente 'inf' ou um valor grande.\n",
        "            # MedPy levanta erro para isso, então tratamos.\n",
        "            hd95 = float('inf')\n",
        "        else:\n",
        "            try:\n",
        "                dice = binary.dc(pred_mask, label_mask)\n",
        "                hd95 = binary.hd95(pred_mask, label_mask)\n",
        "            except Exception as metric_e:\n",
        "                print(f\"  AVISO: Erro ao calcular métricas para {patient_id}: {metric_e}. Definindo como NaN.\")\n",
        "\n",
        "        resultados.append({'Paciente': patient_id, 'Dice': round(dice, 4), 'HD95': round(hd95, 2)})\n",
        "        evaluated_count += 1\n",
        "    except Exception as e:\n",
        "        print(f\"  ❌ Erro geral ao processar paciente {patient_id} para métricas: {e}\")\n",
        "        resultados.append({'Paciente': patient_id, 'Dice': float('nan'), 'HD95': float('nan')})\n",
        "\n",
        "df = pd.DataFrame(resultados)\n",
        "output_csv_path = os.path.join(preds_calcium_in_valve_dir, 'metrics_summary_calcio.csv')\n",
        "df.to_csv(output_csv_path, index=False)\n",
        "\n",
        "print(f\"\\n✅ Sumário das métricas salvo em: {output_csv_path} ({evaluated_count} pacientes avaliados)\")\n",
        "print(\"\\nDataFrame de resultados:\")\n",
        "print(df)\n"
      ],
      "metadata": {
        "id": "TS_ysEXkPZ7a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "📊 CÉLULA 12 — Visualizações gráficas (Boxplot + Histograma)"
      ],
      "metadata": {
        "id": "VCofDqH8PbSE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ Visualizações gráficas das métricas de desempenho\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "DATASET_ID = \"001\"\n",
        "DATASET_NAME = \"AorticValve\"\n",
        "TRAINER_PLANS = \"nnUNetTrainer_plans_nnUNetResEncUNetLPlans\"\n",
        "CONFIGURATION = \"3d_fullres\"\n",
        "FOLD = \"0\"\n",
        "\n",
        "BASE_NNUNET_RESULTS_DIR = f\"/content/drive/MyDrive/nnunet_data/nnUNet_results/Dataset{DATASET_ID}_{DATASET_NAME}/{TRAINER_PLANS}_{CONFIGURATION}/fold_{FOLD}\"\n",
        "preds_calcium_in_valve_dir = os.path.join(BASE_NNUNET_RESULTS_DIR, 'predictionsTs_calcio_na_valvula')\n",
        "input_csv_path = os.path.join(preds_calcium_in_valve_dir, 'metrics_summary_calcio.csv')\n",
        "\n",
        "print(f\"Tentando carregar dados de: {input_csv_path}\")\n",
        "\n",
        "if not os.path.exists(input_csv_path):\n",
        "    print(f\"ERRO: Arquivo de métricas '{input_csv_path}' não encontrado.\\nCertifique-se de que a Célula 11 foi executada com sucesso e gerou o CSV.\")\n",
        "else:\n",
        "    df = pd.read_csv(input_csv_path)\n",
        "\n",
        "    if df.empty:\n",
        "        print(\"AVISO: O DataFrame de métricas está vazio. Não há dados para plotar.\")\n",
        "    else:\n",
        "        sns.set(style=\"whitegrid\")\n",
        "\n",
        "        # Boxplot Dice\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        sns.boxplot(y=df['Dice'])\n",
        "        plt.title('Distribuição do Coeficiente Dice (Cálcio na Válvula Predita)', fontsize=14)\n",
        "        plt.ylabel('Coeficiente Dice', fontsize=12)\n",
        "        plt.grid(True, linestyle='--', alpha=0.7)\n",
        "        plt.show()\n",
        "\n",
        "        # Boxplot HD95\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        # Remover NaNs para o boxplot, se houver, pois o HD95 pode ser 'inf' para máscaras vazias\n",
        "        hd95_data = df['HD95'].replace([np.inf, -np.inf], np.nan).dropna() # Remove inf e NaN\n",
        "        if hd95_data.empty:\n",
        "            print(\"AVISO: Não há dados válidos para plotar o Boxplot HD95.\")\n",
        "        else:\n",
        "            sns.boxplot(y=hd95_data)\n",
        "            plt.title('Distribuição da Distância de Hausdorff 95 (Cálcio na Válvula Predita)', fontsize=14)\n",
        "            plt.ylabel('HD95 (mm)', fontsize=12)\n",
        "            plt.grid(True, linestyle='--', alpha=0.7)\n",
        "            plt.show()\n",
        "\n",
        "        # Histograma Dice\n",
        "        plt.figure(figsize=(10, 7))\n",
        "        sns.histplot(df['Dice'], bins=15, kde=True, color='skyblue', edgecolor='black')\n",
        "        plt.title('Histograma do Coeficiente Dice (Cálcio na Válvula Predita)', fontsize=14)\n",
        "        plt.xlabel('Coeficiente Dice', fontsize=12)\n",
        "        plt.ylabel('Frequência', fontsize=12)\n",
        "        plt.grid(True, linestyle='--', alpha=0.7)\n",
        "        plt.show()\n",
        "\n",
        "        print(\"\\n✅ Visualizações geradas com sucesso!\")\n"
      ],
      "metadata": {
        "id": "Ud6EdT9YPc5j"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}