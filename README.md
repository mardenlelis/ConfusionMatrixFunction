# ConfusionFunction

## Description

The `ConfusionFunction` class calculates performance metrics for binary classifiers using the confusion matrix. Additionally, it generates error, ROC, and AUC curve plots for different classification thresholds.

## Features

- Calculation of metrics such as accuracy, precision, recall, F1 score, specificity, negative predictive value, false positive rate, false discovery rate, false negative rate, Cohen's kappa, and Jaccard index.
- Generation of error, ROC, and AUC curve plots.
- Printing of confusion matrix and associated metrics for different thresholds.

Metrics Explained

Accuracy
Formula: 
    (ğ‘‡ğ‘ƒ+ğ‘‡ğ‘)/(ğ‘‡ğ‘ƒ+ğ¹ğ‘ƒ+ğ‘‡ğ‘+ğ¹ğ‘)
    (TP+TN)/(TP+FP+TN+FN)
Description: Measures the proportion of true results (both true positives and true negatives) among the total number of cases examined.

Precision
Formula: 
    ğ‘‡ğ‘ƒ/(ğ‘‡ğ‘ƒ+ğ¹ğ‘ƒ)
    TP/(TP+FP)
Description: Measures the proportion of positive results that are true positives (i.e., the accuracy of the positive predictions).

Recall (Sensitivity)
Formula: 
    ğ‘‡ğ‘ƒ/(ğ‘‡ğ‘ƒ+ğ¹ğ‘)
    TP/(TP+FN)
Description: Measures the proportion of true positives that are correctly identified by the model.

F1 Score
Formula: 
    2â‹…(ğ‘ƒğ‘Ÿğ‘’ğ‘ğ‘–ğ‘ ğ‘–ğ‘œğ‘›â‹…ğ‘…ğ‘’ğ‘ğ‘ğ‘™ğ‘™)/(ğ‘ƒğ‘Ÿğ‘’ğ‘ğ‘–ğ‘ ğ‘–ğ‘œğ‘›+ğ‘…ğ‘’ğ‘ğ‘ğ‘™ğ‘™)
    2â‹…(Precisionâ‹…Recall)/(Precision+Recall)
Description: The harmonic mean of precision and recall, providing a balance between the two.

Specificity (True Negative Rate)
Formula: 
    ğ‘‡ğ‘/(ğ‘‡ğ‘+ğ¹ğ‘ƒ)
    TN/(TN+FP)
Description: Measures the proportion of true negatives that are correctly identified.

Negative Predictive Value (NPV)
Formula: 
    ğ‘‡ğ‘/(ğ‘‡ğ‘+ğ¹ğ‘)
    TN/(TN+FN)
Description: Measures the proportion of negative results that are true negatives.

False Positive Rate (FPR)
Formula: 
    ğ¹ğ‘ƒ/(ğ¹ğ‘ƒ+ğ‘‡ğ‘)
    FP/(FP+TN)
Description: Measures the proportion of false positives among all negatives.

False Discovery Rate (FDR)
Formula: 
    ğ¹ğ‘ƒ/(ğ¹ğ‘ƒ+ğ‘‡ğ‘ƒ)
    FP/(FP+TP)
Description: Measures the proportion of false positives among all positive results.

False Negative Rate (FNR)
Formula: 
    ğ¹ğ‘/(ğ¹ğ‘+ğ‘‡ğ‘ƒ)
    FN/(FN+TP)
    Description: Measures the proportion of false negatives among all positives.

Cohen's Kappa
Formula: 
    ğœ…=(ğ‘ğ‘œâˆ’ğ‘ğ‘’)/(1âˆ’ğ‘ğ‘’)
    Îº=(poâˆ’pe)/(1âˆ’pe)
    ğ‘ğ‘œpo = observed agreement
    ğ‘ğ‘’pe = expected agreement by chance
Description: A measure of inter-rater agreement that considers the agreement occurring by chance.

Jaccard Index
Formula: 
    ğ‘‡ğ‘ƒ/(ğ‘‡ğ‘ƒ+ğ¹ğ‘ƒ+ğ¹ğ‘)
    TP/(TP+FP+FN)
Description: Measures the similarity between the predicted and actual classes.

## Installation

Clone this repository:

```bash
git clone https://github.com/your-username/ConfusionFunction.git
cd ConfusionFunction


